{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-07667f6c8b53>:38: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-07667f6c8b53>:38: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available:  False\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "#This script is used to train the model\n",
    "import click as ck\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import math\n",
    "from collections import deque\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Embedding, Conv1D, Flatten, Concatenate,\n",
    "    MaxPooling1D, Dropout, Maximum, Layer,LSTM, Dense,TimeDistributed,experimental,Bidirectional\n",
    ")\n",
    "# run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "# from tensorflow.keras.layers.experimental import (RandomFourierFeatures)\n",
    "# from tensorflow.keras.layers import (\n",
    "#     Input, Dense, Embedding, Conv1D, Flatten, Concatenate,\n",
    "#     MaxPooling1D, Dropout,LSTM, Dense,TimeDistributed,\n",
    "# )\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, matthews_corrcoef\n",
    "from aminoacids import MAXLEN, to_onehot\n",
    "from utils import Ontology, FUNC_DICT, is_exp_code\n",
    "\n",
    "# from kerastuner.tuners import RandomSearch\n",
    "# from kerastuner import HyperModel\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "run_opts = tf.compat.v1.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff=7000\n",
    "n1=int (ff*0.7)\n",
    "n2=int(ff*0.2)\n",
    "n3=int(ff*0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HPOLayer(Layer):\n",
    "\n",
    "    def __init__(self, nb_classes, **kwargs):\n",
    "        self.nb_classes = nb_classes\n",
    "        self.hpo_matrix = np.zeros((nb_classes, nb_classes), dtype=np.float32)\n",
    "        super(HPOLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def set_hpo_matrix(self, hpo_matrix):\n",
    "        self.hpo_matrix = hpo_matrix\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(HPOLayer, self).get_config()\n",
    "        config['nb_classes'] = self.nb_classes\n",
    "        return config\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        assert input_shape[1] == self.nb_classes\n",
    "        self.kernel = K.variable(\n",
    "            self.hpo_matrix, name='{}_kernel'.format(self.name))\n",
    "        self.non_trainable_weights.append(self.kernel)\n",
    "        super(HPOLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.keras.backend.repeat(x, self.nb_classes)\n",
    "        return tf.math.multiply(x, self.kernel)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [input_shape[0], self.nb_classes, self.nb_classes] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main(hp_file, data_file, terms_file, gos_file, model_file,\n",
    "         out_file, fold, batch_size, epochs, load, logger_file, threshold,\n",
    "         device):\n",
    "    gos_df = pd.read_pickle(gos_file)\n",
    "#     gos_df = gos_df.iloc[:2000,:] \n",
    "    gos = gos_df['gos'].values.flatten()\n",
    "    gos_dict = {v: i for i, v in enumerate(gos)}\n",
    "\n",
    "    # cross validation settings\n",
    "    # model_file = f'fold{fold}_' + model_file\n",
    "    # out_file = f'fold{fold}_' + out_file\n",
    "    params = {\n",
    "        'input_shape': (len(gos),),\n",
    "        'nb_layers': 1,\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'rate': 0.3,\n",
    "        'learning_rate': 0.001,\n",
    "        'units': 1500, # 750\n",
    "        'model_file': model_file\n",
    "    }\n",
    "    \n",
    "    print('Params:', params)\n",
    "    global hpo\n",
    "    hpo = Ontology(hp_file, with_rels=True)\n",
    "    terms_df = pd.read_pickle(terms_file)\n",
    "    terms_df = terms_df.iloc[:2600,:] \n",
    "    global terms\n",
    "    terms = terms_df['terms'].values.flatten()\n",
    "    print('Phenotypes', len(terms))\n",
    "    global term_set\n",
    "    term_set = set(terms)\n",
    "    train_df, valid_df, test_df = load_data(data_file, terms, fold)\n",
    "    terms_dict = {v: i for i, v in enumerate(terms)}\n",
    "    hpo_matrix = get_hpo_matrix(hpo, terms_dict)\n",
    "    nb_classes = len(terms)\n",
    "    params['nb_classes'] = nb_classes\n",
    "    print(len(terms_dict))\n",
    "    test_steps = int(math.ceil(len(test_df) / batch_size))\n",
    "    test_generator = DFGenerator(test_df, gos_dict, terms_dict,\n",
    "                                 batch_size)\n",
    "    valid_steps = int(math.ceil(len(valid_df) / batch_size))\n",
    "    train_steps = int(math.ceil(len(train_df) / batch_size))\n",
    "\n",
    "    xy_generator = DFGenerator(train_df, gos_dict, terms_dict,\n",
    "                                  len(train_df))\n",
    "    x, y = xy_generator[0]\n",
    "    val_generator = DFGenerator(valid_df, gos_dict, terms_dict,\n",
    "                                  len(valid_df))\n",
    "    val_x, val_y = val_generator[0]\n",
    "\n",
    "    train_generator = DFGenerator(train_df, gos_dict, terms_dict,\n",
    "                                  batch_size)\n",
    "    valid_generator = DFGenerator(valid_df, gos_dict, terms_dict,\n",
    "                                  batch_size)\n",
    "    \n",
    "    with tf.device(device):\n",
    "        if load:\n",
    "            print('Loading pretrained model')\n",
    "            model = load_model(model_file, custom_objects={'HPOLayer': HPOLayer})\n",
    "            flat_model = load_model(model_file + '_flat.h5')\n",
    "        else:\n",
    "            print('Creating a new model')\n",
    "            flat_model = MyHyperModel(params)\n",
    "            # flat_model = create_flat_model(params)\n",
    "\n",
    "            print(\"Training data size: %d\" % len(train_df))\n",
    "            print(\"Validation data size: %d\" % len(valid_df))\n",
    "            checkpointer = ModelCheckpoint(\n",
    "                filepath=model_file + '_flat.h5',\n",
    "                verbose=1, save_best_only=True)\n",
    "            earlystopper = EarlyStopping(monitor='val_loss', patience=6, verbose=1)\n",
    "            logger = CSVLogger(logger_file)\n",
    "\n",
    "            # print('Starting training the flat model')\n",
    "            # flat_model.summary()\n",
    "            # flat_model.fit(\n",
    "            #     train_generator,\n",
    "            #     steps_per_epoch=train_steps,\n",
    "            #     epochs=epochs,\n",
    "            #     validation_data=valid_generator,\n",
    "            #     validation_steps=valid_steps,\n",
    "            #     max_queue_size=batch_size,\n",
    "            #     workers=12,\n",
    "            #     callbacks=[checkpointer, earlystopper])\n",
    "\n",
    "            tuner = RandomSearch(\n",
    "                flat_model,\n",
    "                objective='val_loss',\n",
    "                max_trials=1,\n",
    "                directory='data-cafa',\n",
    "                project_name='pheno')\n",
    "            tuner.search(\n",
    "                x, y, epochs=3, validation_data=(val_x, val_y),\n",
    "                callbacks=[earlystopper])\n",
    "            tuner.results_summary()\n",
    "            logging.info('Loading best model')\n",
    "            flat_model = tuner.get_best_models(num_models=1)[0]\n",
    "            flat_model.summary()\n",
    "            loss = flat_model.evaluate(val_x, val_y)\n",
    "            print('Valid loss %f' % loss)\n",
    "            flat_model.save(model_file + '_flat.h5')\n",
    "\n",
    "            model = create_model(params, hpo_matrix)\n",
    "\n",
    "            checkpointer = ModelCheckpoint(\n",
    "                filepath=model_file,\n",
    "                verbose=1, save_best_only=True)\n",
    "            model.summary()\n",
    "            print('Starting training the flat model')\n",
    "            model.fit(\n",
    "                train_generator,\n",
    "                steps_per_epoch=train_steps,\n",
    "                epochs=epochs,\n",
    "                validation_data=valid_generator,\n",
    "                validation_steps=valid_steps,\n",
    "                max_queue_size=batch_size,\n",
    "                workers=12,\n",
    "                callbacks=[logger, checkpointer, earlystopper])\n",
    "\n",
    "            logging.info('Loading best model')\n",
    "            model = load_model(model_file, custom_objects={'HPOLayer': HPOLayer})\n",
    "            flat_model = load_model(model_file + '_flat.h5')\n",
    "            \n",
    "        logging.info('Evaluating model')\n",
    "        loss = flat_model.evaluate(test_generator, steps=test_steps)\n",
    "#         print('Flat Test loss %f' % loss)\n",
    "        loss = model.evaluate(test_generator, steps=test_steps)\n",
    "#         print('Test loss %f' % loss)\n",
    "\n",
    "        logging.info('Predicting')\n",
    "        preds = model.predict(test_generator, steps=test_steps, verbose=1)\n",
    "        flat_preds = flat_model.predict(test_generator, steps=test_steps, verbose=1)\n",
    "        all_terms_df = pd.read_pickle(terms_file)\n",
    "        all_terms = all_terms_df['terms'].values\n",
    "        all_terms_dict = {v:k for k,v in enumerate(all_terms)}\n",
    "        all_labels = np.zeros((len(test_df), len(all_terms)), dtype=np.int32)\n",
    "        for i, row in enumerate(test_df.itertuples()):\n",
    "            for hp_id in row.hp_annotations:\n",
    "                if hp_id in all_terms_dict:\n",
    "                    all_labels[i, all_terms_dict[hp_id]] = 1\n",
    "        \n",
    "        all_preds = np.zeros((len(test_df), len(all_terms)), dtype=np.float32)\n",
    "        all_flat_preds = np.zeros((len(test_df), len(all_terms)), dtype=np.float32)\n",
    "        for i in range(len(test_df)):\n",
    "            for j in range(nb_classes):\n",
    "                all_preds[i, all_terms_dict[terms[j]]] = preds[i, j]\n",
    "                all_flat_preds[i, all_terms_dict[terms[j]]] = flat_preds[i, j]\n",
    "        logging.info('Computing performance:')\n",
    "        roc_auc = compute_roc(all_labels, all_preds)\n",
    "        print('ROC AUC: %.2f' % (roc_auc,))\n",
    "        flat_roc_auc = compute_roc(all_labels, all_flat_preds)\n",
    "        print('FLAT ROC AUC: %.2f' % (flat_roc_auc,))\n",
    "        test_df['preds'] = list(preds)\n",
    "        print(test_df)\n",
    "        logging.info('Saving predictions')\n",
    "        test_df.to_pickle(out_file)\n",
    "\n",
    "        test_df['preds'] = list(flat_preds)\n",
    "        test_df.to_pickle(out_file + '_flat.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc(labels, preds):\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr, tpr, _ = roc_curve(labels.flatten(), preds.flatten())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    return roc_auc\n",
    "\n",
    "def load_data(data_file, terms, fold=1):\n",
    "    df = pd.read_pickle(data_file)\n",
    "    # Split train/valid\n",
    "    n = len(df)\n",
    "    n1=int (n*0.5)\n",
    "    n2=int(n*0.4)\n",
    "    n3=int(n*0.1)\n",
    "    index = np.arange(n)\n",
    "    np.random.seed(seed=10)\n",
    "    np.random.shuffle(index)\n",
    "    index = list(index)\n",
    "    train_index = []\n",
    "    test_index = []\n",
    "    print()\n",
    "    # fn = n / 5\n",
    "    # # 5 fold cross-validation\n",
    "    # for i in range(1, 6):\n",
    "    #     start = int((i - 1) * fn)\n",
    "    #     end = int(i * fn)\n",
    "    #     if i == fold:\n",
    "    #         test_index += index[start:end]\n",
    "    #     else:\n",
    "    #         train_index += index[start:end]\n",
    "    # assert n == len(test_index) + len(train_index)\n",
    "    # train_df = df.iloc[train_index]\n",
    "    # test_df = df.iloc[test_index]\n",
    "\n",
    "    # valid_n = int(len(train_df) * 0.9)\n",
    "    # valid_df = train_df.iloc[valid_n:]\n",
    "    # train_df = train_df.iloc[:valid_n]\n",
    "     \n",
    "    # All Swissprot proteins\n",
    "    train_n = int(n * 0.9)\n",
    "#     train_df = df.iloc[index[:train_n]]\n",
    "#     valid_df = df.iloc[index[train_n:]]\n",
    "#     test_df=df.iloc[index[train_n:]]\n",
    "    train_df = df.iloc[index[:n1]]\n",
    "    valid_df = df.iloc[index[n1+1:n1+n2]]\n",
    "    test_df=df.iloc[index[n1+n2+1:]]\n",
    "    print(len(train_df),len(valid_df),len(test_df))\n",
    "#     print(n1,n1+n2)\n",
    "#     print(n1+n2,n1+n2+n3)\n",
    "    # CAFA2 Test data\n",
    "    # train_n = int(n * 0.9)\n",
    "    # train_df = df.iloc[index[:train_n]]\n",
    "    # valid_df = df.iloc[index[train_n:]]\n",
    "    # test_df = pd.read_pickle('data-cafa/human_test.pkl')\n",
    "    print(len(df), len(train_df), len(valid_df), len(test_df))\n",
    "    return train_df, valid_df, test_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class MyHyperModel(HyperModel):\n",
    "\n",
    "#     def __init__(self, params):\n",
    "#         self.params = params\n",
    "\n",
    "#     def build(self, hp):\n",
    "#         inp = Input(shape=self.params['input_shape'], dtype=np.float32)\n",
    "#         net = inp\n",
    "#         for i in range(self.params['nb_layers']):\n",
    "#             net = Dense(\n",
    "#                 units=hp.Int(\n",
    "#                     'units', min_value=250, max_value=2000, step=250),\n",
    "#                 name=f'dense_{i}', activation='relu')(net)\n",
    "#             net = Dropout(hp.Choice('rate', values=[0.3, 0.5]))(net)\n",
    "#         output = Dense(\n",
    "#             self.params['nb_classes'], activation='sigmoid',\n",
    "#             name='dense_out')(net)\n",
    "\n",
    "#         model = Model(inputs=inp, outputs=output)\n",
    "#         model.summary()\n",
    "#         model.compile(\n",
    "#             optimizer=Adam(\n",
    "#                 hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "#             loss=self.params['loss'])\n",
    "#         return model\n",
    "\n",
    "\n",
    "def get_hpo_matrix(hpo, terms_dict):\n",
    "    nb_classes = len(terms_dict)\n",
    "    res = np.zeros((nb_classes, nb_classes), dtype=np.float32)\n",
    "    for hp_id, i in terms_dict.items():\n",
    "        subs = hpo.get_term_set(hp_id)\n",
    "        res[i, i] = 1\n",
    "        for h_id in subs:\n",
    "            if h_id in terms_dict:\n",
    "                res[i, terms_dict[h_id]] = 1\n",
    "    return res\n",
    "\n",
    "\n",
    "def create_flat_model(params):\n",
    "    inp = Input(shape=params['input_shape'], dtype=np.float32)\n",
    "    net = inp\n",
    "    for i in range(params['nb_layers']):\n",
    "        net = Dense(\n",
    "            units=params['units'], name=f'dense_{i}', activation='relu')(net)\n",
    "        net = Dropout(rate=params['rate'])(net)\n",
    "    net = Dense(\n",
    "        params['nb_classes'], activation='sigmoid',\n",
    "        name='dense_out')(net)\n",
    "    output = Flatten()(net)\n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    model.summary()\n",
    "    model.compile(\n",
    "        optimizer=Adam(lr=params['learning_rate']),\n",
    "        loss=params['loss'])\n",
    "    logging.info('Compilation finished')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DFGenerator(Sequence):                                                                                                               \n",
    "                                                                                                                                         \n",
    "    def __init__(self, df, gos_dict, terms_dict, batch_size):\n",
    "        self.start = 0\n",
    "        self.size = len(df)\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.terms_dict = terms_dict\n",
    "        self.gos_dict = gos_dict\n",
    "                                                                                                                                         \n",
    "    def __len__(self):                                                                                                                   \n",
    "        return np.ceil(len(self.df) / float(self.batch_size)).astype(np.int32)                                                           \n",
    "                                                                                                                                         \n",
    "    def __getitem__(self, idx):                                                                                                          \n",
    "        batch_index = np.arange(                                                                                                         \n",
    "            idx * self.batch_size, min(self.size, (idx + 1) * self.batch_size))                                                          \n",
    "        df = self.df.iloc[batch_index]                                                                                                   \n",
    "        data_seq = np.zeros((len(df), MAXLEN, 21), dtype=np.float32)\n",
    "        data_gos = np.zeros((len(df), len(self.gos_dict)), dtype=np.float32)\n",
    "        labels = np.zeros((len(df), len(self.terms_dict)), dtype=np.int32)\n",
    "        for i, row in enumerate(df.itertuples()):\n",
    "            data_seq[i, :] = to_onehot(row.sequences)\n",
    "            \n",
    "            for item in row.deepgo_annotations:\n",
    "                t_id, score = item.split('|')\n",
    "                if t_id in self.gos_dict:\n",
    "                    data_gos[i, self.gos_dict[t_id]] = float(score)\n",
    "\n",
    "            for t_id in row.iea_annotations:\n",
    "                if t_id in self.gos_dict:\n",
    "                    data_gos[i, self.gos_dict[t_id]] = 1\n",
    "\n",
    "            for t_id in row.go_annotations:\n",
    "                if t_id in self.gos_dict:\n",
    "                    data_gos[i, self.gos_dict[t_id]] = 1\n",
    "                \n",
    "            for t_id in row.hp_annotations:\n",
    "                if t_id in self.terms_dict:\n",
    "                    labels[i, self.terms_dict[t_id]] = 1\n",
    "        return (data_gos, labels)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "len(range (8,32,8))\n",
    "max_kernel=32\n",
    "Kernels=range (8,max_kernel,8)\n",
    "for i in range (len(Kernels)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model CNN only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(params, hpo_matrix):\n",
    "    inp = Input(shape=params['input_shape'], dtype=np.float32)\n",
    "    # Load flat model\n",
    "    flat_model = load_model(params['model_file'] + '_flat.h5') \n",
    "    net = flat_model(inp) \n",
    "    hpo_layer = HPOLayer(params['nb_classes'])\n",
    "    hpo_layer.trainable = False\n",
    "    hpo_layer.set_hpo_matrix(hpo_matrix)\n",
    "    net = hpo_layer(net)\n",
    "    Matrix=[]\n",
    "    max_kernel=32\n",
    "    Kernels=range (8,max_kernel,8)\n",
    "#     nb_filters=32\n",
    "#     nb_filters=64\n",
    "    nb_filters=20\n",
    "    for i in range (len(Kernels)):\n",
    "        conv = (Conv1D(filters=nb_filters,     \n",
    "                       kernel_size=Kernels[i],     \n",
    "                       padding='valid',kernel_initializer= 'glorot_normal'))(net)\n",
    "        pool=MaxPooling1D(pool_size=16)(conv)\n",
    "        \n",
    "#         lstm=LSTM(Kernels[i], activation=\"tanh\")(pool)\n",
    "#         B_LSTM=(Bidirectional(LSTM(Kernels[i], return_sequences=True), input_shape=(10, 1)))(net)\n",
    "        flat= Flatten()(pool)\n",
    "        Matrix.append(flat)\n",
    "    net=Concatenate(axis=1)(Matrix)\n",
    "#     output = Flatten()(net)\n",
    "#     output=RandomFourierFeatures( params['nb_classes'],kernel_initializer='laplacian',scale=None,trainable=False, name='Output_SVM')(net)\n",
    "    \n",
    "    #net=Dense(params['nb_classes'],activation='relu')(net)\n",
    "    #output=Dense(params['nb_classes'], activation='sigmoid', name='dense_out')(net)\n",
    "    output=Dense(params['nb_classes'], activation='sigmoid', name='dense_out')(net)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "#     model.compile( optimizer=Adam(lr=params['learning_rate']),   \n",
    "    model.compile(\n",
    "        optimizer=Adam(lr=params['learning_rate']),\n",
    "        metrics=['accuracy'],\n",
    "        loss=params['loss'])    \n",
    "    logging.info('Compilation finished')\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(params, hpo_matrix):\n",
    "    inp = Input(shape=params['input_shape'], dtype=np.float32)\n",
    "    # Load flat model\n",
    "    flat_model = load_model(params['model_file'] + '_flat.h5') \n",
    "    net = flat_model(inp) \n",
    "    hpo_layer = HPOLayer(params['nb_classes'])\n",
    "    hpo_layer.trainable = False\n",
    "    hpo_layer.set_hpo_matrix(hpo_matrix)\n",
    "    net = hpo_layer(net)\n",
    "    Matrix=[]\n",
    "    max_kernel=32\n",
    "    Kernels=range (8,max_kernel,8)\n",
    "#     nb_filters=32\n",
    "#     nb_filters=64\n",
    "    nb_filters=20\n",
    "    for i in range (len(Kernels)):\n",
    "        conv = (Conv1D(filters=nb_filters,     \n",
    "                       kernel_size=Kernels[i],     \n",
    "                       padding='valid',kernel_initializer= 'glorot_normal'))(net)\n",
    "        pool=MaxPooling1D(pool_size=16)(conv)\n",
    "        \n",
    "        lstm=LSTM(Kernels[i], activation=\"tanh\")(pool)\n",
    "        flat= Flatten()(lstm)\n",
    "        Matrix.append(flat)\n",
    "    net=Concatenate(axis=1)(Matrix)\n",
    "#     output = Flatten()(net)\n",
    "#     output=RandomFourierFeatures( params['nb_classes'],kernel_initializer='laplacian',scale=None,trainable=False, name='Output_SVM')(net)\n",
    "    \n",
    "    #net=Dense(params['nb_classes'],activation='relu')(net)\n",
    "    #output=Dense(params['nb_classes'], activation='sigmoid', name='dense_out')(net)\n",
    "    output=Dense(params['nb_classes'], activation='sigmoid', name='dense_out')(net)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "#     model.compile( optimizer=Adam(lr=params['learning_rate']),   \n",
    "    model.compile(\n",
    "        optimizer=Adam(lr=params['learning_rate']),\n",
    "        metrics=['accuracy'],\n",
    "        loss=params['loss'])    \n",
    "    logging.info('Compilation finished')\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model LSTM only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(params, hpo_matrix):\n",
    "    inp = Input(shape=params['input_shape'], dtype=np.float32)\n",
    "    # Load flat model\n",
    "    flat_model = load_model(params['model_file'] + '_flat.h5') \n",
    "    net = flat_model(inp) \n",
    "    hpo_layer = HPOLayer(params['nb_classes'])\n",
    "    hpo_layer.trainable = False\n",
    "    hpo_layer.set_hpo_matrix(hpo_matrix)\n",
    "    net = hpo_layer(net)\n",
    "    Matrix=[]\n",
    "    max_kernel=32\n",
    "    Kernels=range (8,max_kernel,8)\n",
    "#     nb_filters=32\n",
    "#     nb_filters=64\n",
    "    nb_filters=20\n",
    "    for i in range (len(Kernels)):\n",
    "#         conv = (Conv1D(filters=nb_filters,     \n",
    "#                        kernel_size=Kernels[i],     \n",
    "#                        padding='valid',kernel_initializer= 'glorot_normal'))(net)\n",
    "#         pool=MaxPooling1D(pool_size=16)(conv)\n",
    "        \n",
    "        lstm=LSTM(Kernels[i], activation=\"tanh\")(net)\n",
    "        flat= Flatten()(lstm)\n",
    "        Matrix.append(flat)\n",
    "    net=Concatenate(axis=1)(Matrix)\n",
    "#     output = Flatten()(net)\n",
    "#     output=RandomFourierFeatures( params['nb_classes'],kernel_initializer='laplacian',scale=None,trainable=False, name='Output_SVM')(net)\n",
    "    \n",
    "    #net=Dense(params['nb_classes'],activation='relu')(net)\n",
    "    #output=Dense(params['nb_classes'], activation='sigmoid', name='dense_out')(net)\n",
    "    output=Dense(params['nb_classes'], activation='sigmoid', name='dense_out')(net)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "#     model.compile( optimizer=Adam(lr=params['learning_rate']),   \n",
    "    model.compile(\n",
    "        optimizer=Adam(lr=params['learning_rate']),\n",
    "        metrics=['accuracy'],\n",
    "        loss=params['loss'])    \n",
    "    logging.info('Compilation finished')\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial_7 --> CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'main' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-078a53e760a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# main(hp_file, data_file, terms_file, gos_file, model_file,out_file, fold, batch_size, epochs, load, logger_file, threshold,device):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m main('data/hp.obo',\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m      \u001b[1;34m'data/My_Implementations/human.pkl'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'main' is not defined"
     ]
    }
   ],
   "source": [
    "# main(hp_file, data_file, terms_file, gos_file, model_file,out_file, fold, batch_size, epochs, load, logger_file, threshold,device):\n",
    "\n",
    "main('data/hp.obo',\n",
    "     \n",
    "     'data/My_Implementations/human.pkl',\n",
    "     \n",
    "     'data/all_terms.pkl',\n",
    "     \n",
    "     'data/My_Implementations/gos.pkl',\n",
    "     \n",
    "     'data/My_Implementations/Trial_7/model_mohamed.h5',\n",
    "     \n",
    "     'data/My_Implementations/Trial_7/predictions.pkl',\n",
    "     \n",
    "     1,\n",
    "#batch_size\n",
    "     10,\n",
    "#Number of epochs     \n",
    "     1024,\n",
    "     \n",
    "     False,\n",
    "                                   \n",
    "     'data/My_Implementations/Trial_7/training.csv',\n",
    "     \n",
    "     0.5,\n",
    "    \n",
    "     'GPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {'input_shape': (24274,), 'nb_layers': 1, 'loss': 'binary_crossentropy', 'rate': 0.3, 'learning_rate': 0.001, 'units': 1500, 'model_file': 'data/My_Implementations/Trial_7/model_mohamed.h5'}\n",
      "Phenotypes 2600\n",
      "3933 3539 394 394\n",
      "2600\n",
      "Loading pretrained model\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "INFO:root:Evaluating model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 2s - loss: 0.151 - ETA: 1s - loss: 0.146 - ETA: 0s - loss: 0.141 - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.144 - ETA: 0s - loss: 0.144 - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.144 - ETA: 0s - loss: 0.144 - ETA: 0s - loss: 0.145 - ETA: 0s - loss: 0.145 - 1s 19ms/step - loss: 0.1451\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 17s - loss: 0.1651 - accuracy: 0.947 - ETA: 10s - loss: 0.1461 - accuracy: 0.953 - ETA: 8s - loss: 0.1523 - accuracy: 0.951 - ETA: 6s - loss: 0.1500 - accuracy: 0.95 - ETA: 5s - loss: 0.1388 - accuracy: 0.95 - ETA: 5s - loss: 0.1398 - accuracy: 0.95 - ETA: 4s - loss: 0.1437 - accuracy: 0.95 - ETA: 4s - loss: 0.1407 - accuracy: 0.95 - ETA: 4s - loss: 0.1421 - accuracy: 0.95 - ETA: 3s - loss: 0.1447 - accuracy: 0.95 - ETA: 3s - loss: 0.1498 - accuracy: 0.95 - ETA: 3s - loss: 0.1448 - accuracy: 0.95 - ETA: 3s - loss: 0.1460 - accuracy: 0.95 - ETA: 3s - loss: 0.1436 - accuracy: 0.95 - ETA: 2s - loss: 0.1447 - accuracy: 0.95 - ETA: 2s - loss: 0.1445 - accuracy: 0.95 - ETA: 2s - loss: 0.1454 - accuracy: 0.95 - ETA: 2s - loss: 0.1444 - accuracy: 0.95 - ETA: 2s - loss: 0.1438 - accuracy: 0.95 - ETA: 2s - loss: 0.1431 - accuracy: 0.95 - ETA: 2s - loss: 0.1449 - accuracy: 0.95 - ETA: 1s - loss: 0.1456 - accuracy: 0.95 - ETA: 1s - loss: 0.1453 - accuracy: 0.95 - ETA: 1s - loss: 0.1444 - accuracy: 0.95 - ETA: 1s - loss: 0.1472 - accuracy: 0.95 - ETA: 1s - loss: 0.1464 - accuracy: 0.95 - ETA: 1s - loss: 0.1479 - accuracy: 0.95 - ETA: 1s - loss: 0.1483 - accuracy: 0.95 - ETA: 1s - loss: 0.1462 - accuracy: 0.95 - ETA: 1s - loss: 0.1472 - accuracy: 0.95 - ETA: 0s - loss: 0.1478 - accuracy: 0.95 - ETA: 0s - loss: 0.1476 - accuracy: 0.95 - ETA: 0s - loss: 0.1461 - accuracy: 0.95 - ETA: 0s - loss: 0.1457 - accuracy: 0.95 - ETA: 0s - loss: 0.1469 - accuracy: 0.95 - ETA: 0s - loss: 0.1472 - accuracy: 0.95 - ETA: 0s - loss: 0.1476 - accuracy: 0.95 - ETA: 0s - loss: 0.1470 - accuracy: 0.95 - ETA: 0s - loss: 0.1469 - accuracy: 0.95 - 4s 103ms/step - loss: 0.1464 - accuracy: 0.9526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Predicting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 14 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 4s 99ms/step\n",
      "40/40 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing performance:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.90\n",
      "FLAT ROC AUC: 0.90\n",
      "       genes                                     hp_annotations  \\\n",
      "3800   65109  {HP:0100490, HP:0000274, HP:0003549, HP:001182...   \n",
      "2698  284111  {HP:0001098, HP:0030669, HP:0002063, HP:000736...   \n",
      "3532    7045  {HP:0000549, HP:0007827, HP:0000495, HP:000049...   \n",
      "58      8289  {HP:0002086, HP:0030669, HP:0005108, HP:000123...   \n",
      "3689    7355  {HP:0001098, HP:0030669, HP:0007369, HP:000057...   \n",
      "2       8195  {HP:0000512, HP:0000008, HP:0004383, HP:000079...   \n",
      "208      353  {HP:0002719, HP:0012622, HP:0003774, HP:000079...   \n",
      "3883   57167  {HP:0000008, HP:0030669, HP:0000544, HP:000115...   \n",
      "2647   30009  {HP:0002086, HP:0000433, HP:0000005, HP:010058...   \n",
      "938     1644  {HP:0001266, HP:0002019, HP:0000366, HP:000245...   \n",
      "251   254394  {HP:0000008, HP:0000858, HP:0010460, HP:000081...   \n",
      "391     8893  {HP:0008193, HP:0000008, HP:0001098, HP:000235...   \n",
      "1299   10342  {HP:0001098, HP:0006802, HP:0030188, HP:000254...   \n",
      "1372    2253  {HP:0000008, HP:0002086, HP:0000062, HP:000086...   \n",
      "1735   27445  {HP:0001098, HP:0030669, HP:0007369, HP:000017...   \n",
      "3178   22943  {HP:0002086, HP:0000613, HP:0006824, HP:000230...   \n",
      "541     1001  {HP:0001098, HP:0030669, HP:0100257, HP:000354...   \n",
      "3683    7343  {HP:0001251, HP:0001249, HP:0010993, HP:001196...   \n",
      "959     1678  {HP:0011474, HP:0000512, HP:0001098, HP:000208...   \n",
      "1222   51196  {HP:0012622, HP:0003676, HP:0003774, HP:001257...   \n",
      "2065    3785  {HP:0001266, HP:0002086, HP:0007369, HP:000235...   \n",
      "2590   29914  {HP:0007856, HP:0007759, HP:0000478, HP:001149...   \n",
      "3704    7390  {HP:0030669, HP:0002757, HP:0003549, HP:002533...   \n",
      "1851    3094  {HP:0100490, HP:0100257, HP:0003549, HP:000254...   \n",
      "3058    6390  {HP:0000008, HP:0030669, HP:0000544, HP:000354...   \n",
      "575     1071  {HP:0003124, HP:0010979, HP:0003077, HP:000310...   \n",
      "2329    4649  {HP:0002086, HP:0100285, HP:0006824, HP:003031...   \n",
      "3424    6892  {HP:0002087, HP:0002719, HP:0002086, HP:000043...   \n",
      "1280    2121  {HP:0000008, HP:0002086, HP:0010438, HP:001106...   \n",
      "1660    2707  {HP:0001824, HP:0005595, HP:0002817, HP:000115...   \n",
      "...      ...                                                ...   \n",
      "3617    7170  {HP:0005346, HP:0002086, HP:0012727, HP:000178...   \n",
      "574    91179  {HP:0002086, HP:0003100, HP:0003995, HP:003066...   \n",
      "3194    6578  {HP:0002086, HP:0000864, HP:0001231, HP:000189...   \n",
      "3435    6904  {HP:0001098, HP:0030669, HP:0007369, HP:000201...   \n",
      "3416    6878  {HP:0030669, HP:0000177, HP:0000366, HP:000042...   \n",
      "2102    3909  {HP:0100490, HP:0002086, HP:0008386, HP:000354...   \n",
      "2443    4998  {HP:0011863, HP:0100490, HP:0000237, HP:000310...   \n",
      "239     8604  {HP:0000817, HP:0001344, HP:0000735, HP:020013...   \n",
      "356      631  {HP:0010920, HP:0000478, HP:0100019, HP:000000...   \n",
      "1552    2556  {HP:0001824, HP:0002086, HP:0002019, HP:000292...   \n",
      "2419    4914  {HP:0010832, HP:0003549, HP:0011821, HP:000281...   \n",
      "3492   23387  {HP:0002086, HP:0002308, HP:0001892, HP:000201...   \n",
      "2550  259236  {HP:0000364, HP:0000005, HP:0000365, HP:000000...   \n",
      "40     57410  {HP:0001098, HP:0000570, HP:0100763, HP:000139...   \n",
      "2538    5261  {HP:0001394, HP:0001626, HP:0001408, HP:000129...   \n",
      "2304    4598  {HP:0001036, HP:0001098, HP:0002086, HP:003066...   \n",
      "1097    1889  {HP:0001824, HP:0010438, HP:0000436, HP:000201...   \n",
      "1032   50939  {HP:0000512, HP:0001098, HP:0007677, HP:000061...   \n",
      "2042    3737  {HP:0001098, HP:0030669, HP:0002063, HP:000736...   \n",
      "1949   85358  {HP:0002188, HP:0030669, HP:0010438, HP:000510...   \n",
      "1520    2516  {HP:0008193, HP:0000008, HP:0002086, HP:000006...   \n",
      "733    25914  {HP:0002536, HP:0001249, HP:0010993, HP:000024...   \n",
      "2009    3643  {HP:0001824, HP:0007495, HP:0000008, HP:000208...   \n",
      "1180   51142  {HP:0001288, HP:0001626, HP:0001337, HP:000201...   \n",
      "3195   63924  {HP:0000008, HP:0001732, HP:0040064, HP:000157...   \n",
      "3441    6911  {HP:0000008, HP:0002086, HP:0030669, HP:000036...   \n",
      "1344   10397  {HP:0001288, HP:0003134, HP:0003383, HP:004006...   \n",
      "527      974  {HP:0002086, HP:0030669, HP:0025426, HP:000354...   \n",
      "3197   63925  {HP:0002536, HP:0007369, HP:0003549, HP:001182...   \n",
      "1289    2137  {HP:0003995, HP:0000436, HP:0005003, HP:000115...   \n",
      "\n",
      "                                         go_annotations  \\\n",
      "3800  {GO:0008150, GO:0005737, GO:0005634, GO:001060...   \n",
      "2698  {GO:0006810, GO:0008150, GO:0005886, GO:000573...   \n",
      "3532  {GO:0044237, GO:0022610, GO:0007600, GO:001953...   \n",
      "58    {GO:1902680, GO:0008150, GO:0016514, GO:190350...   \n",
      "3689  {GO:0098588, GO:0006810, GO:0008150, GO:000588...   \n",
      "2     {GO:0030031, GO:0035735, GO:0008150, GO:000573...   \n",
      "208   {GO:0099503, GO:0008150, GO:0005737, GO:000563...   \n",
      "3883  {GO:0008150, GO:0005634, GO:0043233, GO:003198...   \n",
      "2647  {GO:1902680, GO:0008150, GO:1903508, GO:000098...   \n",
      "938   {GO:0044237, GO:1901362, GO:0018130, GO:190156...   \n",
      "251   {GO:0008150, GO:0005634, GO:0097373, GO:003303...   \n",
      "391   {GO:0048869, GO:0007417, GO:0008150, GO:000641...   \n",
      "1299  {GO:0098588, GO:0006810, GO:0008150, GO:000588...   \n",
      "1372  {GO:0048869, GO:0008150, GO:0060429, GO:000716...   \n",
      "1735  {GO:0005938, GO:0005737, GO:0044456, GO:009973...   \n",
      "3178  {GO:0098588, GO:0060429, GO:0050808, GO:000173...   \n",
      "541   {GO:0048869, GO:0022610, GO:0044848, GO:000815...   \n",
      "3683  {GO:0065004, GO:0008150, GO:0005634, GO:000106...   \n",
      "959   {GO:0008104, GO:0006810, GO:0008150, GO:000573...   \n",
      "1222  {GO:0008150, GO:0005737, GO:0051480, GO:000716...   \n",
      "2065  {GO:0022890, GO:0071805, GO:0006810, GO:000815...   \n",
      "2590  {GO:0044237, GO:1990748, GO:0098588, GO:004428...   \n",
      "3704  {GO:1901362, GO:0044237, GO:0043436, GO:190156...   \n",
      "1851  {GO:0008150, GO:0005737, GO:0005634, GO:000916...   \n",
      "3058  {GO:0044237, GO:0008150, GO:0005886, GO:000573...   \n",
      "575   {GO:0048869, GO:0008150, GO:1905954, GO:003435...   \n",
      "2329  {GO:0048869, GO:0035556, GO:0007600, GO:000815...   \n",
      "3424  {GO:0098588, GO:0008150, GO:0005737, GO:000013...   \n",
      "1280  {GO:0098588, GO:0008150, GO:0005886, GO:003299...   \n",
      "1660  {GO:0005575, GO:0043226, GO:0030054, GO:004322...   \n",
      "...                                                 ...   \n",
      "3617  {GO:0008150, GO:0005865, GO:0005737, GO:004264...   \n",
      "574                                                  {}   \n",
      "3194  {GO:0005575, GO:0006810, GO:0005215, GO:000815...   \n",
      "3435  {GO:0048869, GO:0008150, GO:0032886, GO:000573...   \n",
      "3416  {GO:1902680, GO:0008150, GO:1903508, GO:000573...   \n",
      "2102  {GO:0048869, GO:0001705, GO:0035987, GO:000815...   \n",
      "2443  {GO:0008150, GO:0005737, GO:0005634, GO:005125...   \n",
      "239   {GO:0008150, GO:0008509, GO:0005737, GO:005511...   \n",
      "356   {GO:0008150, GO:0005198, GO:0009987, GO:000699...   \n",
      "1552  {GO:0008503, GO:0005575, GO:0042165, GO:000588...   \n",
      "2419  {GO:0048869, GO:0010465, GO:0120035, GO:009858...   \n",
      "3492  {GO:0044237, GO:0035556, GO:0019538, GO:000679...   \n",
      "2550                                                 {}   \n",
      "40    {GO:0044433, GO:0098588, GO:0006810, GO:000815...   \n",
      "2538  {GO:0031331, GO:0008150, GO:0005737, GO:005511...   \n",
      "2304  {GO:0044237, GO:1901362, GO:0004496, GO:004428...   \n",
      "1097  {GO:0099503, GO:0098588, GO:0008150, GO:000573...   \n",
      "1032  {GO:0007600, GO:0005575, GO:0008150, GO:000520...   \n",
      "2042  {GO:0022890, GO:0071805, GO:0006810, GO:000815...   \n",
      "1949  {GO:0048869, GO:0008150, GO:0005737, GO:004873...   \n",
      "1520  {GO:1902680, GO:0008150, GO:1903508, GO:000098...   \n",
      "733   {GO:0005815, GO:0005886, GO:0032991, GO:003606...   \n",
      "2009  {GO:0045840, GO:0098588, GO:0045821, GO:190169...   \n",
      "1180  {GO:1902680, GO:0008150, GO:0070482, GO:190350...   \n",
      "3195  {GO:0030031, GO:0008150, GO:0005737, GO:012003...   \n",
      "3441  {GO:0009888, GO:0008150, GO:0032502, GO:004885...   \n",
      "1344  {GO:0098588, GO:0008150, GO:0070482, GO:000573...   \n",
      "527   {GO:0008150, GO:0002376, GO:0005886, GO:005079...   \n",
      "3197  {GO:0048869, GO:0007417, GO:0008150, GO:000563...   \n",
      "1289  {GO:0044237, GO:0019538, GO:1901566, GO:000815...   \n",
      "\n",
      "                                        iea_annotations  \\\n",
      "3800  {GO:0008150, GO:0005737, GO:0005634, GO:001060...   \n",
      "2698  {GO:0008150, GO:0008509, GO:0005737, GO:000563...   \n",
      "3532  {GO:0048869, GO:0022610, GO:0008150, GO:000573...   \n",
      "58    {GO:0048869, GO:0030900, GO:0045814, GO:000741...   \n",
      "3689  {GO:0098588, GO:0006810, GO:0005459, GO:000815...   \n",
      "2     {GO:0030031, GO:0007389, GO:0060429, GO:005091...   \n",
      "208   {GO:0099503, GO:0008150, GO:1901699, GO:000573...   \n",
      "3883  {GO:1902680, GO:0060429, GO:0008150, GO:000200...   \n",
      "2647  {GO:0007389, GO:0060429, GO:1903508, GO:000098...   \n",
      "938   {GO:0007568, GO:0099503, GO:0008150, GO:004429...   \n",
      "251   {GO:0008150, GO:0005634, GO:0048609, GO:009737...   \n",
      "391   {GO:0048869, GO:0030900, GO:0007568, GO:000741...   \n",
      "1299  {GO:0098588, GO:0008150, GO:0051091, GO:000573...   \n",
      "1372  {GO:0045840, GO:0007389, GO:0045745, GO:006042...   \n",
      "1735  {GO:0008150, GO:0043954, GO:0050808, GO:000573...   \n",
      "3178  {GO:0120035, GO:0007389, GO:0098588, GO:000173...   \n",
      "541   {GO:0048869, GO:0022610, GO:0044848, GO:000815...   \n",
      "3683  {GO:0065004, GO:1902680, GO:0008150, GO:190350...   \n",
      "959   {GO:0008104, GO:0006810, GO:0008150, GO:004316...   \n",
      "1222  {GO:0098588, GO:0008150, GO:0007166, GO:000573...   \n",
      "2065  {GO:0008150, GO:0022832, GO:0015075, GO:004300...   \n",
      "2590  {GO:0098588, GO:0008150, GO:1901661, GO:000573...   \n",
      "3704  {GO:0044237, GO:1901362, GO:0043436, GO:190156...   \n",
      "1851  {GO:0008150, GO:0005737, GO:0005634, GO:000916...   \n",
      "3058  {GO:0008150, GO:0005737, GO:0055114, GO:000563...   \n",
      "575   {GO:0048869, GO:0008150, GO:1905954, GO:003435...   \n",
      "2329  {GO:0048869, GO:0120035, GO:0008150, GO:006042...   \n",
      "3424  {GO:0098588, GO:0008150, GO:0005737, GO:000013...   \n",
      "1280  {GO:0007224, GO:0098588, GO:0005815, GO:000815...   \n",
      "1660  {GO:0006810, GO:1901700, GO:0032526, GO:000815...   \n",
      "...                                                 ...   \n",
      "3617  {GO:0008150, GO:0005865, GO:0070062, GO:000573...   \n",
      "574   {GO:0022610, GO:0006810, GO:0008150, GO:000591...   \n",
      "3194  {GO:0006810, GO:0015718, GO:0008150, GO:000686...   \n",
      "3435  {GO:0048869, GO:0022610, GO:0008150, GO:003433...   \n",
      "3416  {GO:0065004, GO:1902680, GO:0008150, GO:190350...   \n",
      "2102  {GO:0048869, GO:0022610, GO:0008150, GO:000173...   \n",
      "2443  {GO:1901988, GO:0008150, GO:0005737, GO:000563...   \n",
      "239   {GO:0008150, GO:0008509, GO:0005737, GO:005511...   \n",
      "356   {GO:0048869, GO:0005938, GO:0060429, GO:000206...   \n",
      "1552  {GO:0008150, GO:0008509, GO:0007166, GO:002283...   \n",
      "2419  {GO:0120035, GO:0010465, GO:0098588, GO:006042...   \n",
      "3492  {GO:0008150, GO:0005737, GO:0007165, GO:000646...   \n",
      "2550  {GO:0007600, GO:0008150, GO:0048731, GO:000300...   \n",
      "40    {GO:0048869, GO:0007417, GO:0098588, GO:000815...   \n",
      "2538  {GO:0031331, GO:0008150, GO:0005737, GO:005511...   \n",
      "2304  {GO:0008150, GO:0005737, GO:1901617, GO:001921...   \n",
      "1097  {GO:0099503, GO:0098588, GO:0008150, GO:000573...   \n",
      "1032  {GO:0007600, GO:0008150, GO:0005201, GO:003609...   \n",
      "2042  {GO:0097623, GO:0008150, GO:0044297, GO:002155...   \n",
      "1949  {GO:0120035, GO:0098588, GO:0050808, GO:000716...   \n",
      "1520  {GO:0048869, GO:0007568, GO:1902680, GO:000815...   \n",
      "733   {GO:0007389, GO:0005815, GO:0009799, GO:000815...   \n",
      "2009  {GO:0045840, GO:0098588, GO:0045821, GO:005080...   \n",
      "1180  {GO:1902680, GO:0008150, GO:0070482, GO:190350...   \n",
      "3195  {GO:0030031, GO:0008150, GO:0005886, GO:000573...   \n",
      "3441  {GO:0048869, GO:0120035, GO:0007389, GO:000057...   \n",
      "1344  {GO:0048869, GO:0098588, GO:0008150, GO:009917...   \n",
      "527   {GO:0048869, GO:0008150, GO:0002764, GO:000716...   \n",
      "3197  {GO:0048869, GO:0030900, GO:0007417, GO:000815...   \n",
      "1289  {GO:0008150, GO:0005737, GO:0007165, GO:000698...   \n",
      "\n",
      "                                     deepgo_annotations  \\\n",
      "3800  [GO:0000184|0.563, GO:0000375|0.596, GO:000037...   \n",
      "2698  [GO:0003333|0.046, GO:0003674|0.653, GO:000521...   \n",
      "3532  [GO:0001525|0.174, GO:0001568|0.174, GO:000194...   \n",
      "58    [GO:0000003|0.224, GO:0000075|0.011, GO:000007...   \n",
      "3689  [GO:0000139|0.385, GO:0003674|0.645, GO:000521...   \n",
      "2     [GO:0000003|0.595, GO:0000226|0.861, GO:000110...   \n",
      "208   [GO:0000166|0.285, GO:0000287|0.041, GO:000177...   \n",
      "3883  [GO:0000003|0.072, GO:0000118|0.046, GO:000012...   \n",
      "2647  [GO:0000003|0.073, GO:0000018|0.065, GO:000012...   \n",
      "938   [GO:0000096|0.006, GO:0000098|0.006, GO:000030...   \n",
      "251   [GO:0000003|0.151, GO:0000082|0.047, GO:000022...   \n",
      "391   [GO:0000003|0.173, GO:0001541|0.12, GO:0002181...   \n",
      "1299  [GO:0000139|0.547, GO:0003674|0.865, GO:000548...   \n",
      "1372  [GO:0000003|0.534, GO:0000165|0.565, GO:000022...   \n",
      "1735  [GO:0000003|0.026, GO:0000149|0.009, GO:000027...   \n",
      "3178  [GO:0000122|0.345, GO:0000165|0.298, GO:000090...   \n",
      "541   [GO:0000003|0.069, GO:0000165|0.039, GO:000022...   \n",
      "3683  [GO:0000003|0.043, GO:0000183|0.156, GO:000097...   \n",
      "959   [GO:0003008|0.091, GO:0003674|0.57, GO:0005488...   \n",
      "1222  [GO:0000003|0.084, GO:0000165|0.578, GO:000018...   \n",
      "2065  [GO:0000003|0.023, GO:0000302|0.004, GO:000032...   \n",
      "2590  [GO:0000139|0.427, GO:0001885|0.162, GO:000206...   \n",
      "3704  [GO:0003674|0.93, GO:0003824|0.881, GO:0004852...   \n",
      "1851  [GO:0000118|0.115, GO:0003674|0.683, GO:000382...   \n",
      "3058  [GO:0000104|0.087, GO:0003674|0.485, GO:000382...   \n",
      "575   [GO:0003674|0.936, GO:0005215|0.638, GO:000531...   \n",
      "2329  [GO:0000003|0.135, GO:0000146|0.214, GO:000016...   \n",
      "3424  [GO:0000139|0.343, GO:0002376|0.747, GO:000239...   \n",
      "1280  [GO:0001501|0.313, GO:0003416|0.192, GO:000557...   \n",
      "1660  [GO:0000003|0.233, GO:0000139|0.019, GO:000030...   \n",
      "...                                                 ...   \n",
      "3617  [GO:0000302|0.057, GO:0000902|0.057, GO:000170...   \n",
      "574   [GO:0003674|0.74, GO:0004888|0.138, GO:0005044...   \n",
      "3194  [GO:0000323|0.031, GO:0001775|0.014, GO:000188...   \n",
      "3435  [GO:0000003|0.168, GO:0000226|0.587, GO:000027...   \n",
      "3416  [GO:0000003|0.054, GO:0000123|0.238, GO:000012...   \n",
      "2102  [GO:0000003|0.069, GO:0000226|0.05, GO:0000768...   \n",
      "2443  [GO:0000003|0.048, GO:0000070|0.028, GO:000007...   \n",
      "239   [GO:0000002|0.02, GO:0000959|0.007, GO:0001558...   \n",
      "356   [GO:0001654|0.183, GO:0002064|0.183, GO:000208...   \n",
      "1552  [GO:0001505|0.028, GO:0001662|0.023, GO:000220...   \n",
      "2419  [GO:0000003|0.088, GO:0000139|0.208, GO:000016...   \n",
      "3492  [GO:0000003|0.018, GO:0000075|0.011, GO:000007...   \n",
      "2550  [GO:0003008|0.544, GO:0007275|0.725, GO:000742...   \n",
      "40    [GO:0005575|0.995, GO:0005622|0.992, GO:000562...   \n",
      "2538  [GO:0000003|0.059, GO:0000082|0.067, GO:000008...   \n",
      "2304  [GO:0000166|0.15, GO:0000287|0.127, GO:0003674...   \n",
      "1097  [GO:0000003|0.065, GO:0000139|0.09, GO:0000165...   \n",
      "1032  [GO:0003008|0.317, GO:0003674|0.777, GO:000519...   \n",
      "2042  [GO:0000003|0.035, GO:0000082|0.019, GO:000027...   \n",
      "1949  [GO:0000003|0.011, GO:0000165|0.144, GO:000016...   \n",
      "1520  [GO:0000003|0.449, GO:0000122|0.119, GO:000022...   \n",
      "733   [GO:0005575|0.757, GO:0005622|0.732, GO:000562...   \n",
      "2009  [GO:0000003|0.444, GO:0000045|0.011, GO:000013...   \n",
      "1180  [GO:0001666|0.383, GO:0003674|0.826, GO:000367...   \n",
      "3195  [GO:0000228|0.02, GO:0000737|0.109, GO:0000785...   \n",
      "3441  [GO:0000122|0.176, GO:0000228|0.211, GO:000057...   \n",
      "1344  [GO:0000165|0.116, GO:0001655|0.048, GO:000166...   \n",
      "527   [GO:0002253|0.225, GO:0002376|0.731, GO:000242...   \n",
      "3197  [GO:0000902|0.348, GO:0000976|0.265, GO:000097...   \n",
      "1289  [GO:0000003|0.044, GO:0000139|0.115, GO:000027...   \n",
      "\n",
      "                                              sequences  \\\n",
      "3800  MKEEKEHRPKEKRVTLLTPAGATGSGGGTSGDSSKGEDKQDRNKEK...   \n",
      "2698  MASALSYVSKFKSFVILFVTPLLLLPLVILMPAKFVRCAYVIILMA...   \n",
      "3532  MALFVRLLALALALALGPAATLAGPAKSPYQLVLQHSRLRGRQHGP...   \n",
      "58    MAAQVAPAAASSLGNPPPPPPSELKKAEQQQREEAGGEAAAAAAAE...   \n",
      "3689  MAAVGAGGSTAAPGPGAVSAGALEPGTASAAHRRLKYISLAVLVVQ...   \n",
      "2     MSRLEAKKPSLCKSEPLTTERVRTTLSVLKRIVTSCYGPSGRLKQL...   \n",
      "208   MADSELQLVEQRIRSFPDFPTPGVVFRDISPVLKDPASFRAAIGLL...   \n",
      "3883  MSRRKQAKPQHINSEEDQGEQQPQQQTPEFADAAPAAPAAGELGAP...   \n",
      "2647  MGIVEPGCGDMLTGTEPMPGSDEGRAPGADPQHRYFYPEPGAQDAD...   \n",
      "938   MNASEFRRRGKEMVDYMANYMEGIEGRQVYPDVEPGYLRPLIPAAA...   \n",
      "251   MNSDQVTLVGQVFESYVSEYHKNDILLILKERDEDAHYPVVVNAMT...   \n",
      "391   MAAPVVAPPGVVVSRANKRSGAGPGGSGGGGARGAEEEPPPPLQAV...   \n",
      "1299  MNGQLDLSGKLIIKAQLGEDIRRIPIHNEDITYDELVLMMQRVFRG...   \n",
      "1372  MGSPRSALSCLLLHLLVLCLQAQEGPGRGPALGRELASLFRAGREP...   \n",
      "1735  MGNEASLEGEGLPEGLAAAAAAGGGASGAGSPSHTAIPAGMEADLS...   \n",
      "3178  MMALGAAGATRVFVAMVAAALGGHPLLGVSATLNSVLNSNAIKNLP...   \n",
      "541   MGLPRGPLASLLLLQVCWLQCAASEPCRAVFREAEVTLEAGGAEQE...   \n",
      "3683  MNGEADCPTDLEMAAPKGQDRWSQEDMLTLLECMKNNLPSNDSSKF...   \n",
      "959   MDSSSSSSAAGLGAVDPQLQHFIEVETQKQRFQQLVHQMTELCWEK...   \n",
      "1222  MTSEEMTASVLIPVTQRKVVSAQSAADESSEKVSDINISKAHTVRR...   \n",
      "2065  MVQKSRNGGVYPGPSGEKKLKVGFVGLDPGAPDSTRDGALLIAGSE...   \n",
      "2590  MAASQVLGEKINILSGETVKAGDRDPLGNDCPEQDRLPQRSWRQKC...   \n",
      "3704  MKVLLLKDAKEDDCGQDPYIRELGLYGLEATLIPVLSFEFLSLPSF...   \n",
      "1851  MADEIAKAQVARPGGDTIFGKIIRKEIPAKIIFEDDRCLAFHDISP...   \n",
      "3058  MAAVVALSLRRRLPATTLGGACLQASRGAQTAAATAPRIKKFAIYR...   \n",
      "575   MLAATVLTLALLGNAHACSKGTSHEAGIVCRITKPALLVLNHETAK...   \n",
      "2329  MNINDGGRRRFEDNEHTLRIYPGAISEGTIYCPIPARKNSTAAEVI...   \n",
      "3424  MKSLSLLLAVALGLATAVSAGPAVIECWFVEDASGKGLAKRPGALL...   \n",
      "1280  MARGGAACKSDARLLLGRDALRPAPALLAPAVLLGAALGLGLGLWL...   \n",
      "1660  MDWKTLQALLSGVNKYSTAFGRIWLSVVFVFRVLVYVVAAERVWGD...   \n",
      "...                                                 ...   \n",
      "3617  MMEAIKKKMQMLKLDKENALDRAEQAEAEQKQAEERSKQLEDELAA...   \n",
      "574   MEGAGPRGAGPARRRGAGGPPSPLLPSLLLLLLLWMLPDTVAPQEL...   \n",
      "3194  MGLLPKLGASQGSDTSTSRAGRCARSVFGNIKVFVLCQGLLQLCQL...   \n",
      "3435  MALSDEPAAGGPEEEAEDETLAFGAALEAFGESAETRALLGRLREV...   \n",
      "3416  MAEEKKLKLSNTVLPSESMKVVAESMGIAQIQEETCQLLTDEVSYR...   \n",
      "2102  MAAAARPRGRALGPVLPPTPLLLLVLRVLPACGATARDPGAAAGLS...   \n",
      "2443  MAHYPTRLKTRKTYSWVGRPLLDRKLHYQTYREMCVKTEGCSTEIH...   \n",
      "239   MAVKVQTTKRGDPHELRNIFLQYASTEVDGERYMTPEDFVQRYLGL...   \n",
      "356   MYRRSYVFQTRKEQYEHADEASRAAEPERPADEGWAGATSLAALQG...   \n",
      "1552  MIITQTSHCYMTSLGILFLINILPGTTGQGESRRQEPGDFVKQDIG...   \n",
      "2419  MLRGGRRGQLGWHSWAAGPGSLLAWLILASAGAAPCPDACCPHGSS...   \n",
      "3492  MAAAAASGAGGAAGAGTGGAGPAGRLLPPPAPGSPAAPAAVSPAAG...   \n",
      "2550  MAGWPGAGPLCVLGGAALGVCLAGVAGQLVEPSTAPPKPKPPPLTK...   \n",
      "40    MWFFARDPVRDFPFELIPEPPEGGLPGPWALHRGRKKATGSPVSIF...   \n",
      "2538  MTLDVGPEDELPDWAAAKEFYQKYDPKDVIGRGVSSVVRRCVHRAT...   \n",
      "2304  MLSEVLLVSAPGKVILHGEHAVVHGKVALAVSLNLRTFLRLQPHSN...   \n",
      "1097  MRGVWPPPVSALLSALGMSTYKRATLDEEDLVDSLSEGDAYPNGLQ...   \n",
      "1032  MIMFPLFGKISLGILIFVLIEGDFPSLTAQTYLSIEEIQEPKSAVS...   \n",
      "2042  MTVATGDPADEAAALPGHPQDTYDPEADHECCERVVINISGLRFET...   \n",
      "1949  MDGPGASAVVVRVGIPDLQQTKCLRLDPAAPVWAAKQRVLCALNHS...   \n",
      "1520  MDYSYDEDLDELCPVCGDKVSGYHYGLLTCESCKGFFKRTVQNNKH...   \n",
      "733   MVLAGLIRKLGHQLAEIRERALKSILCKIEHNLICYADLIQERQLF...   \n",
      "2009  MATGGRRGAAAAPLLVAVAALLLGAAGHLYPGEVCPGMDIRNNLTR...   \n",
      "1180  MPRGSRSRTSRMAPPASRAPQMRAAPRPAPVAQPPAAAPPSAVGSS...   \n",
      "3195  MEYAMKSLSLLYPKSLSRHVSVRTSVVTQQLLSEPSPKAPRARPCR...   \n",
      "3441  MYHPRELYPSLGAGYRLGPAQPGADSSFPPALAEGYRYPELDTPKL...   \n",
      "1344  MSREMQDVDLAEVKPLVEKGETITGLLQEFDVQEQDIETLHGSVHV...   \n",
      "527   MARLALSPVPSHWMVALLLLLSAEPVPAARSEDRYRNPKGSACSRI...   \n",
      "3197  MEENEVESSSDAAPGPGRPEEPSESGLGVGTSEAVSADSSDAAAAP...   \n",
      "1289  MTGYTMLRNGGAGNGGQTCMLRWSNRIRLTWLSFTLFVILVFFPLI...   \n",
      "\n",
      "                                            expressions  \\\n",
      "3800  [0.2777778, 0.33333334, 0.2962963, 1.0, 0.2222...   \n",
      "2698  [0.018867925, 0.018867925, 0.003144654, 0.0025...   \n",
      "3532  [0.0024600246, 0.0012300123, 0.007380074, 0.00...   \n",
      "58    [0.2, 0.23076923, 0.24615385, 1.0, 0.41538462,...   \n",
      "3689  [0.22580644, 0.29032257, 0.29032257, 0.6774193...   \n",
      "2     [0.5, 0.71428573, 0.39285713, 0.4642857, 0.428...   \n",
      "208   [0.11235955, 0.11797753, 0.15730338, 1.0, 0.60...   \n",
      "3883  [0.009090909, 0.009090909, 0.018181818, 0.0, 0...   \n",
      "2647  [0.0043478264, 0.0043478264, 0.02173913, 0.782...   \n",
      "938   [0.0021739132, 0.0021739132, 0.0021739132, 0.0...   \n",
      "251   [0.12857142, 0.14285715, 0.2857143, 0.5714286,...   \n",
      "391   [0.25, 0.33333334, 0.31666666, 0.75, 0.4333333...   \n",
      "1299  [0.2, 0.23809524, 0.2857143, 0.7904762, 0.4285...   \n",
      "1372  [0.3, 0.3, 0.05, 0.4, 0.0, 0.1, 0.05, 0.0, 0.0...   \n",
      "1735  [0.16666667, 0.25, 0.083333336, 0.0, 0.0277777...   \n",
      "3178  [0.0009708738, 0.0009708738, 0.00038834952, 9....   \n",
      "541   [0.020833334, 0.041666668, 0.018749999, 0.0062...   \n",
      "3683  [0.30666667, 0.37333333, 0.36, 0.81333333, 0.4...   \n",
      "959   [0.1764706, 0.23529412, 0.1764706, 1.0, 0.2352...   \n",
      "1222  [0.11111111, 0.11111111, 0.11111111, 0.0370370...   \n",
      "2065  [0.32142857, 0.37244898, 0.025510205, 0.001020...   \n",
      "2590  [0.1764706, 0.23529412, 0.5294118, 0.88235295,...   \n",
      "3704  [0.88461536, 0.9230769, 0.42307693, 0.88461536...   \n",
      "1851  [0.42068964, 0.50574714, 0.29655173, 1.0, 0.42...   \n",
      "3058  [0.34188035, 0.41880342, 0.34188035, 0.982906,...   \n",
      "575   [0.008, 0.008, 0.016, 0.2, 0.06, 0.006, 0.0139...   \n",
      "2329  [0.2, 0.25, 0.15, 0.45, 0.15, 0.15, 0.5, 0.15,...   \n",
      "3424  [0.15238096, 0.17142858, 0.10952381, 0.7571428...   \n",
      "1280  [0.02, 0.02, 0.016, 0.016, 0.12, 0.02, 0.66, 0...   \n",
      "1660  [0.002072539, 0.0025906735, 0.00051813474, 0.0...   \n",
      "...                                                 ...   \n",
      "3617  [0.066755675, 0.07209613, 0.06275033, 0.287049...   \n",
      "574   [0.033333335, 0.044444446, 0.022222223, 0.0055...   \n",
      "3194  [0.0041841003, 0.008368201, 0.008368201, 0.000...   \n",
      "3435  [0.21276596, 0.27659574, 0.19148937, 0.4255319...   \n",
      "3416  [0.4090909, 0.5, 0.18181819, 0.54545456, 0.5, ...   \n",
      "2102  [0.0625, 0.0625, 0.0625, 0.009375, 0.03125, 0....   \n",
      "2443  [0.0057142857, 0.008571429, 0.008571429, 1.0, ...   \n",
      "239   [0.40384614, 0.6730769, 0.23076923, 0.5576923,...   \n",
      "356   [0.2857143, 0.42857143, 0.71428573, 0.0, 0.142...   \n",
      "1552  [0.8, 1.0, 0.05, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0,...   \n",
      "2419  [0.04, 0.04, 0.060000002, 0.02, 0.16, 0.04, 0....   \n",
      "3492  [0.3809524, 0.47619048, 0.52380955, 0.30952382...   \n",
      "2550  [0.45454547, 0.54545456, 0.45454547, 0.0, 0.09...   \n",
      "40    [0.37209302, 0.5, 0.29069766, 0.6744186, 0.779...   \n",
      "2538  [0.085365854, 0.1097561, 0.085365854, 0.158536...   \n",
      "2304  [0.25, 0.25, 0.6666667, 0.6666667, 0.45833334,...   \n",
      "1097  [0.09259259, 0.09259259, 0.08024691, 0.0679012...   \n",
      "1032  [0.1, 0.2, 0.2, 0.2, 0.2, 0.1, 0.4, 0.2, 0.1, ...   \n",
      "2042  [0.36666667, 0.6333333, 0.06666667, 0.01666666...   \n",
      "1949  [0.23381294, 0.24820144, 0.09352518, 0.0007194...   \n",
      "1520  [0.00035335688, 0.00035335688, 0.0, 0.00070671...   \n",
      "733   [0.045454547, 0.09090909, 0.22727273, 1.0, 0.0...   \n",
      "2009  [0.07070707, 0.09090909, 0.13131313, 0.2222222...   \n",
      "1180  [0.34213686, 0.3877551, 0.3037215, 1.0, 0.8583...   \n",
      "3195  [0.00023584906, 0.00023584906, 0.00023584906, ...   \n",
      "3441  [0.083333336, 0.083333336, 0.16666667, 0.25, 0...   \n",
      "1344  [0.05740741, 0.087037034, 0.43333334, 0.009259...   \n",
      "527   [0.0013953489, 0.0013953489, 0.0023255814, 0.8...   \n",
      "3197  [0.25714287, 0.2857143, 0.2, 0.6, 0.37142858, ...   \n",
      "1289  [0.5263158, 0.68421054, 0.47368422, 0.2631579,...   \n",
      "\n",
      "                                                  preds  \n",
      "3800  [0.18848793, 0.037430957, 0.1480441, 0.0560620...  \n",
      "2698  [0.18848805, 0.037431024, 0.14804427, 0.056062...  \n",
      "3532  [0.18848795, 0.03743097, 0.14804412, 0.0560620...  \n",
      "58    [0.18848793, 0.037430957, 0.14804408, 0.056062...  \n",
      "3689  [0.18848802, 0.03743101, 0.14804421, 0.0560621...  \n",
      "2     [0.18848793, 0.037430957, 0.1480441, 0.0560620...  \n",
      "208   [0.18848793, 0.037430964, 0.14804411, 0.056062...  \n",
      "3883  [0.18848793, 0.037430957, 0.14804408, 0.056062...  \n",
      "2647  [0.18848793, 0.037430957, 0.14804408, 0.056062...  \n",
      "938   [0.18848793, 0.037430964, 0.14804411, 0.056062...  \n",
      "251   [0.18848793, 0.037430957, 0.14804411, 0.056062...  \n",
      "391   [0.18848793, 0.037430957, 0.14804408, 0.056062...  \n",
      "1299  [0.18848793, 0.037430957, 0.1480441, 0.0560620...  \n",
      "1372  [0.1884879, 0.037430957, 0.14804406, 0.0560620...  \n",
      "1735  [0.18848793, 0.037430957, 0.1480441, 0.0560620...  \n",
      "3178  [0.18848793, 0.037430957, 0.14804408, 0.056062...  \n",
      "541   [0.18848793, 0.037430957, 0.14804408, 0.056062...  \n",
      "3683  [0.18848793, 0.037430957, 0.14804408, 0.056062...  \n",
      "959   [0.18848805, 0.037431035, 0.14804424, 0.056062...  \n",
      "1222  [0.18848793, 0.037430957, 0.14804408, 0.056062...  \n",
      "2065  [0.18848799, 0.03743098, 0.14804415, 0.0560620...  \n",
      "2590  [0.18848799, 0.03743099, 0.14804415, 0.0560620...  \n",
      "3704  [0.18848802, 0.037430998, 0.1480442, 0.0560621...  \n",
      "1851  [0.18848793, 0.037430957, 0.1480441, 0.0560620...  \n",
      "3058  [0.18848802, 0.03743101, 0.1480442, 0.05606211...  \n",
      "575   [0.18848795, 0.03743097, 0.14804412, 0.0560620...  \n",
      "2329  [0.18848793, 0.037430957, 0.1480441, 0.0560620...  \n",
      "3424  [0.18848793, 0.037430964, 0.14804411, 0.056062...  \n",
      "1280  [0.18848799, 0.03743098, 0.14804414, 0.0560620...  \n",
      "1660  [0.18848802, 0.037430998, 0.1480442, 0.0560621...  \n",
      "...                                                 ...  \n",
      "3617  [0.18848813, 0.037431076, 0.14804435, 0.056062...  \n",
      "574   [0.18848822, 0.03743113, 0.14804442, 0.0560622...  \n",
      "3194  [0.18848982, 0.037431993, 0.14804626, 0.056063...  \n",
      "3435  [0.18848793, 0.037430957, 0.1480441, 0.0560620...  \n",
      "3416  [0.18848793, 0.037430957, 0.14804408, 0.056062...  \n",
      "2102  [0.18848793, 0.037430964, 0.1480441, 0.0560620...  \n",
      "2443  [0.18848793, 0.037430957, 0.1480441, 0.0560620...  \n",
      "239   [0.18848793, 0.037430964, 0.14804411, 0.056062...  \n",
      "356   [0.18848819, 0.037431113, 0.14804442, 0.056062...  \n",
      "1552  [0.18848799, 0.03743099, 0.14804417, 0.0560621...  \n",
      "2419  [0.18848793, 0.037430957, 0.14804408, 0.056062...  \n",
      "3492  [0.18848793, 0.037430964, 0.14804411, 0.056062...  \n",
      "2550  [0.18849026, 0.0374322, 0.14804669, 0.05606374...  \n",
      "40    [0.18848793, 0.037430957, 0.1480441, 0.0560620...  \n",
      "2538  [0.18848793, 0.037430957, 0.1480441, 0.0560620...  \n",
      "2304  [0.18848793, 0.037430957, 0.14804411, 0.056062...  \n",
      "1097  [0.18848793, 0.037430957, 0.1480441, 0.0560620...  \n",
      "1032  [0.18849616, 0.037435267, 0.14805292, 0.056067...  \n",
      "2042  [0.18848793, 0.037430964, 0.14804411, 0.056062...  \n",
      "1949  [0.18848793, 0.037430957, 0.14804408, 0.056062...  \n",
      "1520  [0.18848793, 0.037430957, 0.14804408, 0.056062...  \n",
      "733   [0.18848799, 0.03743099, 0.14804417, 0.0560620...  \n",
      "2009  [0.18848793, 0.037430957, 0.1480441, 0.0560620...  \n",
      "1180  [0.18848793, 0.037430957, 0.14804408, 0.056062...  \n",
      "3195  [0.18848805, 0.037431035, 0.14804427, 0.056062...  \n",
      "3441  [0.1884879, 0.037430957, 0.14804408, 0.0560620...  \n",
      "1344  [0.18848793, 0.03743097, 0.1480441, 0.05606205...  \n",
      "527   [0.18848795, 0.03743097, 0.14804411, 0.0560620...  \n",
      "3197  [0.18848793, 0.037430964, 0.1480441, 0.0560620...  \n",
      "1289  [0.18848795, 0.03743097, 0.14804411, 0.0560620...  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[394 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saving predictions\n"
     ]
    }
   ],
   "source": [
    "# main(hp_file, data_file, terms_file, gos_file, model_file,out_file, fold, batch_size, epochs, load, logger_file, threshold,device):\n",
    "\n",
    "main('data/hp.obo',\n",
    "     \n",
    "     'data/My_Implementations/human.pkl',\n",
    "     \n",
    "     'data/all_terms.pkl',\n",
    "     \n",
    "     'data/My_Implementations/gos.pkl',\n",
    "     \n",
    "     'data/My_Implementations/Trial_7/model_mohamed.h5',\n",
    "     \n",
    "     'data/My_Implementations/Trial_7/predictions.pkl',\n",
    "     \n",
    "     1,\n",
    "#batch_size\n",
    "     10,\n",
    "#Number of epochs     \n",
    "     1024,\n",
    "     \n",
    "     'load',\n",
    "                                   \n",
    "     'data/My_Implementations/Trial_7/training.csv',\n",
    "     \n",
    "     0.5,\n",
    "    \n",
    "     'GPU:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial_evaluating_onTrial_7_with_40% from the dataset=testset_12-8-2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {'input_shape': (24274,), 'nb_layers': 1, 'loss': 'binary_crossentropy', 'rate': 0.3, 'learning_rate': 0.001, 'units': 1500, 'model_file': 'data/My_Implementations/Trial_7/model_mohamed.h5'}\n",
      "Phenotypes 2600\n",
      "\n",
      "1966 1572 393\n",
      "3933 1966 1572 393\n",
      "2600\n",
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:h5py._conv:Creating converter from 3 to 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "INFO:root:Evaluating model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 33s 51ms/step - loss: 0.1454\n",
      "40/40 [==============================] - 139s 3s/step - loss: 0.1467 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Predicting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 141s 3s/step\n",
      "40/40 [==============================] - 2s 55ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing performance:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saving predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAT ROC AUC: 0.90\n",
      "       genes                                     hp_annotations  \\\n",
      "2698  284111  {HP:0100547, HP:0002355, HP:0012444, HP:000629...   \n",
      "3532    7045  {HP:0000118, HP:0000005, HP:0007802, HP:000354...   \n",
      "58      8289  {HP:0002817, HP:0100547, HP:0009115, HP:000199...   \n",
      "3689    7355  {HP:0100547, HP:0012444, HP:0000574, HP:000000...   \n",
      "2       8195  {HP:0010460, HP:0002817, HP:0010438, HP:000866...   \n",
      "...      ...                                                ...   \n",
      "3441    6911  {HP:0010460, HP:0100547, HP:0009115, HP:000339...   \n",
      "1344   10397  {HP:0002817, HP:0006916, HP:0003474, HP:001101...   \n",
      "527      974  {HP:0011821, HP:0011017, HP:0002721, HP:000000...   \n",
      "3197   63925  {HP:0011821, HP:0100547, HP:0009115, HP:001244...   \n",
      "1289    2137  {HP:0002817, HP:0100547, HP:0011017, HP:003031...   \n",
      "\n",
      "                                         go_annotations  \\\n",
      "2698  {GO:0005623, GO:0015849, GO:0005886, GO:004442...   \n",
      "3532  {GO:0005623, GO:0032502, GO:0050789, GO:005087...   \n",
      "58    {GO:0046483, GO:0044271, GO:0007154, GO:005078...   \n",
      "3689  {GO:0005623, GO:0072334, GO:0006012, GO:004423...   \n",
      "2     {GO:0061512, GO:0032502, GO:0007017, GO:014029...   \n",
      "...                                                 ...   \n",
      "3441  {GO:0003676, GO:0008150, GO:0009653, GO:003250...   \n",
      "1344  {GO:0031410, GO:0007154, GO:0050789, GO:007016...   \n",
      "527   {GO:0005623, GO:0002376, GO:0051716, GO:000715...   \n",
      "3197  {GO:0002376, GO:0032502, GO:0050789, GO:000828...   \n",
      "1289  {GO:0005623, GO:0051716, GO:0042175, GO:003462...   \n",
      "\n",
      "                                        iea_annotations  \\\n",
      "2698  {GO:0015740, GO:0005886, GO:0022804, GO:000582...   \n",
      "3532  {GO:0032502, GO:0050789, GO:0050877, GO:000828...   \n",
      "58    {GO:0051172, GO:0016458, GO:0061061, GO:004648...   \n",
      "3689  {GO:0005623, GO:0072334, GO:0006012, GO:004423...   \n",
      "2     {GO:0008064, GO:0016462, GO:0007017, GO:003287...   \n",
      "...                                                 ...   \n",
      "3441  {GO:0051172, GO:0046483, GO:0044271, GO:000715...   \n",
      "1344  {GO:0031410, GO:0002376, GO:0014037, GO:003250...   \n",
      "527   {GO:0002376, GO:0032502, GO:0007154, GO:005078...   \n",
      "3197  {GO:0002376, GO:0046483, GO:0044271, GO:002195...   \n",
      "1289  {GO:0042175, GO:0007154, GO:0050789, GO:190113...   \n",
      "\n",
      "                                     deepgo_annotations  \\\n",
      "2698  [GO:0003333|0.046, GO:0003674|0.653, GO:000521...   \n",
      "3532  [GO:0001525|0.174, GO:0001568|0.174, GO:000194...   \n",
      "58    [GO:0000003|0.224, GO:0000075|0.011, GO:000007...   \n",
      "3689  [GO:0000139|0.385, GO:0003674|0.645, GO:000521...   \n",
      "2     [GO:0000003|0.595, GO:0000226|0.861, GO:000110...   \n",
      "...                                                 ...   \n",
      "3441  [GO:0000122|0.176, GO:0000228|0.211, GO:000057...   \n",
      "1344  [GO:0000165|0.116, GO:0001655|0.048, GO:000166...   \n",
      "527   [GO:0002253|0.225, GO:0002376|0.731, GO:000242...   \n",
      "3197  [GO:0000902|0.348, GO:0000976|0.265, GO:000097...   \n",
      "1289  [GO:0000003|0.044, GO:0000139|0.115, GO:000027...   \n",
      "\n",
      "                                              sequences  \\\n",
      "2698  MASALSYVSKFKSFVILFVTPLLLLPLVILMPAKFVRCAYVIILMA...   \n",
      "3532  MALFVRLLALALALALGPAATLAGPAKSPYQLVLQHSRLRGRQHGP...   \n",
      "58    MAAQVAPAAASSLGNPPPPPPSELKKAEQQQREEAGGEAAAAAAAE...   \n",
      "3689  MAAVGAGGSTAAPGPGAVSAGALEPGTASAAHRRLKYISLAVLVVQ...   \n",
      "2     MSRLEAKKPSLCKSEPLTTERVRTTLSVLKRIVTSCYGPSGRLKQL...   \n",
      "...                                                 ...   \n",
      "3441  MYHPRELYPSLGAGYRLGPAQPGADSSFPPALAEGYRYPELDTPKL...   \n",
      "1344  MSREMQDVDLAEVKPLVEKGETITGLLQEFDVQEQDIETLHGSVHV...   \n",
      "527   MARLALSPVPSHWMVALLLLLSAEPVPAARSEDRYRNPKGSACSRI...   \n",
      "3197  MEENEVESSSDAAPGPGRPEEPSESGLGVGTSEAVSADSSDAAAAP...   \n",
      "1289  MTGYTMLRNGGAGNGGQTCMLRWSNRIRLTWLSFTLFVILVFFPLI...   \n",
      "\n",
      "                                            expressions  \\\n",
      "2698  [0.018867925, 0.018867925, 0.003144654, 0.0025...   \n",
      "3532  [0.0024600246, 0.0012300123, 0.007380074, 0.00...   \n",
      "58    [0.2, 0.23076923, 0.24615385, 1.0, 0.41538462,...   \n",
      "3689  [0.22580644, 0.29032257, 0.29032257, 0.6774193...   \n",
      "2     [0.5, 0.71428573, 0.39285713, 0.4642857, 0.428...   \n",
      "...                                                 ...   \n",
      "3441  [0.083333336, 0.083333336, 0.16666667, 0.25, 0...   \n",
      "1344  [0.05740741, 0.087037034, 0.43333334, 0.009259...   \n",
      "527   [0.0013953489, 0.0013953489, 0.0023255814, 0.8...   \n",
      "3197  [0.25714287, 0.2857143, 0.2, 0.6, 0.37142858, ...   \n",
      "1289  [0.5263158, 0.68421054, 0.47368422, 0.2631579,...   \n",
      "\n",
      "                                                  preds  \n",
      "2698  [0.188488, 0.03743103, 0.14804429, 0.056062162...  \n",
      "3532  [0.18848792, 0.037431, 0.14804411, 0.056062043...  \n",
      "58    [0.18848789, 0.037430942, 0.14804411, 0.056062...  \n",
      "3689  [0.188488, 0.03743097, 0.14804423, 0.056062073...  \n",
      "2     [0.18848792, 0.037430912, 0.14804411, 0.056062...  \n",
      "...                                                 ...  \n",
      "3441  [0.18848789, 0.037430942, 0.14804411, 0.056062...  \n",
      "1344  [0.18848792, 0.037430912, 0.14804411, 0.056062...  \n",
      "527   [0.18848792, 0.03743097, 0.14804411, 0.0560620...  \n",
      "3197  [0.18848789, 0.037430942, 0.14804411, 0.056062...  \n",
      "1289  [0.18848792, 0.037431, 0.14804411, 0.056062043...  \n",
      "\n",
      "[393 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# main(hp_file, data_file, terms_file, gos_file, model_file,out_file, fold, batch_size, epochs, load, logger_file, threshold,device):\n",
    "\n",
    "main('data/hp.obo',\n",
    "     \n",
    "     'data/My_Implementations/human.pkl',\n",
    "     \n",
    "     'data/all_terms.pkl',\n",
    "     \n",
    "     'data/My_Implementations/gos.pkl',\n",
    "     \n",
    "     'data/My_Implementations/Trial_7/model_mohamed.h5',\n",
    "     \n",
    "     'data/My_Implementations/Trial_7/predictions_40_perc.pkl',\n",
    "     \n",
    "     1,\n",
    "#batch_size\n",
    "     10,\n",
    "#Number of epochs     \n",
    "     1024,\n",
    "     \n",
    "     'load',\n",
    "                                   \n",
    "     'data/My_Implementations/Trial_7/training.csv',\n",
    "     \n",
    "     0.5,\n",
    "    \n",
    "     'GPU:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial_6 --> CNN only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {'input_shape': (24274,), 'nb_layers': 1, 'loss': 'binary_crossentropy', 'rate': 0.3, 'learning_rate': 0.001, 'units': 1500, 'model_file': 'data/My_Implementations/Trial_6/model_mohamed.h5'}\n",
      "Phenotypes 2600\n",
      "\n",
      "2753 785 393\n",
      "3933 2753 785 393\n",
      "2600\n",
      "Creating a new model\n",
      "Training data size: 2753\n",
      "Validation data size: 785\n",
      "INFO:tensorflow:Reloading Oracle from data-cafa\\pheno\\oracle.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from data-cafa\\pheno\\oracle.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 24274)]           0         \n",
      "_________________________________________________________________\n",
      "dense_0 (Dense)              (None, 250)               6068750   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_out (Dense)            (None, 2600)              652600    \n",
      "=================================================================\n",
      "Total params: 6,721,350\n",
      "Trainable params: 6,721,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Reloading Tuner from data-cafa\\pheno\\tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from data-cafa\\pheno\\tuner0.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 24274)]           0         \n",
      "_________________________________________________________________\n",
      "dense_0 (Dense)              (None, 750)               18206250  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "dense_out (Dense)            (None, 2600)              1952600   \n",
      "=================================================================\n",
      "Total params: 20,158,850\n",
      "Trainable params: 20,158,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2753 samples, validate on 785 samples\n",
      "Epoch 1/3\n",
      "2753/2753 [==============================] - ETA: 39s - loss: 0.69 - ETA: 5s - loss: 0.6876 - ETA: 3s - loss: 0.678 - ETA: 2s - loss: 0.663 - ETA: 1s - loss: 0.643 - ETA: 1s - loss: 0.616 - ETA: 1s - loss: 0.586 - ETA: 0s - loss: 0.554 - ETA: 0s - loss: 0.521 - ETA: 0s - loss: 0.494 - ETA: 0s - loss: 0.468 - ETA: 0s - loss: 0.446 - ETA: 0s - loss: 0.426 - ETA: 0s - loss: 0.405 - ETA: 0s - loss: 0.390 - 2s 887us/sample - loss: 0.3907 - val_loss: 0.1743\n",
      "Epoch 2/3\n",
      "2753/2753 [==============================] - ETA: 0s - loss: 0.239 - ETA: 0s - loss: 0.181 - ETA: 0s - loss: 0.190 - ETA: 0s - loss: 0.186 - ETA: 0s - loss: 0.186 - ETA: 0s - loss: 0.185 - ETA: 0s - loss: 0.184 - ETA: 0s - loss: 0.183 - ETA: 0s - loss: 0.181 - ETA: 0s - loss: 0.179 - ETA: 0s - loss: 0.179 - ETA: 0s - loss: 0.178 - ETA: 0s - loss: 0.177 - ETA: 0s - loss: 0.176 - ETA: 0s - loss: 0.175 - 2s 692us/sample - loss: 0.1760 - val_loss: 0.1527\n",
      "Epoch 3/3\n",
      "2753/2753 [==============================] - ETA: 0s - loss: 0.142 - ETA: 0s - loss: 0.149 - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.161 - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.157 - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.156 - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.155 - 2s 726us/sample - loss: 0.1550 - val_loss: 0.1435\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hp values:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.0001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-rate: 0.5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units: 750</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.14352638112131957</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Results summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Results in data-cafa\\pheno</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Showing 10 best trials</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Objective: Objective(name='val_loss', direction='min') Score: 0.14352638112131957</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 24274)]           0         \n",
      "_________________________________________________________________\n",
      "dense_0 (Dense)              (None, 750)               18206250  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "dense_out (Dense)            (None, 2600)              1952600   \n",
      "=================================================================\n",
      "Total params: 20,158,850\n",
      "Trainable params: 20,158,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 24274)]           0         \n",
      "_________________________________________________________________\n",
      "dense_0 (Dense)              (None, 750)               18206250  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 750)               0         \n",
      "_________________________________________________________________\n",
      "dense_out (Dense)            (None, 2600)              1952600   \n",
      "=================================================================\n",
      "Total params: 20,158,850\n",
      "Trainable params: 20,158,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "785/785 [==============================] - ETA: 1s - loss: 0.149 - ETA: 0s - loss: 0.143 - ETA: 0s - loss: 0.143 - 0s 192us/sample - loss: 0.1435\n",
      "Valid loss 0.143526\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 24274)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model (Model)                   (None, 2600)         20158850    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "hpo_layer (HPOLayer)            (None, 2600, 2600)   6760000     model[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 2593, 20)     416020      hpo_layer[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 2585, 20)     832020      hpo_layer[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 2577, 20)     1248020     hpo_layer[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 162, 20)      0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 161, 20)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 161, 20)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 3240)         0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3220)         0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 3220)         0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 9680)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_out (Dense)               (None, 2600)         25170600    concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 54,585,510\n",
      "Trainable params: 47,825,510\n",
      "Non-trainable params: 6,760,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Compilation finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 24274)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model (Model)                   (None, 2600)         20158850    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "hpo_layer (HPOLayer)            (None, 2600, 2600)   6760000     model[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 2593, 20)     416020      hpo_layer[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 2585, 20)     832020      hpo_layer[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 2577, 20)     1248020     hpo_layer[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 162, 20)      0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 161, 20)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 161, 20)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 3240)         0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3220)         0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 3220)         0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 9680)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_out (Dense)               (None, 2600)         25170600    concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 54,585,510\n",
      "Trainable params: 47,825,510\n",
      "Non-trainable params: 6,760,000\n",
      "__________________________________________________________________________________________________\n",
      "Starting training the flat model\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 276 steps, validate for 79 steps\n",
      "Epoch 1/1024\n",
      "184/276 [===================>..........] - ETA: 15:01 - loss: 0.6933 - accuracy: 0.498 - ETA: 7:49 - loss: 0.6839 - accuracy: 0.703 - ETA: 5:25 - loss: 0.6653 - accuracy: 0.78 - ETA: 4:13 - loss: 0.6278 - accuracy: 0.82 - ETA: 3:30 - loss: 0.5582 - accuracy: 0.85 - ETA: 3:01 - loss: 0.5012 - accuracy: 0.86 - ETA: 2:40 - loss: 0.4539 - accuracy: 0.88 - ETA: 2:24 - loss: 0.4282 - accuracy: 0.89 - ETA: 2:12 - loss: 0.4074 - accuracy: 0.89 - ETA: 2:03 - loss: 0.3844 - accuracy: 0.90 - ETA: 1:55 - loss: 0.3783 - accuracy: 0.90 - ETA: 1:48 - loss: 0.3667 - accuracy: 0.91 - ETA: 1:42 - loss: 0.3588 - accuracy: 0.91 - ETA: 1:37 - loss: 0.3526 - accuracy: 0.91 - ETA: 1:33 - loss: 0.3406 - accuracy: 0.91 - ETA: 1:29 - loss: 0.3308 - accuracy: 0.91 - ETA: 1:26 - loss: 0.3206 - accuracy: 0.91 - ETA: 1:23 - loss: 0.3126 - accuracy: 0.92 - ETA: 1:20 - loss: 0.3054 - accuracy: 0.92 - ETA: 1:18 - loss: 0.2974 - accuracy: 0.92 - ETA: 1:16 - loss: 0.2904 - accuracy: 0.92 - ETA: 1:14 - loss: 0.2839 - accuracy: 0.92 - ETA: 1:12 - loss: 0.2772 - accuracy: 0.92 - ETA: 1:10 - loss: 0.2710 - accuracy: 0.93 - ETA: 1:09 - loss: 0.2671 - accuracy: 0.93 - ETA: 1:07 - loss: 0.2613 - accuracy: 0.93 - ETA: 1:06 - loss: 0.2572 - accuracy: 0.93 - ETA: 1:04 - loss: 0.2530 - accuracy: 0.93 - ETA: 1:03 - loss: 0.2493 - accuracy: 0.93 - ETA: 1:02 - loss: 0.2473 - accuracy: 0.93 - ETA: 1:01 - loss: 0.2474 - accuracy: 0.93 - ETA: 1:00 - loss: 0.2447 - accuracy: 0.93 - ETA: 59s - loss: 0.2406 - accuracy: 0.9371 - ETA: 58s - loss: 0.2381 - accuracy: 0.937 - ETA: 57s - loss: 0.2365 - accuracy: 0.937 - ETA: 56s - loss: 0.2342 - accuracy: 0.938 - ETA: 56s - loss: 0.2315 - accuracy: 0.938 - ETA: 55s - loss: 0.2284 - accuracy: 0.939 - ETA: 54s - loss: 0.2252 - accuracy: 0.940 - ETA: 53s - loss: 0.2236 - accuracy: 0.940 - ETA: 53s - loss: 0.2215 - accuracy: 0.941 - ETA: 52s - loss: 0.2208 - accuracy: 0.941 - ETA: 51s - loss: 0.2183 - accuracy: 0.941 - ETA: 51s - loss: 0.2167 - accuracy: 0.941 - ETA: 50s - loss: 0.2155 - accuracy: 0.942 - ETA: 50s - loss: 0.2131 - accuracy: 0.942 - ETA: 49s - loss: 0.2117 - accuracy: 0.942 - ETA: 49s - loss: 0.2096 - accuracy: 0.943 - ETA: 48s - loss: 0.2078 - accuracy: 0.943 - ETA: 48s - loss: 0.2057 - accuracy: 0.944 - ETA: 47s - loss: 0.2049 - accuracy: 0.944 - ETA: 47s - loss: 0.2033 - accuracy: 0.944 - ETA: 46s - loss: 0.2022 - accuracy: 0.944 - ETA: 46s - loss: 0.2001 - accuracy: 0.945 - ETA: 45s - loss: 0.1990 - accuracy: 0.945 - ETA: 45s - loss: 0.1983 - accuracy: 0.945 - ETA: 44s - loss: 0.1971 - accuracy: 0.946 - ETA: 44s - loss: 0.1952 - accuracy: 0.946 - ETA: 44s - loss: 0.1947 - accuracy: 0.946 - ETA: 43s - loss: 0.1937 - accuracy: 0.946 - ETA: 43s - loss: 0.1925 - accuracy: 0.947 - ETA: 42s - loss: 0.1918 - accuracy: 0.947 - ETA: 42s - loss: 0.1902 - accuracy: 0.947 - ETA: 42s - loss: 0.1898 - accuracy: 0.947 - ETA: 41s - loss: 0.1884 - accuracy: 0.947 - ETA: 41s - loss: 0.1884 - accuracy: 0.947 - ETA: 41s - loss: 0.1889 - accuracy: 0.947 - ETA: 40s - loss: 0.1882 - accuracy: 0.947 - ETA: 40s - loss: 0.1873 - accuracy: 0.947 - ETA: 40s - loss: 0.1861 - accuracy: 0.948 - ETA: 39s - loss: 0.1855 - accuracy: 0.948 - ETA: 39s - loss: 0.1847 - accuracy: 0.948 - ETA: 39s - loss: 0.1847 - accuracy: 0.948 - ETA: 38s - loss: 0.1836 - accuracy: 0.948 - ETA: 38s - loss: 0.1825 - accuracy: 0.949 - ETA: 38s - loss: 0.1813 - accuracy: 0.949 - ETA: 37s - loss: 0.1808 - accuracy: 0.949 - ETA: 37s - loss: 0.1808 - accuracy: 0.949 - ETA: 37s - loss: 0.1810 - accuracy: 0.949 - ETA: 37s - loss: 0.1801 - accuracy: 0.949 - ETA: 36s - loss: 0.1804 - accuracy: 0.949 - ETA: 36s - loss: 0.1796 - accuracy: 0.949 - ETA: 36s - loss: 0.1789 - accuracy: 0.949 - ETA: 35s - loss: 0.1784 - accuracy: 0.949 - ETA: 35s - loss: 0.1777 - accuracy: 0.949 - ETA: 35s - loss: 0.1771 - accuracy: 0.950 - ETA: 35s - loss: 0.1763 - accuracy: 0.950 - ETA: 34s - loss: 0.1761 - accuracy: 0.950 - ETA: 34s - loss: 0.1754 - accuracy: 0.950 - ETA: 34s - loss: 0.1750 - accuracy: 0.950 - ETA: 34s - loss: 0.1747 - accuracy: 0.950 - ETA: 33s - loss: 0.1746 - accuracy: 0.950 - ETA: 33s - loss: 0.1743 - accuracy: 0.950 - ETA: 33s - loss: 0.1737 - accuracy: 0.950 - ETA: 33s - loss: 0.1735 - accuracy: 0.950 - ETA: 32s - loss: 0.1729 - accuracy: 0.950 - ETA: 32s - loss: 0.1723 - accuracy: 0.950 - ETA: 32s - loss: 0.1724 - accuracy: 0.950 - ETA: 32s - loss: 0.1720 - accuracy: 0.950 - ETA: 31s - loss: 0.1719 - accuracy: 0.950 - ETA: 31s - loss: 0.1716 - accuracy: 0.950 - ETA: 31s - loss: 0.1717 - accuracy: 0.950 - ETA: 31s - loss: 0.1715 - accuracy: 0.950 - ETA: 30s - loss: 0.1711 - accuracy: 0.950 - ETA: 30s - loss: 0.1710 - accuracy: 0.950 - ETA: 30s - loss: 0.1710 - accuracy: 0.950 - ETA: 30s - loss: 0.1707 - accuracy: 0.950 - ETA: 30s - loss: 0.1699 - accuracy: 0.951 - ETA: 29s - loss: 0.1694 - accuracy: 0.951 - ETA: 29s - loss: 0.1691 - accuracy: 0.951 - ETA: 29s - loss: 0.1688 - accuracy: 0.951 - ETA: 29s - loss: 0.1692 - accuracy: 0.951 - ETA: 28s - loss: 0.1687 - accuracy: 0.951 - ETA: 28s - loss: 0.1685 - accuracy: 0.951 - ETA: 28s - loss: 0.1691 - accuracy: 0.950 - ETA: 28s - loss: 0.1692 - accuracy: 0.950 - ETA: 28s - loss: 0.1697 - accuracy: 0.950 - ETA: 27s - loss: 0.1693 - accuracy: 0.950 - ETA: 27s - loss: 0.1689 - accuracy: 0.950 - ETA: 27s - loss: 0.1687 - accuracy: 0.950 - ETA: 27s - loss: 0.1685 - accuracy: 0.950 - ETA: 27s - loss: 0.1680 - accuracy: 0.951 - ETA: 26s - loss: 0.1677 - accuracy: 0.951 - ETA: 26s - loss: 0.1675 - accuracy: 0.951 - ETA: 26s - loss: 0.1670 - accuracy: 0.951 - ETA: 26s - loss: 0.1667 - accuracy: 0.951 - ETA: 26s - loss: 0.1670 - accuracy: 0.951 - ETA: 25s - loss: 0.1667 - accuracy: 0.951 - ETA: 25s - loss: 0.1665 - accuracy: 0.951 - ETA: 25s - loss: 0.1662 - accuracy: 0.951 - ETA: 25s - loss: 0.1659 - accuracy: 0.951 - ETA: 25s - loss: 0.1655 - accuracy: 0.951 - ETA: 24s - loss: 0.1653 - accuracy: 0.951 - ETA: 24s - loss: 0.1651 - accuracy: 0.951 - ETA: 24s - loss: 0.1647 - accuracy: 0.951 - ETA: 24s - loss: 0.1643 - accuracy: 0.952 - ETA: 24s - loss: 0.1642 - accuracy: 0.952 - ETA: 23s - loss: 0.1639 - accuracy: 0.952 - ETA: 23s - loss: 0.1635 - accuracy: 0.952 - ETA: 23s - loss: 0.1631 - accuracy: 0.952 - ETA: 23s - loss: 0.1629 - accuracy: 0.952 - ETA: 23s - loss: 0.1627 - accuracy: 0.952 - ETA: 22s - loss: 0.1626 - accuracy: 0.952 - ETA: 22s - loss: 0.1623 - accuracy: 0.952 - ETA: 22s - loss: 0.1619 - accuracy: 0.952 - ETA: 22s - loss: 0.1617 - accuracy: 0.952 - ETA: 22s - loss: 0.1618 - accuracy: 0.952 - ETA: 21s - loss: 0.1615 - accuracy: 0.952 - ETA: 21s - loss: 0.1611 - accuracy: 0.952 - ETA: 21s - loss: 0.1610 - accuracy: 0.952 - ETA: 21s - loss: 0.1608 - accuracy: 0.952 - ETA: 21s - loss: 0.1607 - accuracy: 0.952 - ETA: 20s - loss: 0.1604 - accuracy: 0.953 - ETA: 20s - loss: 0.1604 - accuracy: 0.953 - ETA: 20s - loss: 0.1607 - accuracy: 0.952 - ETA: 20s - loss: 0.1604 - accuracy: 0.952 - ETA: 20s - loss: 0.1602 - accuracy: 0.952 - ETA: 20s - loss: 0.1602 - accuracy: 0.952 - ETA: 20s - loss: 0.1600 - accuracy: 0.953 - ETA: 19s - loss: 0.1599 - accuracy: 0.953 - ETA: 19s - loss: 0.1598 - accuracy: 0.953 - ETA: 19s - loss: 0.1597 - accuracy: 0.953 - ETA: 19s - loss: 0.1594 - accuracy: 0.953 - ETA: 19s - loss: 0.1592 - accuracy: 0.953 - ETA: 18s - loss: 0.1593 - accuracy: 0.953 - ETA: 18s - loss: 0.1593 - accuracy: 0.953 - ETA: 18s - loss: 0.1593 - accuracy: 0.953 - ETA: 18s - loss: 0.1591 - accuracy: 0.953 - ETA: 18s - loss: 0.1592 - accuracy: 0.953 - ETA: 17s - loss: 0.1589 - accuracy: 0.953 - ETA: 17s - loss: 0.1587 - accuracy: 0.953 - ETA: 17s - loss: 0.1586 - accuracy: 0.953 - ETA: 17s - loss: 0.1583 - accuracy: 0.953 - ETA: 17s - loss: 0.1580 - accuracy: 0.953 - ETA: 17s - loss: 0.1577 - accuracy: 0.953 - ETA: 16s - loss: 0.1576 - accuracy: 0.953 - ETA: 16s - loss: 0.1576 - accuracy: 0.953 - ETA: 16s - loss: 0.1577 - accuracy: 0.953 - ETA: 16s - loss: 0.1576 - accuracy: 0.953 - ETA: 16s - loss: 0.1576 - accuracy: 0.953 - ETA: 16s - loss: 0.1573 - accuracy: 0.953 - ETA: 15s - loss: 0.1572 - accuracy: 0.953 - ETA: 15s - loss: 0.1571 - accuracy: 0.953 - ETA: 15s - loss: 0.1572 - accuracy: 0.953275/276 [============================>.] - ETA: 15s - loss: 0.1569 - accuracy: 0.953 - ETA: 15s - loss: 0.1569 - accuracy: 0.953 - ETA: 14s - loss: 0.1565 - accuracy: 0.953 - ETA: 14s - loss: 0.1564 - accuracy: 0.953 - ETA: 14s - loss: 0.1563 - accuracy: 0.953 - ETA: 14s - loss: 0.1561 - accuracy: 0.953 - ETA: 14s - loss: 0.1560 - accuracy: 0.953 - ETA: 14s - loss: 0.1559 - accuracy: 0.953 - ETA: 13s - loss: 0.1557 - accuracy: 0.953 - ETA: 13s - loss: 0.1557 - accuracy: 0.953 - ETA: 13s - loss: 0.1557 - accuracy: 0.953 - ETA: 13s - loss: 0.1555 - accuracy: 0.953 - ETA: 13s - loss: 0.1555 - accuracy: 0.953 - ETA: 13s - loss: 0.1553 - accuracy: 0.953 - ETA: 12s - loss: 0.1552 - accuracy: 0.953 - ETA: 12s - loss: 0.1551 - accuracy: 0.953 - ETA: 12s - loss: 0.1551 - accuracy: 0.953 - ETA: 12s - loss: 0.1551 - accuracy: 0.953 - ETA: 12s - loss: 0.1549 - accuracy: 0.953 - ETA: 11s - loss: 0.1547 - accuracy: 0.953 - ETA: 11s - loss: 0.1547 - accuracy: 0.953 - ETA: 11s - loss: 0.1546 - accuracy: 0.953 - ETA: 11s - loss: 0.1547 - accuracy: 0.953 - ETA: 11s - loss: 0.1545 - accuracy: 0.953 - ETA: 11s - loss: 0.1546 - accuracy: 0.953 - ETA: 10s - loss: 0.1544 - accuracy: 0.953 - ETA: 10s - loss: 0.1543 - accuracy: 0.954 - ETA: 10s - loss: 0.1542 - accuracy: 0.954 - ETA: 10s - loss: 0.1541 - accuracy: 0.954 - ETA: 10s - loss: 0.1539 - accuracy: 0.954 - ETA: 10s - loss: 0.1540 - accuracy: 0.954 - ETA: 9s - loss: 0.1540 - accuracy: 0.954 - ETA: 9s - loss: 0.1537 - accuracy: 0.95 - ETA: 9s - loss: 0.1534 - accuracy: 0.95 - ETA: 9s - loss: 0.1535 - accuracy: 0.95 - ETA: 9s - loss: 0.1534 - accuracy: 0.95 - ETA: 9s - loss: 0.1533 - accuracy: 0.95 - ETA: 8s - loss: 0.1531 - accuracy: 0.95 - ETA: 8s - loss: 0.1530 - accuracy: 0.95 - ETA: 8s - loss: 0.1530 - accuracy: 0.95 - ETA: 8s - loss: 0.1530 - accuracy: 0.95 - ETA: 8s - loss: 0.1528 - accuracy: 0.95 - ETA: 8s - loss: 0.1529 - accuracy: 0.95 - ETA: 7s - loss: 0.1527 - accuracy: 0.95 - ETA: 7s - loss: 0.1527 - accuracy: 0.95 - ETA: 7s - loss: 0.1526 - accuracy: 0.95 - ETA: 7s - loss: 0.1524 - accuracy: 0.95 - ETA: 7s - loss: 0.1523 - accuracy: 0.95 - ETA: 7s - loss: 0.1523 - accuracy: 0.95 - ETA: 6s - loss: 0.1521 - accuracy: 0.95 - ETA: 6s - loss: 0.1521 - accuracy: 0.95 - ETA: 6s - loss: 0.1519 - accuracy: 0.95 - ETA: 6s - loss: 0.1518 - accuracy: 0.95 - ETA: 6s - loss: 0.1517 - accuracy: 0.95 - ETA: 6s - loss: 0.1517 - accuracy: 0.95 - ETA: 5s - loss: 0.1515 - accuracy: 0.95 - ETA: 5s - loss: 0.1512 - accuracy: 0.95 - ETA: 5s - loss: 0.1514 - accuracy: 0.95 - ETA: 5s - loss: 0.1514 - accuracy: 0.95 - ETA: 5s - loss: 0.1513 - accuracy: 0.95 - ETA: 5s - loss: 0.1513 - accuracy: 0.95 - ETA: 4s - loss: 0.1513 - accuracy: 0.95 - ETA: 4s - loss: 0.1512 - accuracy: 0.95 - ETA: 4s - loss: 0.1510 - accuracy: 0.95 - ETA: 4s - loss: 0.1508 - accuracy: 0.95 - ETA: 4s - loss: 0.1506 - accuracy: 0.95 - ETA: 4s - loss: 0.1505 - accuracy: 0.95 - ETA: 3s - loss: 0.1505 - accuracy: 0.95 - ETA: 3s - loss: 0.1504 - accuracy: 0.95 - ETA: 3s - loss: 0.1502 - accuracy: 0.95 - ETA: 3s - loss: 0.1501 - accuracy: 0.95 - ETA: 3s - loss: 0.1498 - accuracy: 0.95 - ETA: 3s - loss: 0.1496 - accuracy: 0.95 - ETA: 2s - loss: 0.1496 - accuracy: 0.95 - ETA: 2s - loss: 0.1496 - accuracy: 0.95 - ETA: 2s - loss: 0.1496 - accuracy: 0.95 - ETA: 2s - loss: 0.1496 - accuracy: 0.95 - ETA: 2s - loss: 0.1496 - accuracy: 0.95 - ETA: 2s - loss: 0.1499 - accuracy: 0.95 - ETA: 1s - loss: 0.1498 - accuracy: 0.95 - ETA: 1s - loss: 0.1497 - accuracy: 0.95 - ETA: 1s - loss: 0.1497 - accuracy: 0.95 - ETA: 1s - loss: 0.1495 - accuracy: 0.95 - ETA: 1s - loss: 0.1495 - accuracy: 0.95 - ETA: 1s - loss: 0.1494 - accuracy: 0.95 - ETA: 0s - loss: 0.1492 - accuracy: 0.95 - ETA: 0s - loss: 0.1491 - accuracy: 0.95 - ETA: 0s - loss: 0.1490 - accuracy: 0.95 - ETA: 0s - loss: 0.1490 - accuracy: 0.95 - ETA: 0s - loss: 0.1491 - accuracy: 0.95 - ETA: 0s - loss: 0.1493 - accuracy: 0.9551\n",
      "Epoch 00001: val_loss improved from inf to 0.13382, saving model to data/My_Implementations/Trial_6/model_mohamed.h5\n",
      "276/276 [==============================] - 51s 186ms/step - loss: 0.1492 - accuracy: 0.9552 - val_loss: 0.1338 - val_accuracy: 0.9576\n",
      "Epoch 2/1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/276 [===================>..........] - ETA: 1:35 - loss: 0.1770 - accuracy: 0.94 - ETA: 1:08 - loss: 0.1460 - accuracy: 0.95 - ETA: 58s - loss: 0.1490 - accuracy: 0.9503 - ETA: 54s - loss: 0.1436 - accuracy: 0.952 - ETA: 51s - loss: 0.1401 - accuracy: 0.955 - ETA: 49s - loss: 0.1398 - accuracy: 0.955 - ETA: 47s - loss: 0.1351 - accuracy: 0.958 - ETA: 46s - loss: 0.1400 - accuracy: 0.955 - ETA: 45s - loss: 0.1369 - accuracy: 0.956 - ETA: 45s - loss: 0.1343 - accuracy: 0.957 - ETA: 44s - loss: 0.1318 - accuracy: 0.958 - ETA: 44s - loss: 0.1323 - accuracy: 0.957 - ETA: 43s - loss: 0.1337 - accuracy: 0.957 - ETA: 43s - loss: 0.1335 - accuracy: 0.957 - ETA: 42s - loss: 0.1383 - accuracy: 0.956 - ETA: 42s - loss: 0.1386 - accuracy: 0.956 - ETA: 41s - loss: 0.1384 - accuracy: 0.956 - ETA: 41s - loss: 0.1386 - accuracy: 0.956 - ETA: 41s - loss: 0.1376 - accuracy: 0.956 - ETA: 41s - loss: 0.1360 - accuracy: 0.957 - ETA: 40s - loss: 0.1369 - accuracy: 0.956 - ETA: 40s - loss: 0.1362 - accuracy: 0.956 - ETA: 40s - loss: 0.1343 - accuracy: 0.957 - ETA: 39s - loss: 0.1342 - accuracy: 0.957 - ETA: 39s - loss: 0.1327 - accuracy: 0.958 - ETA: 39s - loss: 0.1328 - accuracy: 0.957 - ETA: 39s - loss: 0.1334 - accuracy: 0.957 - ETA: 39s - loss: 0.1329 - accuracy: 0.958 - ETA: 38s - loss: 0.1335 - accuracy: 0.957 - ETA: 38s - loss: 0.1335 - accuracy: 0.957 - ETA: 38s - loss: 0.1323 - accuracy: 0.958 - ETA: 38s - loss: 0.1334 - accuracy: 0.958 - ETA: 38s - loss: 0.1341 - accuracy: 0.957 - ETA: 37s - loss: 0.1345 - accuracy: 0.957 - ETA: 37s - loss: 0.1340 - accuracy: 0.957 - ETA: 37s - loss: 0.1335 - accuracy: 0.958 - ETA: 37s - loss: 0.1330 - accuracy: 0.958 - ETA: 37s - loss: 0.1320 - accuracy: 0.958 - ETA: 36s - loss: 0.1329 - accuracy: 0.958 - ETA: 36s - loss: 0.1335 - accuracy: 0.958 - ETA: 36s - loss: 0.1324 - accuracy: 0.958 - ETA: 36s - loss: 0.1324 - accuracy: 0.958 - ETA: 36s - loss: 0.1315 - accuracy: 0.958 - ETA: 35s - loss: 0.1304 - accuracy: 0.959 - ETA: 35s - loss: 0.1299 - accuracy: 0.959 - ETA: 35s - loss: 0.1297 - accuracy: 0.959 - ETA: 35s - loss: 0.1288 - accuracy: 0.959 - ETA: 35s - loss: 0.1311 - accuracy: 0.959 - ETA: 35s - loss: 0.1308 - accuracy: 0.959 - ETA: 34s - loss: 0.1317 - accuracy: 0.958 - ETA: 34s - loss: 0.1311 - accuracy: 0.959 - ETA: 34s - loss: 0.1325 - accuracy: 0.958 - ETA: 34s - loss: 0.1332 - accuracy: 0.958 - ETA: 34s - loss: 0.1332 - accuracy: 0.958 - ETA: 34s - loss: 0.1327 - accuracy: 0.958 - ETA: 33s - loss: 0.1325 - accuracy: 0.958 - ETA: 33s - loss: 0.1325 - accuracy: 0.958 - ETA: 33s - loss: 0.1331 - accuracy: 0.958 - ETA: 33s - loss: 0.1331 - accuracy: 0.958 - ETA: 33s - loss: 0.1332 - accuracy: 0.958 - ETA: 33s - loss: 0.1333 - accuracy: 0.958 - ETA: 32s - loss: 0.1330 - accuracy: 0.958 - ETA: 32s - loss: 0.1331 - accuracy: 0.958 - ETA: 32s - loss: 0.1339 - accuracy: 0.957 - ETA: 32s - loss: 0.1337 - accuracy: 0.958 - ETA: 32s - loss: 0.1351 - accuracy: 0.957 - ETA: 32s - loss: 0.1352 - accuracy: 0.957 - ETA: 31s - loss: 0.1348 - accuracy: 0.957 - ETA: 31s - loss: 0.1343 - accuracy: 0.957 - ETA: 31s - loss: 0.1342 - accuracy: 0.958 - ETA: 31s - loss: 0.1342 - accuracy: 0.957 - ETA: 31s - loss: 0.1341 - accuracy: 0.957 - ETA: 31s - loss: 0.1342 - accuracy: 0.957 - ETA: 30s - loss: 0.1341 - accuracy: 0.957 - ETA: 30s - loss: 0.1341 - accuracy: 0.957 - ETA: 30s - loss: 0.1342 - accuracy: 0.957 - ETA: 30s - loss: 0.1339 - accuracy: 0.958 - ETA: 30s - loss: 0.1339 - accuracy: 0.957 - ETA: 30s - loss: 0.1341 - accuracy: 0.957 - ETA: 29s - loss: 0.1341 - accuracy: 0.957 - ETA: 29s - loss: 0.1341 - accuracy: 0.957 - ETA: 29s - loss: 0.1340 - accuracy: 0.957 - ETA: 29s - loss: 0.1340 - accuracy: 0.957 - ETA: 29s - loss: 0.1341 - accuracy: 0.957 - ETA: 29s - loss: 0.1342 - accuracy: 0.957 - ETA: 29s - loss: 0.1341 - accuracy: 0.957 - ETA: 28s - loss: 0.1338 - accuracy: 0.957 - ETA: 28s - loss: 0.1338 - accuracy: 0.957 - ETA: 28s - loss: 0.1340 - accuracy: 0.957 - ETA: 28s - loss: 0.1336 - accuracy: 0.957 - ETA: 28s - loss: 0.1337 - accuracy: 0.957 - ETA: 28s - loss: 0.1336 - accuracy: 0.957 - ETA: 27s - loss: 0.1335 - accuracy: 0.957 - ETA: 27s - loss: 0.1331 - accuracy: 0.958 - ETA: 27s - loss: 0.1329 - accuracy: 0.958 - ETA: 27s - loss: 0.1327 - accuracy: 0.958 - ETA: 27s - loss: 0.1327 - accuracy: 0.958 - ETA: 27s - loss: 0.1332 - accuracy: 0.958 - ETA: 27s - loss: 0.1328 - accuracy: 0.958 - ETA: 26s - loss: 0.1326 - accuracy: 0.958 - ETA: 26s - loss: 0.1330 - accuracy: 0.958 - ETA: 26s - loss: 0.1326 - accuracy: 0.958 - ETA: 26s - loss: 0.1331 - accuracy: 0.958 - ETA: 26s - loss: 0.1328 - accuracy: 0.958 - ETA: 26s - loss: 0.1328 - accuracy: 0.958 - ETA: 25s - loss: 0.1329 - accuracy: 0.958 - ETA: 25s - loss: 0.1328 - accuracy: 0.958 - ETA: 25s - loss: 0.1327 - accuracy: 0.958 - ETA: 25s - loss: 0.1329 - accuracy: 0.958 - ETA: 25s - loss: 0.1331 - accuracy: 0.958 - ETA: 25s - loss: 0.1336 - accuracy: 0.957 - ETA: 24s - loss: 0.1340 - accuracy: 0.957 - ETA: 24s - loss: 0.1341 - accuracy: 0.957 - ETA: 24s - loss: 0.1342 - accuracy: 0.957 - ETA: 24s - loss: 0.1340 - accuracy: 0.957 - ETA: 24s - loss: 0.1342 - accuracy: 0.957 - ETA: 24s - loss: 0.1342 - accuracy: 0.957 - ETA: 24s - loss: 0.1341 - accuracy: 0.957 - ETA: 23s - loss: 0.1339 - accuracy: 0.957 - ETA: 23s - loss: 0.1340 - accuracy: 0.957 - ETA: 23s - loss: 0.1341 - accuracy: 0.957 - ETA: 23s - loss: 0.1346 - accuracy: 0.957 - ETA: 23s - loss: 0.1348 - accuracy: 0.957 - ETA: 23s - loss: 0.1346 - accuracy: 0.957 - ETA: 22s - loss: 0.1346 - accuracy: 0.957 - ETA: 22s - loss: 0.1344 - accuracy: 0.957 - ETA: 22s - loss: 0.1343 - accuracy: 0.957 - ETA: 22s - loss: 0.1346 - accuracy: 0.957 - ETA: 22s - loss: 0.1348 - accuracy: 0.957 - ETA: 22s - loss: 0.1349 - accuracy: 0.957 - ETA: 22s - loss: 0.1347 - accuracy: 0.957 - ETA: 21s - loss: 0.1347 - accuracy: 0.957 - ETA: 21s - loss: 0.1347 - accuracy: 0.957 - ETA: 21s - loss: 0.1346 - accuracy: 0.957 - ETA: 21s - loss: 0.1345 - accuracy: 0.957 - ETA: 21s - loss: 0.1343 - accuracy: 0.957 - ETA: 21s - loss: 0.1341 - accuracy: 0.957 - ETA: 20s - loss: 0.1340 - accuracy: 0.957 - ETA: 20s - loss: 0.1340 - accuracy: 0.957 - ETA: 20s - loss: 0.1340 - accuracy: 0.957 - ETA: 20s - loss: 0.1336 - accuracy: 0.957 - ETA: 20s - loss: 0.1337 - accuracy: 0.957 - ETA: 20s - loss: 0.1334 - accuracy: 0.957 - ETA: 20s - loss: 0.1334 - accuracy: 0.957 - ETA: 19s - loss: 0.1341 - accuracy: 0.957 - ETA: 19s - loss: 0.1341 - accuracy: 0.957 - ETA: 19s - loss: 0.1343 - accuracy: 0.957 - ETA: 19s - loss: 0.1340 - accuracy: 0.957 - ETA: 19s - loss: 0.1339 - accuracy: 0.957 - ETA: 19s - loss: 0.1335 - accuracy: 0.957 - ETA: 18s - loss: 0.1338 - accuracy: 0.957 - ETA: 18s - loss: 0.1337 - accuracy: 0.957 - ETA: 18s - loss: 0.1337 - accuracy: 0.957 - ETA: 18s - loss: 0.1337 - accuracy: 0.957 - ETA: 18s - loss: 0.1335 - accuracy: 0.957 - ETA: 18s - loss: 0.1332 - accuracy: 0.957 - ETA: 18s - loss: 0.1334 - accuracy: 0.957 - ETA: 17s - loss: 0.1337 - accuracy: 0.957 - ETA: 17s - loss: 0.1337 - accuracy: 0.957 - ETA: 17s - loss: 0.1335 - accuracy: 0.957 - ETA: 17s - loss: 0.1336 - accuracy: 0.957 - ETA: 17s - loss: 0.1335 - accuracy: 0.957 - ETA: 17s - loss: 0.1334 - accuracy: 0.957 - ETA: 17s - loss: 0.1334 - accuracy: 0.957 - ETA: 16s - loss: 0.1335 - accuracy: 0.957 - ETA: 16s - loss: 0.1336 - accuracy: 0.957 - ETA: 16s - loss: 0.1334 - accuracy: 0.957 - ETA: 16s - loss: 0.1337 - accuracy: 0.957 - ETA: 16s - loss: 0.1334 - accuracy: 0.957 - ETA: 16s - loss: 0.1332 - accuracy: 0.957 - ETA: 15s - loss: 0.1329 - accuracy: 0.958 - ETA: 15s - loss: 0.1327 - accuracy: 0.958 - ETA: 15s - loss: 0.1327 - accuracy: 0.958 - ETA: 15s - loss: 0.1326 - accuracy: 0.958 - ETA: 15s - loss: 0.1325 - accuracy: 0.958 - ETA: 15s - loss: 0.1324 - accuracy: 0.958 - ETA: 15s - loss: 0.1323 - accuracy: 0.958 - ETA: 14s - loss: 0.1322 - accuracy: 0.958 - ETA: 14s - loss: 0.1321 - accuracy: 0.958 - ETA: 14s - loss: 0.1323 - accuracy: 0.958 - ETA: 14s - loss: 0.1323 - accuracy: 0.958 - ETA: 14s - loss: 0.1322 - accuracy: 0.958 - ETA: 14s - loss: 0.1321 - accuracy: 0.958 - ETA: 13s - loss: 0.1321 - accuracy: 0.958 - ETA: 13s - loss: 0.1320 - accuracy: 0.958 - ETA: 13s - loss: 0.1320 - accuracy: 0.9583"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/276 [============================>.] - ETA: 13s - loss: 0.1320 - accuracy: 0.958 - ETA: 13s - loss: 0.1317 - accuracy: 0.958 - ETA: 13s - loss: 0.1317 - accuracy: 0.958 - ETA: 13s - loss: 0.1321 - accuracy: 0.958 - ETA: 12s - loss: 0.1320 - accuracy: 0.958 - ETA: 12s - loss: 0.1319 - accuracy: 0.958 - ETA: 12s - loss: 0.1319 - accuracy: 0.958 - ETA: 12s - loss: 0.1320 - accuracy: 0.958 - ETA: 12s - loss: 0.1320 - accuracy: 0.958 - ETA: 12s - loss: 0.1322 - accuracy: 0.958 - ETA: 11s - loss: 0.1325 - accuracy: 0.958 - ETA: 11s - loss: 0.1324 - accuracy: 0.958 - ETA: 11s - loss: 0.1322 - accuracy: 0.958 - ETA: 11s - loss: 0.1323 - accuracy: 0.958 - ETA: 11s - loss: 0.1321 - accuracy: 0.958 - ETA: 11s - loss: 0.1320 - accuracy: 0.958 - ETA: 11s - loss: 0.1321 - accuracy: 0.958 - ETA: 10s - loss: 0.1322 - accuracy: 0.958 - ETA: 10s - loss: 0.1323 - accuracy: 0.958 - ETA: 10s - loss: 0.1322 - accuracy: 0.958 - ETA: 10s - loss: 0.1322 - accuracy: 0.958 - ETA: 10s - loss: 0.1322 - accuracy: 0.958 - ETA: 10s - loss: 0.1324 - accuracy: 0.958 - ETA: 10s - loss: 0.1322 - accuracy: 0.958 - ETA: 9s - loss: 0.1322 - accuracy: 0.958 - ETA: 9s - loss: 0.1323 - accuracy: 0.95 - ETA: 9s - loss: 0.1322 - accuracy: 0.95 - ETA: 9s - loss: 0.1321 - accuracy: 0.95 - ETA: 9s - loss: 0.1320 - accuracy: 0.95 - ETA: 9s - loss: 0.1320 - accuracy: 0.95 - ETA: 8s - loss: 0.1323 - accuracy: 0.95 - ETA: 8s - loss: 0.1326 - accuracy: 0.95 - ETA: 8s - loss: 0.1326 - accuracy: 0.95 - ETA: 8s - loss: 0.1328 - accuracy: 0.95 - ETA: 8s - loss: 0.1329 - accuracy: 0.95 - ETA: 8s - loss: 0.1328 - accuracy: 0.95 - ETA: 8s - loss: 0.1329 - accuracy: 0.95 - ETA: 7s - loss: 0.1328 - accuracy: 0.95 - ETA: 7s - loss: 0.1329 - accuracy: 0.95 - ETA: 7s - loss: 0.1330 - accuracy: 0.95 - ETA: 7s - loss: 0.1330 - accuracy: 0.95 - ETA: 7s - loss: 0.1329 - accuracy: 0.95 - ETA: 7s - loss: 0.1328 - accuracy: 0.95 - ETA: 6s - loss: 0.1330 - accuracy: 0.95 - ETA: 6s - loss: 0.1331 - accuracy: 0.95 - ETA: 6s - loss: 0.1331 - accuracy: 0.95 - ETA: 6s - loss: 0.1331 - accuracy: 0.95 - ETA: 6s - loss: 0.1329 - accuracy: 0.95 - ETA: 6s - loss: 0.1328 - accuracy: 0.95 - ETA: 6s - loss: 0.1327 - accuracy: 0.95 - ETA: 5s - loss: 0.1327 - accuracy: 0.95 - ETA: 5s - loss: 0.1325 - accuracy: 0.95 - ETA: 5s - loss: 0.1325 - accuracy: 0.95 - ETA: 5s - loss: 0.1323 - accuracy: 0.95 - ETA: 5s - loss: 0.1322 - accuracy: 0.95 - ETA: 5s - loss: 0.1323 - accuracy: 0.95 - ETA: 4s - loss: 0.1324 - accuracy: 0.95 - ETA: 4s - loss: 0.1326 - accuracy: 0.95 - ETA: 4s - loss: 0.1325 - accuracy: 0.95 - ETA: 4s - loss: 0.1325 - accuracy: 0.95 - ETA: 4s - loss: 0.1325 - accuracy: 0.95 - ETA: 4s - loss: 0.1324 - accuracy: 0.95 - ETA: 4s - loss: 0.1323 - accuracy: 0.95 - ETA: 3s - loss: 0.1322 - accuracy: 0.95 - ETA: 3s - loss: 0.1322 - accuracy: 0.95 - ETA: 3s - loss: 0.1321 - accuracy: 0.95 - ETA: 3s - loss: 0.1321 - accuracy: 0.95 - ETA: 3s - loss: 0.1319 - accuracy: 0.95 - ETA: 3s - loss: 0.1318 - accuracy: 0.95 - ETA: 3s - loss: 0.1318 - accuracy: 0.95 - ETA: 2s - loss: 0.1319 - accuracy: 0.95 - ETA: 2s - loss: 0.1319 - accuracy: 0.95 - ETA: 2s - loss: 0.1321 - accuracy: 0.95 - ETA: 2s - loss: 0.1322 - accuracy: 0.95 - ETA: 2s - loss: 0.1322 - accuracy: 0.95 - ETA: 2s - loss: 0.1322 - accuracy: 0.95 - ETA: 1s - loss: 0.1321 - accuracy: 0.95 - ETA: 1s - loss: 0.1322 - accuracy: 0.95 - ETA: 1s - loss: 0.1321 - accuracy: 0.95 - ETA: 1s - loss: 0.1322 - accuracy: 0.95 - ETA: 1s - loss: 0.1322 - accuracy: 0.95 - ETA: 1s - loss: 0.1322 - accuracy: 0.95 - ETA: 1s - loss: 0.1321 - accuracy: 0.95 - ETA: 0s - loss: 0.1321 - accuracy: 0.95 - ETA: 0s - loss: 0.1321 - accuracy: 0.95 - ETA: 0s - loss: 0.1322 - accuracy: 0.95 - ETA: 0s - loss: 0.1322 - accuracy: 0.95 - ETA: 0s - loss: 0.1322 - accuracy: 0.95 - ETA: 0s - loss: 0.1322 - accuracy: 0.9581\n",
      "Epoch 00002: val_loss improved from 0.13382 to 0.12971, saving model to data/My_Implementations/Trial_6/model_mohamed.h5\n",
      "276/276 [==============================] - 48s 173ms/step - loss: 0.1321 - accuracy: 0.9581 - val_loss: 0.1297 - val_accuracy: 0.9581\n",
      "Epoch 3/1024\n",
      "186/276 [===================>..........] - ETA: 1:44 - loss: 0.1263 - accuracy: 0.95 - ETA: 1:12 - loss: 0.1178 - accuracy: 0.95 - ETA: 1:01 - loss: 0.1189 - accuracy: 0.96 - ETA: 56s - loss: 0.1203 - accuracy: 0.9599 - ETA: 53s - loss: 0.1289 - accuracy: 0.956 - ETA: 50s - loss: 0.1234 - accuracy: 0.959 - ETA: 49s - loss: 0.1252 - accuracy: 0.958 - ETA: 47s - loss: 0.1238 - accuracy: 0.958 - ETA: 46s - loss: 0.1254 - accuracy: 0.957 - ETA: 46s - loss: 0.1287 - accuracy: 0.956 - ETA: 45s - loss: 0.1255 - accuracy: 0.957 - ETA: 44s - loss: 0.1234 - accuracy: 0.958 - ETA: 44s - loss: 0.1224 - accuracy: 0.959 - ETA: 43s - loss: 0.1225 - accuracy: 0.958 - ETA: 43s - loss: 0.1207 - accuracy: 0.959 - ETA: 42s - loss: 0.1208 - accuracy: 0.959 - ETA: 42s - loss: 0.1197 - accuracy: 0.960 - ETA: 42s - loss: 0.1199 - accuracy: 0.960 - ETA: 41s - loss: 0.1201 - accuracy: 0.960 - ETA: 41s - loss: 0.1187 - accuracy: 0.960 - ETA: 41s - loss: 0.1189 - accuracy: 0.960 - ETA: 40s - loss: 0.1173 - accuracy: 0.961 - ETA: 40s - loss: 0.1164 - accuracy: 0.961 - ETA: 40s - loss: 0.1158 - accuracy: 0.962 - ETA: 40s - loss: 0.1163 - accuracy: 0.961 - ETA: 39s - loss: 0.1177 - accuracy: 0.961 - ETA: 39s - loss: 0.1171 - accuracy: 0.961 - ETA: 39s - loss: 0.1157 - accuracy: 0.961 - ETA: 39s - loss: 0.1163 - accuracy: 0.961 - ETA: 38s - loss: 0.1168 - accuracy: 0.961 - ETA: 38s - loss: 0.1166 - accuracy: 0.961 - ETA: 38s - loss: 0.1167 - accuracy: 0.961 - ETA: 38s - loss: 0.1151 - accuracy: 0.962 - ETA: 38s - loss: 0.1157 - accuracy: 0.961 - ETA: 37s - loss: 0.1170 - accuracy: 0.961 - ETA: 37s - loss: 0.1173 - accuracy: 0.961 - ETA: 37s - loss: 0.1174 - accuracy: 0.961 - ETA: 37s - loss: 0.1175 - accuracy: 0.961 - ETA: 37s - loss: 0.1180 - accuracy: 0.961 - ETA: 36s - loss: 0.1183 - accuracy: 0.961 - ETA: 36s - loss: 0.1181 - accuracy: 0.961 - ETA: 36s - loss: 0.1187 - accuracy: 0.961 - ETA: 36s - loss: 0.1188 - accuracy: 0.961 - ETA: 36s - loss: 0.1187 - accuracy: 0.961 - ETA: 36s - loss: 0.1190 - accuracy: 0.961 - ETA: 35s - loss: 0.1186 - accuracy: 0.961 - ETA: 35s - loss: 0.1185 - accuracy: 0.961 - ETA: 35s - loss: 0.1195 - accuracy: 0.960 - ETA: 35s - loss: 0.1202 - accuracy: 0.960 - ETA: 35s - loss: 0.1206 - accuracy: 0.960 - ETA: 34s - loss: 0.1206 - accuracy: 0.960 - ETA: 34s - loss: 0.1215 - accuracy: 0.960 - ETA: 34s - loss: 0.1211 - accuracy: 0.960 - ETA: 34s - loss: 0.1206 - accuracy: 0.960 - ETA: 34s - loss: 0.1203 - accuracy: 0.960 - ETA: 34s - loss: 0.1203 - accuracy: 0.960 - ETA: 33s - loss: 0.1201 - accuracy: 0.960 - ETA: 33s - loss: 0.1201 - accuracy: 0.960 - ETA: 33s - loss: 0.1203 - accuracy: 0.960 - ETA: 33s - loss: 0.1208 - accuracy: 0.960 - ETA: 33s - loss: 0.1213 - accuracy: 0.960 - ETA: 33s - loss: 0.1213 - accuracy: 0.960 - ETA: 32s - loss: 0.1221 - accuracy: 0.959 - ETA: 32s - loss: 0.1222 - accuracy: 0.959 - ETA: 32s - loss: 0.1218 - accuracy: 0.959 - ETA: 32s - loss: 0.1214 - accuracy: 0.960 - ETA: 32s - loss: 0.1211 - accuracy: 0.960 - ETA: 32s - loss: 0.1211 - accuracy: 0.960 - ETA: 31s - loss: 0.1216 - accuracy: 0.960 - ETA: 31s - loss: 0.1217 - accuracy: 0.959 - ETA: 31s - loss: 0.1219 - accuracy: 0.959 - ETA: 31s - loss: 0.1221 - accuracy: 0.959 - ETA: 31s - loss: 0.1219 - accuracy: 0.959 - ETA: 31s - loss: 0.1219 - accuracy: 0.959 - ETA: 30s - loss: 0.1221 - accuracy: 0.959 - ETA: 30s - loss: 0.1223 - accuracy: 0.959 - ETA: 30s - loss: 0.1219 - accuracy: 0.959 - ETA: 30s - loss: 0.1222 - accuracy: 0.959 - ETA: 30s - loss: 0.1220 - accuracy: 0.959 - ETA: 30s - loss: 0.1217 - accuracy: 0.960 - ETA: 29s - loss: 0.1215 - accuracy: 0.960 - ETA: 29s - loss: 0.1210 - accuracy: 0.960 - ETA: 29s - loss: 0.1210 - accuracy: 0.960 - ETA: 29s - loss: 0.1214 - accuracy: 0.960 - ETA: 29s - loss: 0.1215 - accuracy: 0.960 - ETA: 29s - loss: 0.1213 - accuracy: 0.960 - ETA: 29s - loss: 0.1213 - accuracy: 0.960 - ETA: 28s - loss: 0.1214 - accuracy: 0.960 - ETA: 28s - loss: 0.1213 - accuracy: 0.960 - ETA: 28s - loss: 0.1210 - accuracy: 0.960 - ETA: 28s - loss: 0.1208 - accuracy: 0.960 - ETA: 28s - loss: 0.1210 - accuracy: 0.960 - ETA: 28s - loss: 0.1208 - accuracy: 0.960 - ETA: 27s - loss: 0.1206 - accuracy: 0.960 - ETA: 27s - loss: 0.1210 - accuracy: 0.960 - ETA: 27s - loss: 0.1209 - accuracy: 0.960 - ETA: 27s - loss: 0.1213 - accuracy: 0.960 - ETA: 27s - loss: 0.1211 - accuracy: 0.960 - ETA: 27s - loss: 0.1211 - accuracy: 0.960 - ETA: 26s - loss: 0.1209 - accuracy: 0.960 - ETA: 26s - loss: 0.1208 - accuracy: 0.960 - ETA: 26s - loss: 0.1208 - accuracy: 0.960 - ETA: 26s - loss: 0.1203 - accuracy: 0.960 - ETA: 26s - loss: 0.1202 - accuracy: 0.960 - ETA: 26s - loss: 0.1200 - accuracy: 0.960 - ETA: 26s - loss: 0.1199 - accuracy: 0.960 - ETA: 25s - loss: 0.1200 - accuracy: 0.960 - ETA: 25s - loss: 0.1206 - accuracy: 0.960 - ETA: 25s - loss: 0.1206 - accuracy: 0.960 - ETA: 25s - loss: 0.1205 - accuracy: 0.960 - ETA: 25s - loss: 0.1207 - accuracy: 0.960 - ETA: 25s - loss: 0.1206 - accuracy: 0.960 - ETA: 24s - loss: 0.1204 - accuracy: 0.960 - ETA: 24s - loss: 0.1201 - accuracy: 0.960 - ETA: 24s - loss: 0.1202 - accuracy: 0.960 - ETA: 24s - loss: 0.1200 - accuracy: 0.960 - ETA: 24s - loss: 0.1202 - accuracy: 0.960 - ETA: 24s - loss: 0.1201 - accuracy: 0.960 - ETA: 24s - loss: 0.1200 - accuracy: 0.960 - ETA: 23s - loss: 0.1202 - accuracy: 0.960 - ETA: 23s - loss: 0.1202 - accuracy: 0.960 - ETA: 23s - loss: 0.1204 - accuracy: 0.960 - ETA: 23s - loss: 0.1205 - accuracy: 0.960 - ETA: 23s - loss: 0.1209 - accuracy: 0.960 - ETA: 23s - loss: 0.1208 - accuracy: 0.960 - ETA: 22s - loss: 0.1209 - accuracy: 0.960 - ETA: 22s - loss: 0.1208 - accuracy: 0.960 - ETA: 22s - loss: 0.1206 - accuracy: 0.960 - ETA: 22s - loss: 0.1206 - accuracy: 0.960 - ETA: 22s - loss: 0.1205 - accuracy: 0.960 - ETA: 22s - loss: 0.1210 - accuracy: 0.960 - ETA: 21s - loss: 0.1209 - accuracy: 0.960 - ETA: 21s - loss: 0.1210 - accuracy: 0.960 - ETA: 21s - loss: 0.1213 - accuracy: 0.960 - ETA: 21s - loss: 0.1214 - accuracy: 0.960 - ETA: 21s - loss: 0.1215 - accuracy: 0.960 - ETA: 21s - loss: 0.1217 - accuracy: 0.960 - ETA: 21s - loss: 0.1221 - accuracy: 0.960 - ETA: 20s - loss: 0.1219 - accuracy: 0.960 - ETA: 20s - loss: 0.1221 - accuracy: 0.960 - ETA: 20s - loss: 0.1220 - accuracy: 0.960 - ETA: 20s - loss: 0.1221 - accuracy: 0.960 - ETA: 20s - loss: 0.1223 - accuracy: 0.960 - ETA: 20s - loss: 0.1226 - accuracy: 0.960 - ETA: 19s - loss: 0.1225 - accuracy: 0.960 - ETA: 19s - loss: 0.1226 - accuracy: 0.960 - ETA: 19s - loss: 0.1225 - accuracy: 0.960 - ETA: 19s - loss: 0.1224 - accuracy: 0.960 - ETA: 19s - loss: 0.1223 - accuracy: 0.960 - ETA: 19s - loss: 0.1223 - accuracy: 0.960 - ETA: 18s - loss: 0.1222 - accuracy: 0.960 - ETA: 18s - loss: 0.1223 - accuracy: 0.960 - ETA: 18s - loss: 0.1227 - accuracy: 0.959 - ETA: 18s - loss: 0.1230 - accuracy: 0.959 - ETA: 18s - loss: 0.1227 - accuracy: 0.960 - ETA: 18s - loss: 0.1229 - accuracy: 0.959 - ETA: 18s - loss: 0.1228 - accuracy: 0.959 - ETA: 17s - loss: 0.1229 - accuracy: 0.959 - ETA: 17s - loss: 0.1229 - accuracy: 0.959 - ETA: 17s - loss: 0.1228 - accuracy: 0.960 - ETA: 17s - loss: 0.1229 - accuracy: 0.959 - ETA: 17s - loss: 0.1230 - accuracy: 0.959 - ETA: 17s - loss: 0.1231 - accuracy: 0.959 - ETA: 17s - loss: 0.1232 - accuracy: 0.959 - ETA: 16s - loss: 0.1230 - accuracy: 0.959 - ETA: 16s - loss: 0.1228 - accuracy: 0.960 - ETA: 16s - loss: 0.1229 - accuracy: 0.959 - ETA: 16s - loss: 0.1229 - accuracy: 0.959 - ETA: 16s - loss: 0.1231 - accuracy: 0.959 - ETA: 16s - loss: 0.1231 - accuracy: 0.959 - ETA: 15s - loss: 0.1231 - accuracy: 0.959 - ETA: 15s - loss: 0.1231 - accuracy: 0.959 - ETA: 15s - loss: 0.1234 - accuracy: 0.959 - ETA: 15s - loss: 0.1233 - accuracy: 0.959 - ETA: 15s - loss: 0.1233 - accuracy: 0.959 - ETA: 15s - loss: 0.1234 - accuracy: 0.959 - ETA: 15s - loss: 0.1234 - accuracy: 0.959 - ETA: 14s - loss: 0.1233 - accuracy: 0.959 - ETA: 14s - loss: 0.1233 - accuracy: 0.959 - ETA: 14s - loss: 0.1234 - accuracy: 0.959 - ETA: 14s - loss: 0.1233 - accuracy: 0.959 - ETA: 14s - loss: 0.1233 - accuracy: 0.959 - ETA: 14s - loss: 0.1236 - accuracy: 0.959 - ETA: 13s - loss: 0.1235 - accuracy: 0.959 - ETA: 13s - loss: 0.1235 - accuracy: 0.959 - ETA: 13s - loss: 0.1235 - accuracy: 0.9596275/276 [============================>.] - ETA: 13s - loss: 0.1236 - accuracy: 0.959 - ETA: 13s - loss: 0.1240 - accuracy: 0.959 - ETA: 13s - loss: 0.1239 - accuracy: 0.959 - ETA: 13s - loss: 0.1237 - accuracy: 0.959 - ETA: 12s - loss: 0.1235 - accuracy: 0.959 - ETA: 12s - loss: 0.1234 - accuracy: 0.959 - ETA: 12s - loss: 0.1235 - accuracy: 0.959 - ETA: 12s - loss: 0.1236 - accuracy: 0.959 - ETA: 12s - loss: 0.1235 - accuracy: 0.959 - ETA: 12s - loss: 0.1237 - accuracy: 0.959 - ETA: 11s - loss: 0.1238 - accuracy: 0.959 - ETA: 11s - loss: 0.1240 - accuracy: 0.959 - ETA: 11s - loss: 0.1242 - accuracy: 0.959 - ETA: 11s - loss: 0.1241 - accuracy: 0.959 - ETA: 11s - loss: 0.1244 - accuracy: 0.959 - ETA: 11s - loss: 0.1243 - accuracy: 0.959 - ETA: 11s - loss: 0.1244 - accuracy: 0.959 - ETA: 10s - loss: 0.1243 - accuracy: 0.959 - ETA: 10s - loss: 0.1242 - accuracy: 0.959 - ETA: 10s - loss: 0.1244 - accuracy: 0.959 - ETA: 10s - loss: 0.1246 - accuracy: 0.959 - ETA: 10s - loss: 0.1247 - accuracy: 0.959 - ETA: 10s - loss: 0.1246 - accuracy: 0.959 - ETA: 10s - loss: 0.1245 - accuracy: 0.959 - ETA: 9s - loss: 0.1245 - accuracy: 0.959 - ETA: 9s - loss: 0.1245 - accuracy: 0.95 - ETA: 9s - loss: 0.1244 - accuracy: 0.95 - ETA: 9s - loss: 0.1243 - accuracy: 0.95 - ETA: 9s - loss: 0.1244 - accuracy: 0.95 - ETA: 9s - loss: 0.1244 - accuracy: 0.95 - ETA: 8s - loss: 0.1245 - accuracy: 0.95 - ETA: 8s - loss: 0.1244 - accuracy: 0.95 - ETA: 8s - loss: 0.1242 - accuracy: 0.95 - ETA: 8s - loss: 0.1242 - accuracy: 0.95 - ETA: 8s - loss: 0.1243 - accuracy: 0.95 - ETA: 8s - loss: 0.1243 - accuracy: 0.95 - ETA: 8s - loss: 0.1245 - accuracy: 0.95 - ETA: 7s - loss: 0.1246 - accuracy: 0.95 - ETA: 7s - loss: 0.1244 - accuracy: 0.95 - ETA: 7s - loss: 0.1244 - accuracy: 0.95 - ETA: 7s - loss: 0.1245 - accuracy: 0.95 - ETA: 7s - loss: 0.1247 - accuracy: 0.95 - ETA: 7s - loss: 0.1247 - accuracy: 0.95 - ETA: 6s - loss: 0.1247 - accuracy: 0.95 - ETA: 6s - loss: 0.1248 - accuracy: 0.95 - ETA: 6s - loss: 0.1247 - accuracy: 0.95 - ETA: 6s - loss: 0.1246 - accuracy: 0.95 - ETA: 6s - loss: 0.1247 - accuracy: 0.95 - ETA: 6s - loss: 0.1247 - accuracy: 0.95 - ETA: 6s - loss: 0.1247 - accuracy: 0.95 - ETA: 5s - loss: 0.1248 - accuracy: 0.95 - ETA: 5s - loss: 0.1251 - accuracy: 0.95 - ETA: 5s - loss: 0.1251 - accuracy: 0.95 - ETA: 5s - loss: 0.1251 - accuracy: 0.95 - ETA: 5s - loss: 0.1250 - accuracy: 0.95 - ETA: 5s - loss: 0.1250 - accuracy: 0.95 - ETA: 4s - loss: 0.1250 - accuracy: 0.95 - ETA: 4s - loss: 0.1248 - accuracy: 0.95 - ETA: 4s - loss: 0.1248 - accuracy: 0.95 - ETA: 4s - loss: 0.1248 - accuracy: 0.95 - ETA: 4s - loss: 0.1246 - accuracy: 0.95 - ETA: 4s - loss: 0.1246 - accuracy: 0.95 - ETA: 4s - loss: 0.1247 - accuracy: 0.95 - ETA: 3s - loss: 0.1246 - accuracy: 0.95 - ETA: 3s - loss: 0.1250 - accuracy: 0.95 - ETA: 3s - loss: 0.1250 - accuracy: 0.95 - ETA: 3s - loss: 0.1249 - accuracy: 0.95 - ETA: 3s - loss: 0.1249 - accuracy: 0.95 - ETA: 3s - loss: 0.1249 - accuracy: 0.95 - ETA: 3s - loss: 0.1249 - accuracy: 0.95 - ETA: 2s - loss: 0.1250 - accuracy: 0.95 - ETA: 2s - loss: 0.1248 - accuracy: 0.95 - ETA: 2s - loss: 0.1247 - accuracy: 0.95 - ETA: 2s - loss: 0.1249 - accuracy: 0.95 - ETA: 2s - loss: 0.1251 - accuracy: 0.95 - ETA: 2s - loss: 0.1251 - accuracy: 0.95 - ETA: 1s - loss: 0.1251 - accuracy: 0.95 - ETA: 1s - loss: 0.1253 - accuracy: 0.95 - ETA: 1s - loss: 0.1254 - accuracy: 0.95 - ETA: 1s - loss: 0.1255 - accuracy: 0.95 - ETA: 1s - loss: 0.1255 - accuracy: 0.95 - ETA: 1s - loss: 0.1255 - accuracy: 0.95 - ETA: 1s - loss: 0.1255 - accuracy: 0.95 - ETA: 0s - loss: 0.1256 - accuracy: 0.95 - ETA: 0s - loss: 0.1254 - accuracy: 0.95 - ETA: 0s - loss: 0.1254 - accuracy: 0.95 - ETA: 0s - loss: 0.1254 - accuracy: 0.95 - ETA: 0s - loss: 0.1255 - accuracy: 0.95 - ETA: 0s - loss: 0.1254 - accuracy: 0.9590\n",
      "Epoch 00003: val_loss improved from 0.12971 to 0.12897, saving model to data/My_Implementations/Trial_6/model_mohamed.h5\n",
      "276/276 [==============================] - 48s 174ms/step - loss: 0.1253 - accuracy: 0.9590 - val_loss: 0.1290 - val_accuracy: 0.9585\n",
      "Epoch 4/1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/276 [===================>..........] - ETA: 1:34 - loss: 0.0734 - accuracy: 0.97 - ETA: 1:07 - loss: 0.0998 - accuracy: 0.96 - ETA: 58s - loss: 0.0986 - accuracy: 0.9676 - ETA: 54s - loss: 0.1158 - accuracy: 0.961 - ETA: 51s - loss: 0.1153 - accuracy: 0.961 - ETA: 49s - loss: 0.1167 - accuracy: 0.960 - ETA: 47s - loss: 0.1207 - accuracy: 0.959 - ETA: 46s - loss: 0.1181 - accuracy: 0.959 - ETA: 46s - loss: 0.1201 - accuracy: 0.959 - ETA: 45s - loss: 0.1218 - accuracy: 0.958 - ETA: 44s - loss: 0.1218 - accuracy: 0.958 - ETA: 44s - loss: 0.1211 - accuracy: 0.958 - ETA: 43s - loss: 0.1215 - accuracy: 0.958 - ETA: 43s - loss: 0.1206 - accuracy: 0.958 - ETA: 42s - loss: 0.1215 - accuracy: 0.958 - ETA: 42s - loss: 0.1218 - accuracy: 0.958 - ETA: 42s - loss: 0.1197 - accuracy: 0.959 - ETA: 41s - loss: 0.1204 - accuracy: 0.959 - ETA: 41s - loss: 0.1210 - accuracy: 0.958 - ETA: 41s - loss: 0.1196 - accuracy: 0.959 - ETA: 40s - loss: 0.1178 - accuracy: 0.960 - ETA: 40s - loss: 0.1179 - accuracy: 0.959 - ETA: 40s - loss: 0.1167 - accuracy: 0.960 - ETA: 40s - loss: 0.1154 - accuracy: 0.960 - ETA: 39s - loss: 0.1148 - accuracy: 0.961 - ETA: 39s - loss: 0.1156 - accuracy: 0.960 - ETA: 39s - loss: 0.1166 - accuracy: 0.960 - ETA: 39s - loss: 0.1167 - accuracy: 0.960 - ETA: 38s - loss: 0.1169 - accuracy: 0.960 - ETA: 38s - loss: 0.1165 - accuracy: 0.960 - ETA: 38s - loss: 0.1170 - accuracy: 0.960 - ETA: 38s - loss: 0.1180 - accuracy: 0.959 - ETA: 38s - loss: 0.1189 - accuracy: 0.959 - ETA: 37s - loss: 0.1192 - accuracy: 0.959 - ETA: 37s - loss: 0.1190 - accuracy: 0.959 - ETA: 37s - loss: 0.1198 - accuracy: 0.959 - ETA: 37s - loss: 0.1196 - accuracy: 0.959 - ETA: 37s - loss: 0.1201 - accuracy: 0.958 - ETA: 36s - loss: 0.1195 - accuracy: 0.959 - ETA: 36s - loss: 0.1189 - accuracy: 0.959 - ETA: 36s - loss: 0.1189 - accuracy: 0.959 - ETA: 36s - loss: 0.1192 - accuracy: 0.959 - ETA: 36s - loss: 0.1190 - accuracy: 0.959 - ETA: 36s - loss: 0.1187 - accuracy: 0.959 - ETA: 35s - loss: 0.1186 - accuracy: 0.959 - ETA: 35s - loss: 0.1193 - accuracy: 0.959 - ETA: 35s - loss: 0.1207 - accuracy: 0.959 - ETA: 35s - loss: 0.1203 - accuracy: 0.959 - ETA: 35s - loss: 0.1198 - accuracy: 0.959 - ETA: 34s - loss: 0.1198 - accuracy: 0.959 - ETA: 34s - loss: 0.1191 - accuracy: 0.959 - ETA: 34s - loss: 0.1189 - accuracy: 0.959 - ETA: 34s - loss: 0.1192 - accuracy: 0.959 - ETA: 34s - loss: 0.1195 - accuracy: 0.959 - ETA: 34s - loss: 0.1194 - accuracy: 0.959 - ETA: 33s - loss: 0.1196 - accuracy: 0.959 - ETA: 33s - loss: 0.1195 - accuracy: 0.959 - ETA: 33s - loss: 0.1199 - accuracy: 0.959 - ETA: 33s - loss: 0.1197 - accuracy: 0.959 - ETA: 33s - loss: 0.1197 - accuracy: 0.959 - ETA: 33s - loss: 0.1196 - accuracy: 0.959 - ETA: 32s - loss: 0.1198 - accuracy: 0.959 - ETA: 32s - loss: 0.1197 - accuracy: 0.959 - ETA: 32s - loss: 0.1194 - accuracy: 0.959 - ETA: 32s - loss: 0.1195 - accuracy: 0.959 - ETA: 32s - loss: 0.1196 - accuracy: 0.959 - ETA: 32s - loss: 0.1190 - accuracy: 0.959 - ETA: 31s - loss: 0.1185 - accuracy: 0.959 - ETA: 31s - loss: 0.1189 - accuracy: 0.959 - ETA: 31s - loss: 0.1193 - accuracy: 0.959 - ETA: 31s - loss: 0.1198 - accuracy: 0.959 - ETA: 31s - loss: 0.1195 - accuracy: 0.959 - ETA: 31s - loss: 0.1196 - accuracy: 0.959 - ETA: 30s - loss: 0.1204 - accuracy: 0.959 - ETA: 30s - loss: 0.1205 - accuracy: 0.959 - ETA: 30s - loss: 0.1210 - accuracy: 0.958 - ETA: 30s - loss: 0.1212 - accuracy: 0.958 - ETA: 30s - loss: 0.1212 - accuracy: 0.958 - ETA: 30s - loss: 0.1211 - accuracy: 0.958 - ETA: 30s - loss: 0.1210 - accuracy: 0.958 - ETA: 29s - loss: 0.1205 - accuracy: 0.959 - ETA: 29s - loss: 0.1212 - accuracy: 0.958 - ETA: 29s - loss: 0.1207 - accuracy: 0.958 - ETA: 29s - loss: 0.1208 - accuracy: 0.958 - ETA: 29s - loss: 0.1207 - accuracy: 0.959 - ETA: 29s - loss: 0.1206 - accuracy: 0.959 - ETA: 28s - loss: 0.1212 - accuracy: 0.958 - ETA: 28s - loss: 0.1209 - accuracy: 0.958 - ETA: 28s - loss: 0.1202 - accuracy: 0.959 - ETA: 28s - loss: 0.1197 - accuracy: 0.959 - ETA: 28s - loss: 0.1198 - accuracy: 0.959 - ETA: 28s - loss: 0.1198 - accuracy: 0.959 - ETA: 27s - loss: 0.1198 - accuracy: 0.959 - ETA: 27s - loss: 0.1198 - accuracy: 0.959 - ETA: 27s - loss: 0.1201 - accuracy: 0.959 - ETA: 27s - loss: 0.1202 - accuracy: 0.959 - ETA: 27s - loss: 0.1202 - accuracy: 0.959 - ETA: 27s - loss: 0.1204 - accuracy: 0.959 - ETA: 27s - loss: 0.1202 - accuracy: 0.959 - ETA: 26s - loss: 0.1202 - accuracy: 0.959 - ETA: 26s - loss: 0.1202 - accuracy: 0.959 - ETA: 26s - loss: 0.1200 - accuracy: 0.959 - ETA: 26s - loss: 0.1198 - accuracy: 0.959 - ETA: 26s - loss: 0.1199 - accuracy: 0.959 - ETA: 26s - loss: 0.1194 - accuracy: 0.959 - ETA: 25s - loss: 0.1194 - accuracy: 0.959 - ETA: 25s - loss: 0.1195 - accuracy: 0.959 - ETA: 25s - loss: 0.1193 - accuracy: 0.959 - ETA: 25s - loss: 0.1196 - accuracy: 0.959 - ETA: 25s - loss: 0.1195 - accuracy: 0.959 - ETA: 25s - loss: 0.1198 - accuracy: 0.959 - ETA: 25s - loss: 0.1199 - accuracy: 0.959 - ETA: 24s - loss: 0.1198 - accuracy: 0.959 - ETA: 24s - loss: 0.1198 - accuracy: 0.959 - ETA: 24s - loss: 0.1199 - accuracy: 0.959 - ETA: 24s - loss: 0.1200 - accuracy: 0.959 - ETA: 24s - loss: 0.1201 - accuracy: 0.959 - ETA: 24s - loss: 0.1200 - accuracy: 0.959 - ETA: 23s - loss: 0.1197 - accuracy: 0.959 - ETA: 23s - loss: 0.1194 - accuracy: 0.959 - ETA: 23s - loss: 0.1192 - accuracy: 0.959 - ETA: 23s - loss: 0.1190 - accuracy: 0.959 - ETA: 23s - loss: 0.1189 - accuracy: 0.960 - ETA: 23s - loss: 0.1190 - accuracy: 0.959 - ETA: 23s - loss: 0.1194 - accuracy: 0.959 - ETA: 22s - loss: 0.1194 - accuracy: 0.959 - ETA: 22s - loss: 0.1193 - accuracy: 0.959 - ETA: 22s - loss: 0.1194 - accuracy: 0.959 - ETA: 22s - loss: 0.1192 - accuracy: 0.959 - ETA: 22s - loss: 0.1191 - accuracy: 0.960 - ETA: 22s - loss: 0.1193 - accuracy: 0.959 - ETA: 21s - loss: 0.1191 - accuracy: 0.960 - ETA: 21s - loss: 0.1191 - accuracy: 0.960 - ETA: 21s - loss: 0.1189 - accuracy: 0.960 - ETA: 21s - loss: 0.1188 - accuracy: 0.960 - ETA: 21s - loss: 0.1187 - accuracy: 0.960 - ETA: 21s - loss: 0.1191 - accuracy: 0.960 - ETA: 21s - loss: 0.1190 - accuracy: 0.960 - ETA: 20s - loss: 0.1190 - accuracy: 0.960 - ETA: 20s - loss: 0.1188 - accuracy: 0.960 - ETA: 20s - loss: 0.1189 - accuracy: 0.960 - ETA: 20s - loss: 0.1187 - accuracy: 0.960 - ETA: 20s - loss: 0.1185 - accuracy: 0.960 - ETA: 20s - loss: 0.1185 - accuracy: 0.960 - ETA: 19s - loss: 0.1183 - accuracy: 0.960 - ETA: 19s - loss: 0.1182 - accuracy: 0.960 - ETA: 19s - loss: 0.1181 - accuracy: 0.960 - ETA: 19s - loss: 0.1182 - accuracy: 0.960 - ETA: 19s - loss: 0.1182 - accuracy: 0.960 - ETA: 19s - loss: 0.1180 - accuracy: 0.960 - ETA: 19s - loss: 0.1178 - accuracy: 0.960 - ETA: 18s - loss: 0.1179 - accuracy: 0.960 - ETA: 18s - loss: 0.1178 - accuracy: 0.960 - ETA: 18s - loss: 0.1182 - accuracy: 0.960 - ETA: 18s - loss: 0.1180 - accuracy: 0.960 - ETA: 18s - loss: 0.1181 - accuracy: 0.960 - ETA: 18s - loss: 0.1180 - accuracy: 0.960 - ETA: 17s - loss: 0.1179 - accuracy: 0.960 - ETA: 17s - loss: 0.1178 - accuracy: 0.960 - ETA: 17s - loss: 0.1177 - accuracy: 0.960 - ETA: 17s - loss: 0.1179 - accuracy: 0.960 - ETA: 17s - loss: 0.1183 - accuracy: 0.960 - ETA: 17s - loss: 0.1180 - accuracy: 0.960 - ETA: 17s - loss: 0.1180 - accuracy: 0.960 - ETA: 16s - loss: 0.1180 - accuracy: 0.960 - ETA: 16s - loss: 0.1181 - accuracy: 0.960 - ETA: 16s - loss: 0.1182 - accuracy: 0.960 - ETA: 16s - loss: 0.1180 - accuracy: 0.960 - ETA: 16s - loss: 0.1179 - accuracy: 0.960 - ETA: 16s - loss: 0.1180 - accuracy: 0.960 - ETA: 15s - loss: 0.1180 - accuracy: 0.960 - ETA: 15s - loss: 0.1179 - accuracy: 0.960 - ETA: 15s - loss: 0.1180 - accuracy: 0.960 - ETA: 15s - loss: 0.1183 - accuracy: 0.960 - ETA: 15s - loss: 0.1184 - accuracy: 0.960 - ETA: 15s - loss: 0.1183 - accuracy: 0.960 - ETA: 15s - loss: 0.1184 - accuracy: 0.960 - ETA: 14s - loss: 0.1184 - accuracy: 0.960 - ETA: 14s - loss: 0.1185 - accuracy: 0.960 - ETA: 14s - loss: 0.1186 - accuracy: 0.960 - ETA: 14s - loss: 0.1186 - accuracy: 0.960 - ETA: 14s - loss: 0.1185 - accuracy: 0.960 - ETA: 14s - loss: 0.1184 - accuracy: 0.960 - ETA: 13s - loss: 0.1185 - accuracy: 0.960 - ETA: 13s - loss: 0.1188 - accuracy: 0.960 - ETA: 13s - loss: 0.1186 - accuracy: 0.9603"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/276 [============================>.] - ETA: 13s - loss: 0.1185 - accuracy: 0.960 - ETA: 13s - loss: 0.1185 - accuracy: 0.960 - ETA: 13s - loss: 0.1186 - accuracy: 0.960 - ETA: 13s - loss: 0.1184 - accuracy: 0.960 - ETA: 12s - loss: 0.1186 - accuracy: 0.960 - ETA: 12s - loss: 0.1187 - accuracy: 0.960 - ETA: 12s - loss: 0.1188 - accuracy: 0.960 - ETA: 12s - loss: 0.1192 - accuracy: 0.960 - ETA: 12s - loss: 0.1193 - accuracy: 0.960 - ETA: 12s - loss: 0.1192 - accuracy: 0.960 - ETA: 12s - loss: 0.1194 - accuracy: 0.959 - ETA: 11s - loss: 0.1195 - accuracy: 0.959 - ETA: 11s - loss: 0.1193 - accuracy: 0.959 - ETA: 11s - loss: 0.1192 - accuracy: 0.960 - ETA: 11s - loss: 0.1194 - accuracy: 0.959 - ETA: 11s - loss: 0.1194 - accuracy: 0.959 - ETA: 11s - loss: 0.1192 - accuracy: 0.960 - ETA: 10s - loss: 0.1191 - accuracy: 0.960 - ETA: 10s - loss: 0.1190 - accuracy: 0.960 - ETA: 10s - loss: 0.1189 - accuracy: 0.960 - ETA: 10s - loss: 0.1188 - accuracy: 0.960 - ETA: 10s - loss: 0.1188 - accuracy: 0.960 - ETA: 10s - loss: 0.1189 - accuracy: 0.960 - ETA: 10s - loss: 0.1187 - accuracy: 0.960 - ETA: 9s - loss: 0.1187 - accuracy: 0.960 - ETA: 9s - loss: 0.1187 - accuracy: 0.96 - ETA: 9s - loss: 0.1186 - accuracy: 0.96 - ETA: 9s - loss: 0.1187 - accuracy: 0.96 - ETA: 9s - loss: 0.1189 - accuracy: 0.96 - ETA: 9s - loss: 0.1190 - accuracy: 0.96 - ETA: 8s - loss: 0.1192 - accuracy: 0.96 - ETA: 8s - loss: 0.1191 - accuracy: 0.96 - ETA: 8s - loss: 0.1191 - accuracy: 0.96 - ETA: 8s - loss: 0.1192 - accuracy: 0.96 - ETA: 8s - loss: 0.1193 - accuracy: 0.96 - ETA: 8s - loss: 0.1192 - accuracy: 0.96 - ETA: 8s - loss: 0.1193 - accuracy: 0.96 - ETA: 7s - loss: 0.1191 - accuracy: 0.96 - ETA: 7s - loss: 0.1192 - accuracy: 0.96 - ETA: 7s - loss: 0.1191 - accuracy: 0.96 - ETA: 7s - loss: 0.1190 - accuracy: 0.96 - ETA: 7s - loss: 0.1191 - accuracy: 0.96 - ETA: 7s - loss: 0.1191 - accuracy: 0.96 - ETA: 6s - loss: 0.1190 - accuracy: 0.96 - ETA: 6s - loss: 0.1190 - accuracy: 0.96 - ETA: 6s - loss: 0.1192 - accuracy: 0.96 - ETA: 6s - loss: 0.1190 - accuracy: 0.96 - ETA: 6s - loss: 0.1191 - accuracy: 0.96 - ETA: 6s - loss: 0.1190 - accuracy: 0.96 - ETA: 6s - loss: 0.1192 - accuracy: 0.95 - ETA: 5s - loss: 0.1191 - accuracy: 0.96 - ETA: 5s - loss: 0.1194 - accuracy: 0.95 - ETA: 5s - loss: 0.1194 - accuracy: 0.95 - ETA: 5s - loss: 0.1194 - accuracy: 0.95 - ETA: 5s - loss: 0.1194 - accuracy: 0.95 - ETA: 5s - loss: 0.1194 - accuracy: 0.95 - ETA: 4s - loss: 0.1192 - accuracy: 0.96 - ETA: 4s - loss: 0.1191 - accuracy: 0.96 - ETA: 4s - loss: 0.1193 - accuracy: 0.96 - ETA: 4s - loss: 0.1192 - accuracy: 0.96 - ETA: 4s - loss: 0.1193 - accuracy: 0.96 - ETA: 4s - loss: 0.1191 - accuracy: 0.96 - ETA: 4s - loss: 0.1190 - accuracy: 0.96 - ETA: 3s - loss: 0.1191 - accuracy: 0.96 - ETA: 3s - loss: 0.1191 - accuracy: 0.96 - ETA: 3s - loss: 0.1191 - accuracy: 0.96 - ETA: 3s - loss: 0.1190 - accuracy: 0.96 - ETA: 3s - loss: 0.1191 - accuracy: 0.96 - ETA: 3s - loss: 0.1189 - accuracy: 0.96 - ETA: 3s - loss: 0.1190 - accuracy: 0.96 - ETA: 2s - loss: 0.1190 - accuracy: 0.96 - ETA: 2s - loss: 0.1190 - accuracy: 0.96 - ETA: 2s - loss: 0.1189 - accuracy: 0.96 - ETA: 2s - loss: 0.1190 - accuracy: 0.96 - ETA: 2s - loss: 0.1189 - accuracy: 0.96 - ETA: 2s - loss: 0.1188 - accuracy: 0.96 - ETA: 1s - loss: 0.1189 - accuracy: 0.96 - ETA: 1s - loss: 0.1189 - accuracy: 0.96 - ETA: 1s - loss: 0.1187 - accuracy: 0.96 - ETA: 1s - loss: 0.1190 - accuracy: 0.96 - ETA: 1s - loss: 0.1188 - accuracy: 0.96 - ETA: 1s - loss: 0.1188 - accuracy: 0.96 - ETA: 1s - loss: 0.1189 - accuracy: 0.96 - ETA: 0s - loss: 0.1188 - accuracy: 0.96 - ETA: 0s - loss: 0.1189 - accuracy: 0.96 - ETA: 0s - loss: 0.1190 - accuracy: 0.96 - ETA: 0s - loss: 0.1188 - accuracy: 0.96 - ETA: 0s - loss: 0.1188 - accuracy: 0.96 - ETA: 0s - loss: 0.1187 - accuracy: 0.9601\n",
      "Epoch 00004: val_loss improved from 0.12897 to 0.12622, saving model to data/My_Implementations/Trial_6/model_mohamed.h5\n",
      "276/276 [==============================] - 48s 173ms/step - loss: 0.1187 - accuracy: 0.9601 - val_loss: 0.1262 - val_accuracy: 0.9587\n",
      "Epoch 5/1024\n",
      "186/276 [===================>..........] - ETA: 1:34 - loss: 0.0917 - accuracy: 0.96 - ETA: 1:07 - loss: 0.0964 - accuracy: 0.96 - ETA: 58s - loss: 0.0915 - accuracy: 0.9697 - ETA: 54s - loss: 0.0985 - accuracy: 0.967 - ETA: 51s - loss: 0.1025 - accuracy: 0.965 - ETA: 49s - loss: 0.1011 - accuracy: 0.966 - ETA: 47s - loss: 0.0992 - accuracy: 0.967 - ETA: 46s - loss: 0.1022 - accuracy: 0.966 - ETA: 45s - loss: 0.1103 - accuracy: 0.962 - ETA: 45s - loss: 0.1070 - accuracy: 0.964 - ETA: 44s - loss: 0.1061 - accuracy: 0.964 - ETA: 44s - loss: 0.1061 - accuracy: 0.964 - ETA: 43s - loss: 0.1070 - accuracy: 0.963 - ETA: 43s - loss: 0.1078 - accuracy: 0.963 - ETA: 42s - loss: 0.1081 - accuracy: 0.963 - ETA: 42s - loss: 0.1078 - accuracy: 0.963 - ETA: 42s - loss: 0.1096 - accuracy: 0.962 - ETA: 41s - loss: 0.1095 - accuracy: 0.962 - ETA: 41s - loss: 0.1077 - accuracy: 0.963 - ETA: 41s - loss: 0.1077 - accuracy: 0.963 - ETA: 40s - loss: 0.1079 - accuracy: 0.963 - ETA: 40s - loss: 0.1085 - accuracy: 0.963 - ETA: 40s - loss: 0.1078 - accuracy: 0.963 - ETA: 40s - loss: 0.1072 - accuracy: 0.963 - ETA: 39s - loss: 0.1071 - accuracy: 0.963 - ETA: 39s - loss: 0.1074 - accuracy: 0.963 - ETA: 39s - loss: 0.1074 - accuracy: 0.963 - ETA: 39s - loss: 0.1078 - accuracy: 0.963 - ETA: 38s - loss: 0.1081 - accuracy: 0.963 - ETA: 38s - loss: 0.1085 - accuracy: 0.963 - ETA: 38s - loss: 0.1078 - accuracy: 0.963 - ETA: 38s - loss: 0.1094 - accuracy: 0.962 - ETA: 38s - loss: 0.1103 - accuracy: 0.962 - ETA: 37s - loss: 0.1108 - accuracy: 0.962 - ETA: 37s - loss: 0.1109 - accuracy: 0.962 - ETA: 37s - loss: 0.1105 - accuracy: 0.962 - ETA: 37s - loss: 0.1107 - accuracy: 0.962 - ETA: 37s - loss: 0.1112 - accuracy: 0.962 - ETA: 36s - loss: 0.1109 - accuracy: 0.962 - ETA: 36s - loss: 0.1108 - accuracy: 0.962 - ETA: 36s - loss: 0.1103 - accuracy: 0.962 - ETA: 36s - loss: 0.1105 - accuracy: 0.962 - ETA: 36s - loss: 0.1108 - accuracy: 0.962 - ETA: 36s - loss: 0.1110 - accuracy: 0.962 - ETA: 35s - loss: 0.1112 - accuracy: 0.962 - ETA: 35s - loss: 0.1106 - accuracy: 0.962 - ETA: 35s - loss: 0.1108 - accuracy: 0.962 - ETA: 35s - loss: 0.1108 - accuracy: 0.962 - ETA: 35s - loss: 0.1105 - accuracy: 0.962 - ETA: 35s - loss: 0.1103 - accuracy: 0.962 - ETA: 34s - loss: 0.1107 - accuracy: 0.962 - ETA: 34s - loss: 0.1107 - accuracy: 0.962 - ETA: 34s - loss: 0.1105 - accuracy: 0.962 - ETA: 34s - loss: 0.1110 - accuracy: 0.962 - ETA: 34s - loss: 0.1113 - accuracy: 0.962 - ETA: 33s - loss: 0.1107 - accuracy: 0.962 - ETA: 33s - loss: 0.1104 - accuracy: 0.962 - ETA: 33s - loss: 0.1105 - accuracy: 0.962 - ETA: 33s - loss: 0.1101 - accuracy: 0.962 - ETA: 33s - loss: 0.1104 - accuracy: 0.962 - ETA: 33s - loss: 0.1106 - accuracy: 0.962 - ETA: 32s - loss: 0.1105 - accuracy: 0.962 - ETA: 32s - loss: 0.1098 - accuracy: 0.962 - ETA: 32s - loss: 0.1094 - accuracy: 0.962 - ETA: 32s - loss: 0.1097 - accuracy: 0.962 - ETA: 32s - loss: 0.1097 - accuracy: 0.962 - ETA: 32s - loss: 0.1097 - accuracy: 0.962 - ETA: 31s - loss: 0.1099 - accuracy: 0.962 - ETA: 31s - loss: 0.1095 - accuracy: 0.962 - ETA: 31s - loss: 0.1091 - accuracy: 0.962 - ETA: 31s - loss: 0.1087 - accuracy: 0.963 - ETA: 31s - loss: 0.1094 - accuracy: 0.962 - ETA: 31s - loss: 0.1100 - accuracy: 0.962 - ETA: 31s - loss: 0.1103 - accuracy: 0.962 - ETA: 30s - loss: 0.1106 - accuracy: 0.962 - ETA: 30s - loss: 0.1106 - accuracy: 0.962 - ETA: 30s - loss: 0.1106 - accuracy: 0.962 - ETA: 30s - loss: 0.1107 - accuracy: 0.962 - ETA: 30s - loss: 0.1105 - accuracy: 0.962 - ETA: 30s - loss: 0.1111 - accuracy: 0.962 - ETA: 29s - loss: 0.1110 - accuracy: 0.962 - ETA: 29s - loss: 0.1106 - accuracy: 0.962 - ETA: 29s - loss: 0.1104 - accuracy: 0.962 - ETA: 29s - loss: 0.1101 - accuracy: 0.962 - ETA: 29s - loss: 0.1103 - accuracy: 0.962 - ETA: 29s - loss: 0.1106 - accuracy: 0.962 - ETA: 28s - loss: 0.1105 - accuracy: 0.962 - ETA: 28s - loss: 0.1108 - accuracy: 0.962 - ETA: 28s - loss: 0.1107 - accuracy: 0.962 - ETA: 28s - loss: 0.1109 - accuracy: 0.962 - ETA: 28s - loss: 0.1111 - accuracy: 0.962 - ETA: 28s - loss: 0.1111 - accuracy: 0.962 - ETA: 27s - loss: 0.1115 - accuracy: 0.962 - ETA: 27s - loss: 0.1114 - accuracy: 0.962 - ETA: 27s - loss: 0.1116 - accuracy: 0.961 - ETA: 27s - loss: 0.1119 - accuracy: 0.961 - ETA: 27s - loss: 0.1118 - accuracy: 0.961 - ETA: 27s - loss: 0.1117 - accuracy: 0.961 - ETA: 26s - loss: 0.1116 - accuracy: 0.962 - ETA: 26s - loss: 0.1114 - accuracy: 0.962 - ETA: 26s - loss: 0.1113 - accuracy: 0.962 - ETA: 26s - loss: 0.1109 - accuracy: 0.962 - ETA: 26s - loss: 0.1110 - accuracy: 0.962 - ETA: 26s - loss: 0.1112 - accuracy: 0.962 - ETA: 25s - loss: 0.1109 - accuracy: 0.962 - ETA: 25s - loss: 0.1106 - accuracy: 0.962 - ETA: 25s - loss: 0.1104 - accuracy: 0.962 - ETA: 25s - loss: 0.1102 - accuracy: 0.962 - ETA: 25s - loss: 0.1103 - accuracy: 0.962 - ETA: 25s - loss: 0.1105 - accuracy: 0.962 - ETA: 25s - loss: 0.1108 - accuracy: 0.962 - ETA: 24s - loss: 0.1109 - accuracy: 0.962 - ETA: 24s - loss: 0.1105 - accuracy: 0.962 - ETA: 24s - loss: 0.1104 - accuracy: 0.962 - ETA: 24s - loss: 0.1101 - accuracy: 0.962 - ETA: 24s - loss: 0.1099 - accuracy: 0.962 - ETA: 24s - loss: 0.1101 - accuracy: 0.962 - ETA: 23s - loss: 0.1104 - accuracy: 0.962 - ETA: 23s - loss: 0.1104 - accuracy: 0.962 - ETA: 23s - loss: 0.1103 - accuracy: 0.962 - ETA: 23s - loss: 0.1101 - accuracy: 0.962 - ETA: 23s - loss: 0.1101 - accuracy: 0.962 - ETA: 23s - loss: 0.1103 - accuracy: 0.962 - ETA: 23s - loss: 0.1103 - accuracy: 0.962 - ETA: 22s - loss: 0.1105 - accuracy: 0.962 - ETA: 22s - loss: 0.1103 - accuracy: 0.962 - ETA: 22s - loss: 0.1103 - accuracy: 0.962 - ETA: 22s - loss: 0.1101 - accuracy: 0.962 - ETA: 22s - loss: 0.1099 - accuracy: 0.962 - ETA: 22s - loss: 0.1099 - accuracy: 0.962 - ETA: 21s - loss: 0.1100 - accuracy: 0.962 - ETA: 21s - loss: 0.1099 - accuracy: 0.962 - ETA: 21s - loss: 0.1100 - accuracy: 0.962 - ETA: 21s - loss: 0.1099 - accuracy: 0.962 - ETA: 21s - loss: 0.1101 - accuracy: 0.962 - ETA: 21s - loss: 0.1103 - accuracy: 0.962 - ETA: 21s - loss: 0.1103 - accuracy: 0.962 - ETA: 20s - loss: 0.1101 - accuracy: 0.962 - ETA: 20s - loss: 0.1105 - accuracy: 0.962 - ETA: 20s - loss: 0.1103 - accuracy: 0.962 - ETA: 20s - loss: 0.1101 - accuracy: 0.962 - ETA: 20s - loss: 0.1103 - accuracy: 0.962 - ETA: 20s - loss: 0.1101 - accuracy: 0.962 - ETA: 20s - loss: 0.1101 - accuracy: 0.962 - ETA: 19s - loss: 0.1100 - accuracy: 0.962 - ETA: 19s - loss: 0.1101 - accuracy: 0.962 - ETA: 19s - loss: 0.1100 - accuracy: 0.962 - ETA: 19s - loss: 0.1100 - accuracy: 0.962 - ETA: 19s - loss: 0.1100 - accuracy: 0.962 - ETA: 19s - loss: 0.1101 - accuracy: 0.962 - ETA: 18s - loss: 0.1100 - accuracy: 0.962 - ETA: 18s - loss: 0.1100 - accuracy: 0.962 - ETA: 18s - loss: 0.1099 - accuracy: 0.962 - ETA: 18s - loss: 0.1100 - accuracy: 0.962 - ETA: 18s - loss: 0.1100 - accuracy: 0.962 - ETA: 18s - loss: 0.1100 - accuracy: 0.962 - ETA: 18s - loss: 0.1102 - accuracy: 0.962 - ETA: 17s - loss: 0.1105 - accuracy: 0.962 - ETA: 17s - loss: 0.1104 - accuracy: 0.962 - ETA: 17s - loss: 0.1105 - accuracy: 0.962 - ETA: 17s - loss: 0.1106 - accuracy: 0.962 - ETA: 17s - loss: 0.1105 - accuracy: 0.962 - ETA: 17s - loss: 0.1110 - accuracy: 0.962 - ETA: 16s - loss: 0.1110 - accuracy: 0.962 - ETA: 16s - loss: 0.1110 - accuracy: 0.962 - ETA: 16s - loss: 0.1108 - accuracy: 0.962 - ETA: 16s - loss: 0.1106 - accuracy: 0.962 - ETA: 16s - loss: 0.1105 - accuracy: 0.962 - ETA: 16s - loss: 0.1108 - accuracy: 0.962 - ETA: 16s - loss: 0.1107 - accuracy: 0.962 - ETA: 15s - loss: 0.1107 - accuracy: 0.962 - ETA: 15s - loss: 0.1110 - accuracy: 0.962 - ETA: 15s - loss: 0.1111 - accuracy: 0.962 - ETA: 15s - loss: 0.1114 - accuracy: 0.962 - ETA: 15s - loss: 0.1114 - accuracy: 0.961 - ETA: 15s - loss: 0.1113 - accuracy: 0.962 - ETA: 14s - loss: 0.1116 - accuracy: 0.961 - ETA: 14s - loss: 0.1115 - accuracy: 0.961 - ETA: 14s - loss: 0.1118 - accuracy: 0.961 - ETA: 14s - loss: 0.1119 - accuracy: 0.961 - ETA: 14s - loss: 0.1119 - accuracy: 0.961 - ETA: 14s - loss: 0.1120 - accuracy: 0.961 - ETA: 14s - loss: 0.1120 - accuracy: 0.961 - ETA: 13s - loss: 0.1118 - accuracy: 0.961 - ETA: 13s - loss: 0.1116 - accuracy: 0.961 - ETA: 13s - loss: 0.1116 - accuracy: 0.9618275/276 [============================>.] - ETA: 13s - loss: 0.1115 - accuracy: 0.961 - ETA: 13s - loss: 0.1114 - accuracy: 0.961 - ETA: 13s - loss: 0.1115 - accuracy: 0.961 - ETA: 13s - loss: 0.1115 - accuracy: 0.961 - ETA: 12s - loss: 0.1114 - accuracy: 0.961 - ETA: 12s - loss: 0.1114 - accuracy: 0.961 - ETA: 12s - loss: 0.1114 - accuracy: 0.961 - ETA: 12s - loss: 0.1114 - accuracy: 0.961 - ETA: 12s - loss: 0.1114 - accuracy: 0.961 - ETA: 12s - loss: 0.1111 - accuracy: 0.962 - ETA: 11s - loss: 0.1111 - accuracy: 0.962 - ETA: 11s - loss: 0.1110 - accuracy: 0.962 - ETA: 11s - loss: 0.1109 - accuracy: 0.962 - ETA: 11s - loss: 0.1110 - accuracy: 0.962 - ETA: 11s - loss: 0.1110 - accuracy: 0.962 - ETA: 11s - loss: 0.1110 - accuracy: 0.962 - ETA: 11s - loss: 0.1110 - accuracy: 0.962 - ETA: 10s - loss: 0.1111 - accuracy: 0.962 - ETA: 10s - loss: 0.1109 - accuracy: 0.962 - ETA: 10s - loss: 0.1109 - accuracy: 0.962 - ETA: 10s - loss: 0.1109 - accuracy: 0.962 - ETA: 10s - loss: 0.1107 - accuracy: 0.962 - ETA: 10s - loss: 0.1109 - accuracy: 0.962 - ETA: 9s - loss: 0.1110 - accuracy: 0.962 - ETA: 9s - loss: 0.1110 - accuracy: 0.96 - ETA: 9s - loss: 0.1112 - accuracy: 0.96 - ETA: 9s - loss: 0.1110 - accuracy: 0.96 - ETA: 9s - loss: 0.1112 - accuracy: 0.96 - ETA: 9s - loss: 0.1111 - accuracy: 0.96 - ETA: 9s - loss: 0.1111 - accuracy: 0.96 - ETA: 8s - loss: 0.1111 - accuracy: 0.96 - ETA: 8s - loss: 0.1111 - accuracy: 0.96 - ETA: 8s - loss: 0.1111 - accuracy: 0.96 - ETA: 8s - loss: 0.1111 - accuracy: 0.96 - ETA: 8s - loss: 0.1113 - accuracy: 0.96 - ETA: 8s - loss: 0.1115 - accuracy: 0.96 - ETA: 8s - loss: 0.1116 - accuracy: 0.96 - ETA: 7s - loss: 0.1116 - accuracy: 0.96 - ETA: 7s - loss: 0.1117 - accuracy: 0.96 - ETA: 7s - loss: 0.1116 - accuracy: 0.96 - ETA: 7s - loss: 0.1116 - accuracy: 0.96 - ETA: 7s - loss: 0.1117 - accuracy: 0.96 - ETA: 7s - loss: 0.1116 - accuracy: 0.96 - ETA: 6s - loss: 0.1116 - accuracy: 0.96 - ETA: 6s - loss: 0.1117 - accuracy: 0.96 - ETA: 6s - loss: 0.1117 - accuracy: 0.96 - ETA: 6s - loss: 0.1116 - accuracy: 0.96 - ETA: 6s - loss: 0.1119 - accuracy: 0.96 - ETA: 6s - loss: 0.1117 - accuracy: 0.96 - ETA: 6s - loss: 0.1116 - accuracy: 0.96 - ETA: 5s - loss: 0.1115 - accuracy: 0.96 - ETA: 5s - loss: 0.1116 - accuracy: 0.96 - ETA: 5s - loss: 0.1116 - accuracy: 0.96 - ETA: 5s - loss: 0.1117 - accuracy: 0.96 - ETA: 5s - loss: 0.1117 - accuracy: 0.96 - ETA: 5s - loss: 0.1117 - accuracy: 0.96 - ETA: 4s - loss: 0.1117 - accuracy: 0.96 - ETA: 4s - loss: 0.1116 - accuracy: 0.96 - ETA: 4s - loss: 0.1116 - accuracy: 0.96 - ETA: 4s - loss: 0.1115 - accuracy: 0.96 - ETA: 4s - loss: 0.1118 - accuracy: 0.96 - ETA: 4s - loss: 0.1117 - accuracy: 0.96 - ETA: 4s - loss: 0.1116 - accuracy: 0.96 - ETA: 3s - loss: 0.1116 - accuracy: 0.96 - ETA: 3s - loss: 0.1116 - accuracy: 0.96 - ETA: 3s - loss: 0.1116 - accuracy: 0.96 - ETA: 3s - loss: 0.1117 - accuracy: 0.96 - ETA: 3s - loss: 0.1117 - accuracy: 0.96 - ETA: 3s - loss: 0.1117 - accuracy: 0.96 - ETA: 3s - loss: 0.1118 - accuracy: 0.96 - ETA: 2s - loss: 0.1118 - accuracy: 0.96 - ETA: 2s - loss: 0.1118 - accuracy: 0.96 - ETA: 2s - loss: 0.1118 - accuracy: 0.96 - ETA: 2s - loss: 0.1118 - accuracy: 0.96 - ETA: 2s - loss: 0.1119 - accuracy: 0.96 - ETA: 2s - loss: 0.1119 - accuracy: 0.96 - ETA: 1s - loss: 0.1119 - accuracy: 0.96 - ETA: 1s - loss: 0.1120 - accuracy: 0.96 - ETA: 1s - loss: 0.1122 - accuracy: 0.96 - ETA: 1s - loss: 0.1122 - accuracy: 0.96 - ETA: 1s - loss: 0.1123 - accuracy: 0.96 - ETA: 1s - loss: 0.1122 - accuracy: 0.96 - ETA: 1s - loss: 0.1121 - accuracy: 0.96 - ETA: 0s - loss: 0.1123 - accuracy: 0.96 - ETA: 0s - loss: 0.1124 - accuracy: 0.96 - ETA: 0s - loss: 0.1124 - accuracy: 0.96 - ETA: 0s - loss: 0.1124 - accuracy: 0.96 - ETA: 0s - loss: 0.1124 - accuracy: 0.96 - ETA: 0s - loss: 0.1124 - accuracy: 0.9616\n",
      "Epoch 00005: val_loss did not improve from 0.12622\n",
      "276/276 [==============================] - 46s 166ms/step - loss: 0.1124 - accuracy: 0.9615 - val_loss: 0.1264 - val_accuracy: 0.9587\n",
      "Epoch 6/1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/276 [===================>..........] - ETA: 1:29 - loss: 0.0995 - accuracy: 0.96 - ETA: 1:08 - loss: 0.0878 - accuracy: 0.97 - ETA: 58s - loss: 0.0874 - accuracy: 0.9709 - ETA: 54s - loss: 0.0822 - accuracy: 0.972 - ETA: 51s - loss: 0.0815 - accuracy: 0.972 - ETA: 49s - loss: 0.0802 - accuracy: 0.973 - ETA: 48s - loss: 0.0831 - accuracy: 0.972 - ETA: 46s - loss: 0.0867 - accuracy: 0.970 - ETA: 46s - loss: 0.0888 - accuracy: 0.970 - ETA: 45s - loss: 0.0922 - accuracy: 0.969 - ETA: 44s - loss: 0.0956 - accuracy: 0.968 - ETA: 44s - loss: 0.0952 - accuracy: 0.968 - ETA: 43s - loss: 0.0969 - accuracy: 0.967 - ETA: 43s - loss: 0.0989 - accuracy: 0.967 - ETA: 42s - loss: 0.0990 - accuracy: 0.966 - ETA: 42s - loss: 0.0982 - accuracy: 0.967 - ETA: 41s - loss: 0.0994 - accuracy: 0.966 - ETA: 41s - loss: 0.0985 - accuracy: 0.967 - ETA: 41s - loss: 0.0997 - accuracy: 0.966 - ETA: 41s - loss: 0.1033 - accuracy: 0.965 - ETA: 40s - loss: 0.1028 - accuracy: 0.965 - ETA: 40s - loss: 0.1041 - accuracy: 0.965 - ETA: 40s - loss: 0.1047 - accuracy: 0.964 - ETA: 40s - loss: 0.1033 - accuracy: 0.965 - ETA: 39s - loss: 0.1042 - accuracy: 0.964 - ETA: 39s - loss: 0.1038 - accuracy: 0.965 - ETA: 39s - loss: 0.1048 - accuracy: 0.964 - ETA: 39s - loss: 0.1049 - accuracy: 0.964 - ETA: 38s - loss: 0.1048 - accuracy: 0.964 - ETA: 38s - loss: 0.1036 - accuracy: 0.965 - ETA: 38s - loss: 0.1050 - accuracy: 0.964 - ETA: 38s - loss: 0.1043 - accuracy: 0.964 - ETA: 38s - loss: 0.1048 - accuracy: 0.964 - ETA: 37s - loss: 0.1045 - accuracy: 0.964 - ETA: 37s - loss: 0.1042 - accuracy: 0.964 - ETA: 37s - loss: 0.1044 - accuracy: 0.964 - ETA: 37s - loss: 0.1040 - accuracy: 0.964 - ETA: 37s - loss: 0.1040 - accuracy: 0.964 - ETA: 36s - loss: 0.1038 - accuracy: 0.964 - ETA: 36s - loss: 0.1038 - accuracy: 0.964 - ETA: 36s - loss: 0.1041 - accuracy: 0.964 - ETA: 36s - loss: 0.1035 - accuracy: 0.965 - ETA: 36s - loss: 0.1023 - accuracy: 0.965 - ETA: 35s - loss: 0.1026 - accuracy: 0.965 - ETA: 35s - loss: 0.1025 - accuracy: 0.965 - ETA: 35s - loss: 0.1016 - accuracy: 0.965 - ETA: 35s - loss: 0.1014 - accuracy: 0.965 - ETA: 35s - loss: 0.1012 - accuracy: 0.965 - ETA: 35s - loss: 0.1008 - accuracy: 0.965 - ETA: 34s - loss: 0.1015 - accuracy: 0.965 - ETA: 34s - loss: 0.1020 - accuracy: 0.965 - ETA: 34s - loss: 0.1026 - accuracy: 0.965 - ETA: 34s - loss: 0.1028 - accuracy: 0.965 - ETA: 34s - loss: 0.1030 - accuracy: 0.965 - ETA: 34s - loss: 0.1026 - accuracy: 0.965 - ETA: 33s - loss: 0.1028 - accuracy: 0.965 - ETA: 33s - loss: 0.1030 - accuracy: 0.965 - ETA: 33s - loss: 0.1033 - accuracy: 0.964 - ETA: 33s - loss: 0.1042 - accuracy: 0.964 - ETA: 33s - loss: 0.1042 - accuracy: 0.964 - ETA: 33s - loss: 0.1044 - accuracy: 0.964 - ETA: 32s - loss: 0.1042 - accuracy: 0.964 - ETA: 32s - loss: 0.1044 - accuracy: 0.964 - ETA: 32s - loss: 0.1043 - accuracy: 0.964 - ETA: 32s - loss: 0.1043 - accuracy: 0.964 - ETA: 32s - loss: 0.1042 - accuracy: 0.964 - ETA: 32s - loss: 0.1050 - accuracy: 0.964 - ETA: 31s - loss: 0.1054 - accuracy: 0.963 - ETA: 31s - loss: 0.1054 - accuracy: 0.963 - ETA: 31s - loss: 0.1051 - accuracy: 0.963 - ETA: 31s - loss: 0.1057 - accuracy: 0.963 - ETA: 31s - loss: 0.1057 - accuracy: 0.963 - ETA: 31s - loss: 0.1052 - accuracy: 0.963 - ETA: 30s - loss: 0.1051 - accuracy: 0.963 - ETA: 30s - loss: 0.1048 - accuracy: 0.963 - ETA: 30s - loss: 0.1049 - accuracy: 0.963 - ETA: 30s - loss: 0.1049 - accuracy: 0.963 - ETA: 30s - loss: 0.1048 - accuracy: 0.963 - ETA: 30s - loss: 0.1053 - accuracy: 0.963 - ETA: 30s - loss: 0.1056 - accuracy: 0.963 - ETA: 29s - loss: 0.1056 - accuracy: 0.963 - ETA: 29s - loss: 0.1054 - accuracy: 0.963 - ETA: 29s - loss: 0.1050 - accuracy: 0.963 - ETA: 29s - loss: 0.1055 - accuracy: 0.963 - ETA: 29s - loss: 0.1056 - accuracy: 0.963 - ETA: 29s - loss: 0.1058 - accuracy: 0.963 - ETA: 28s - loss: 0.1059 - accuracy: 0.963 - ETA: 28s - loss: 0.1060 - accuracy: 0.963 - ETA: 28s - loss: 0.1058 - accuracy: 0.963 - ETA: 28s - loss: 0.1057 - accuracy: 0.963 - ETA: 28s - loss: 0.1059 - accuracy: 0.963 - ETA: 28s - loss: 0.1059 - accuracy: 0.963 - ETA: 27s - loss: 0.1057 - accuracy: 0.963 - ETA: 27s - loss: 0.1056 - accuracy: 0.963 - ETA: 27s - loss: 0.1055 - accuracy: 0.963 - ETA: 27s - loss: 0.1055 - accuracy: 0.963 - ETA: 27s - loss: 0.1055 - accuracy: 0.963 - ETA: 27s - loss: 0.1055 - accuracy: 0.963 - ETA: 27s - loss: 0.1055 - accuracy: 0.963 - ETA: 26s - loss: 0.1055 - accuracy: 0.963 - ETA: 26s - loss: 0.1056 - accuracy: 0.963 - ETA: 26s - loss: 0.1058 - accuracy: 0.963 - ETA: 26s - loss: 0.1062 - accuracy: 0.963 - ETA: 26s - loss: 0.1058 - accuracy: 0.963 - ETA: 26s - loss: 0.1057 - accuracy: 0.963 - ETA: 25s - loss: 0.1059 - accuracy: 0.963 - ETA: 25s - loss: 0.1058 - accuracy: 0.963 - ETA: 25s - loss: 0.1059 - accuracy: 0.963 - ETA: 25s - loss: 0.1059 - accuracy: 0.963 - ETA: 25s - loss: 0.1058 - accuracy: 0.963 - ETA: 25s - loss: 0.1061 - accuracy: 0.963 - ETA: 25s - loss: 0.1058 - accuracy: 0.963 - ETA: 24s - loss: 0.1058 - accuracy: 0.963 - ETA: 24s - loss: 0.1058 - accuracy: 0.963 - ETA: 24s - loss: 0.1058 - accuracy: 0.963 - ETA: 24s - loss: 0.1059 - accuracy: 0.963 - ETA: 24s - loss: 0.1061 - accuracy: 0.963 - ETA: 24s - loss: 0.1059 - accuracy: 0.963 - ETA: 23s - loss: 0.1058 - accuracy: 0.963 - ETA: 23s - loss: 0.1062 - accuracy: 0.963 - ETA: 23s - loss: 0.1061 - accuracy: 0.963 - ETA: 23s - loss: 0.1060 - accuracy: 0.963 - ETA: 23s - loss: 0.1059 - accuracy: 0.963 - ETA: 23s - loss: 0.1061 - accuracy: 0.963 - ETA: 23s - loss: 0.1061 - accuracy: 0.963 - ETA: 22s - loss: 0.1061 - accuracy: 0.963 - ETA: 22s - loss: 0.1063 - accuracy: 0.963 - ETA: 22s - loss: 0.1063 - accuracy: 0.963 - ETA: 22s - loss: 0.1061 - accuracy: 0.963 - ETA: 22s - loss: 0.1062 - accuracy: 0.963 - ETA: 22s - loss: 0.1062 - accuracy: 0.963 - ETA: 21s - loss: 0.1060 - accuracy: 0.963 - ETA: 21s - loss: 0.1059 - accuracy: 0.963 - ETA: 21s - loss: 0.1060 - accuracy: 0.963 - ETA: 21s - loss: 0.1060 - accuracy: 0.963 - ETA: 21s - loss: 0.1061 - accuracy: 0.963 - ETA: 21s - loss: 0.1063 - accuracy: 0.963 - ETA: 21s - loss: 0.1065 - accuracy: 0.963 - ETA: 20s - loss: 0.1066 - accuracy: 0.963 - ETA: 20s - loss: 0.1066 - accuracy: 0.963 - ETA: 20s - loss: 0.1064 - accuracy: 0.963 - ETA: 20s - loss: 0.1065 - accuracy: 0.963 - ETA: 20s - loss: 0.1067 - accuracy: 0.963 - ETA: 20s - loss: 0.1068 - accuracy: 0.963 - ETA: 19s - loss: 0.1066 - accuracy: 0.963 - ETA: 19s - loss: 0.1067 - accuracy: 0.963 - ETA: 19s - loss: 0.1065 - accuracy: 0.963 - ETA: 19s - loss: 0.1067 - accuracy: 0.963 - ETA: 19s - loss: 0.1068 - accuracy: 0.963 - ETA: 19s - loss: 0.1070 - accuracy: 0.963 - ETA: 19s - loss: 0.1072 - accuracy: 0.963 - ETA: 18s - loss: 0.1072 - accuracy: 0.963 - ETA: 18s - loss: 0.1070 - accuracy: 0.963 - ETA: 18s - loss: 0.1072 - accuracy: 0.962 - ETA: 18s - loss: 0.1074 - accuracy: 0.962 - ETA: 18s - loss: 0.1075 - accuracy: 0.962 - ETA: 18s - loss: 0.1075 - accuracy: 0.962 - ETA: 17s - loss: 0.1074 - accuracy: 0.962 - ETA: 17s - loss: 0.1074 - accuracy: 0.962 - ETA: 17s - loss: 0.1075 - accuracy: 0.962 - ETA: 17s - loss: 0.1074 - accuracy: 0.962 - ETA: 17s - loss: 0.1076 - accuracy: 0.962 - ETA: 17s - loss: 0.1075 - accuracy: 0.962 - ETA: 17s - loss: 0.1075 - accuracy: 0.962 - ETA: 16s - loss: 0.1076 - accuracy: 0.962 - ETA: 16s - loss: 0.1076 - accuracy: 0.962 - ETA: 16s - loss: 0.1079 - accuracy: 0.962 - ETA: 16s - loss: 0.1080 - accuracy: 0.962 - ETA: 16s - loss: 0.1080 - accuracy: 0.962 - ETA: 16s - loss: 0.1084 - accuracy: 0.962 - ETA: 15s - loss: 0.1083 - accuracy: 0.962 - ETA: 15s - loss: 0.1083 - accuracy: 0.962 - ETA: 15s - loss: 0.1084 - accuracy: 0.962 - ETA: 15s - loss: 0.1083 - accuracy: 0.962 - ETA: 15s - loss: 0.1084 - accuracy: 0.962 - ETA: 15s - loss: 0.1085 - accuracy: 0.962 - ETA: 15s - loss: 0.1085 - accuracy: 0.962 - ETA: 14s - loss: 0.1085 - accuracy: 0.962 - ETA: 14s - loss: 0.1083 - accuracy: 0.962 - ETA: 14s - loss: 0.1082 - accuracy: 0.962 - ETA: 14s - loss: 0.1082 - accuracy: 0.962 - ETA: 14s - loss: 0.1081 - accuracy: 0.962 - ETA: 14s - loss: 0.1083 - accuracy: 0.962 - ETA: 13s - loss: 0.1083 - accuracy: 0.962 - ETA: 13s - loss: 0.1085 - accuracy: 0.962 - ETA: 13s - loss: 0.1085 - accuracy: 0.9624"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/276 [============================>.] - ETA: 13s - loss: 0.1084 - accuracy: 0.962 - ETA: 13s - loss: 0.1084 - accuracy: 0.962 - ETA: 13s - loss: 0.1083 - accuracy: 0.962 - ETA: 13s - loss: 0.1082 - accuracy: 0.962 - ETA: 12s - loss: 0.1082 - accuracy: 0.962 - ETA: 12s - loss: 0.1081 - accuracy: 0.962 - ETA: 12s - loss: 0.1082 - accuracy: 0.962 - ETA: 12s - loss: 0.1082 - accuracy: 0.962 - ETA: 12s - loss: 0.1082 - accuracy: 0.962 - ETA: 12s - loss: 0.1082 - accuracy: 0.962 - ETA: 12s - loss: 0.1082 - accuracy: 0.962 - ETA: 11s - loss: 0.1085 - accuracy: 0.962 - ETA: 11s - loss: 0.1083 - accuracy: 0.962 - ETA: 11s - loss: 0.1083 - accuracy: 0.962 - ETA: 11s - loss: 0.1082 - accuracy: 0.962 - ETA: 11s - loss: 0.1082 - accuracy: 0.962 - ETA: 11s - loss: 0.1081 - accuracy: 0.962 - ETA: 10s - loss: 0.1080 - accuracy: 0.962 - ETA: 10s - loss: 0.1080 - accuracy: 0.962 - ETA: 10s - loss: 0.1081 - accuracy: 0.962 - ETA: 10s - loss: 0.1084 - accuracy: 0.962 - ETA: 10s - loss: 0.1082 - accuracy: 0.962 - ETA: 10s - loss: 0.1081 - accuracy: 0.962 - ETA: 10s - loss: 0.1080 - accuracy: 0.962 - ETA: 9s - loss: 0.1079 - accuracy: 0.962 - ETA: 9s - loss: 0.1078 - accuracy: 0.96 - ETA: 9s - loss: 0.1079 - accuracy: 0.96 - ETA: 9s - loss: 0.1080 - accuracy: 0.96 - ETA: 9s - loss: 0.1080 - accuracy: 0.96 - ETA: 9s - loss: 0.1079 - accuracy: 0.96 - ETA: 8s - loss: 0.1079 - accuracy: 0.96 - ETA: 8s - loss: 0.1078 - accuracy: 0.96 - ETA: 8s - loss: 0.1080 - accuracy: 0.96 - ETA: 8s - loss: 0.1078 - accuracy: 0.96 - ETA: 8s - loss: 0.1078 - accuracy: 0.96 - ETA: 8s - loss: 0.1078 - accuracy: 0.96 - ETA: 8s - loss: 0.1079 - accuracy: 0.96 - ETA: 7s - loss: 0.1078 - accuracy: 0.96 - ETA: 7s - loss: 0.1078 - accuracy: 0.96 - ETA: 7s - loss: 0.1077 - accuracy: 0.96 - ETA: 7s - loss: 0.1077 - accuracy: 0.96 - ETA: 7s - loss: 0.1076 - accuracy: 0.96 - ETA: 7s - loss: 0.1076 - accuracy: 0.96 - ETA: 6s - loss: 0.1074 - accuracy: 0.96 - ETA: 6s - loss: 0.1075 - accuracy: 0.96 - ETA: 6s - loss: 0.1073 - accuracy: 0.96 - ETA: 6s - loss: 0.1073 - accuracy: 0.96 - ETA: 6s - loss: 0.1071 - accuracy: 0.96 - ETA: 6s - loss: 0.1071 - accuracy: 0.96 - ETA: 6s - loss: 0.1071 - accuracy: 0.96 - ETA: 5s - loss: 0.1070 - accuracy: 0.96 - ETA: 5s - loss: 0.1069 - accuracy: 0.96 - ETA: 5s - loss: 0.1070 - accuracy: 0.96 - ETA: 5s - loss: 0.1071 - accuracy: 0.96 - ETA: 5s - loss: 0.1071 - accuracy: 0.96 - ETA: 5s - loss: 0.1070 - accuracy: 0.96 - ETA: 4s - loss: 0.1070 - accuracy: 0.96 - ETA: 4s - loss: 0.1069 - accuracy: 0.96 - ETA: 4s - loss: 0.1067 - accuracy: 0.96 - ETA: 4s - loss: 0.1069 - accuracy: 0.96 - ETA: 4s - loss: 0.1070 - accuracy: 0.96 - ETA: 4s - loss: 0.1070 - accuracy: 0.96 - ETA: 4s - loss: 0.1071 - accuracy: 0.96 - ETA: 3s - loss: 0.1071 - accuracy: 0.96 - ETA: 3s - loss: 0.1071 - accuracy: 0.96 - ETA: 3s - loss: 0.1071 - accuracy: 0.96 - ETA: 3s - loss: 0.1070 - accuracy: 0.96 - ETA: 3s - loss: 0.1070 - accuracy: 0.96 - ETA: 3s - loss: 0.1070 - accuracy: 0.96 - ETA: 3s - loss: 0.1071 - accuracy: 0.96 - ETA: 2s - loss: 0.1071 - accuracy: 0.96 - ETA: 2s - loss: 0.1071 - accuracy: 0.96 - ETA: 2s - loss: 0.1072 - accuracy: 0.96 - ETA: 2s - loss: 0.1071 - accuracy: 0.96 - ETA: 2s - loss: 0.1071 - accuracy: 0.96 - ETA: 2s - loss: 0.1070 - accuracy: 0.96 - ETA: 1s - loss: 0.1071 - accuracy: 0.96 - ETA: 1s - loss: 0.1072 - accuracy: 0.96 - ETA: 1s - loss: 0.1074 - accuracy: 0.96 - ETA: 1s - loss: 0.1074 - accuracy: 0.96 - ETA: 1s - loss: 0.1075 - accuracy: 0.96 - ETA: 1s - loss: 0.1076 - accuracy: 0.96 - ETA: 1s - loss: 0.1075 - accuracy: 0.96 - ETA: 0s - loss: 0.1075 - accuracy: 0.96 - ETA: 0s - loss: 0.1075 - accuracy: 0.96 - ETA: 0s - loss: 0.1077 - accuracy: 0.96 - ETA: 0s - loss: 0.1078 - accuracy: 0.96 - ETA: 0s - loss: 0.1077 - accuracy: 0.96 - ETA: 0s - loss: 0.1077 - accuracy: 0.9626\n",
      "Epoch 00006: val_loss did not improve from 0.12622\n",
      "276/276 [==============================] - 46s 166ms/step - loss: 0.1077 - accuracy: 0.9626 - val_loss: 0.1280 - val_accuracy: 0.9578\n",
      "Epoch 7/1024\n",
      "186/276 [===================>..........] - ETA: 1:51 - loss: 0.0711 - accuracy: 0.97 - ETA: 1:16 - loss: 0.0776 - accuracy: 0.97 - ETA: 1:04 - loss: 0.0883 - accuracy: 0.97 - ETA: 58s - loss: 0.0956 - accuracy: 0.9683 - ETA: 55s - loss: 0.1022 - accuracy: 0.965 - ETA: 52s - loss: 0.1044 - accuracy: 0.964 - ETA: 50s - loss: 0.1054 - accuracy: 0.963 - ETA: 49s - loss: 0.1053 - accuracy: 0.962 - ETA: 48s - loss: 0.1036 - accuracy: 0.963 - ETA: 47s - loss: 0.1032 - accuracy: 0.963 - ETA: 46s - loss: 0.1041 - accuracy: 0.963 - ETA: 45s - loss: 0.1067 - accuracy: 0.961 - ETA: 45s - loss: 0.1071 - accuracy: 0.961 - ETA: 44s - loss: 0.1061 - accuracy: 0.962 - ETA: 43s - loss: 0.1041 - accuracy: 0.962 - ETA: 43s - loss: 0.1026 - accuracy: 0.963 - ETA: 43s - loss: 0.1038 - accuracy: 0.963 - ETA: 42s - loss: 0.1057 - accuracy: 0.962 - ETA: 42s - loss: 0.1047 - accuracy: 0.962 - ETA: 42s - loss: 0.1053 - accuracy: 0.962 - ETA: 41s - loss: 0.1049 - accuracy: 0.962 - ETA: 41s - loss: 0.1055 - accuracy: 0.962 - ETA: 41s - loss: 0.1044 - accuracy: 0.962 - ETA: 40s - loss: 0.1045 - accuracy: 0.962 - ETA: 40s - loss: 0.1052 - accuracy: 0.962 - ETA: 40s - loss: 0.1056 - accuracy: 0.962 - ETA: 40s - loss: 0.1050 - accuracy: 0.962 - ETA: 39s - loss: 0.1041 - accuracy: 0.963 - ETA: 39s - loss: 0.1027 - accuracy: 0.963 - ETA: 39s - loss: 0.1030 - accuracy: 0.963 - ETA: 39s - loss: 0.1023 - accuracy: 0.963 - ETA: 38s - loss: 0.1020 - accuracy: 0.963 - ETA: 38s - loss: 0.1024 - accuracy: 0.963 - ETA: 38s - loss: 0.1025 - accuracy: 0.963 - ETA: 38s - loss: 0.1020 - accuracy: 0.963 - ETA: 38s - loss: 0.1021 - accuracy: 0.963 - ETA: 37s - loss: 0.1034 - accuracy: 0.963 - ETA: 37s - loss: 0.1033 - accuracy: 0.963 - ETA: 37s - loss: 0.1029 - accuracy: 0.963 - ETA: 37s - loss: 0.1027 - accuracy: 0.963 - ETA: 37s - loss: 0.1022 - accuracy: 0.963 - ETA: 36s - loss: 0.1027 - accuracy: 0.963 - ETA: 36s - loss: 0.1025 - accuracy: 0.963 - ETA: 36s - loss: 0.1021 - accuracy: 0.964 - ETA: 36s - loss: 0.1018 - accuracy: 0.964 - ETA: 36s - loss: 0.1021 - accuracy: 0.964 - ETA: 35s - loss: 0.1024 - accuracy: 0.964 - ETA: 35s - loss: 0.1020 - accuracy: 0.964 - ETA: 35s - loss: 0.1018 - accuracy: 0.964 - ETA: 35s - loss: 0.1024 - accuracy: 0.964 - ETA: 35s - loss: 0.1018 - accuracy: 0.964 - ETA: 35s - loss: 0.1022 - accuracy: 0.964 - ETA: 34s - loss: 0.1021 - accuracy: 0.964 - ETA: 34s - loss: 0.1021 - accuracy: 0.964 - ETA: 34s - loss: 0.1017 - accuracy: 0.964 - ETA: 34s - loss: 0.1019 - accuracy: 0.964 - ETA: 34s - loss: 0.1020 - accuracy: 0.964 - ETA: 33s - loss: 0.1023 - accuracy: 0.964 - ETA: 33s - loss: 0.1022 - accuracy: 0.964 - ETA: 33s - loss: 0.1022 - accuracy: 0.964 - ETA: 33s - loss: 0.1022 - accuracy: 0.964 - ETA: 33s - loss: 0.1022 - accuracy: 0.964 - ETA: 33s - loss: 0.1020 - accuracy: 0.964 - ETA: 32s - loss: 0.1020 - accuracy: 0.964 - ETA: 32s - loss: 0.1017 - accuracy: 0.964 - ETA: 32s - loss: 0.1018 - accuracy: 0.964 - ETA: 32s - loss: 0.1014 - accuracy: 0.964 - ETA: 32s - loss: 0.1014 - accuracy: 0.964 - ETA: 32s - loss: 0.1015 - accuracy: 0.964 - ETA: 31s - loss: 0.1015 - accuracy: 0.964 - ETA: 31s - loss: 0.1012 - accuracy: 0.964 - ETA: 31s - loss: 0.1013 - accuracy: 0.964 - ETA: 31s - loss: 0.1014 - accuracy: 0.964 - ETA: 31s - loss: 0.1014 - accuracy: 0.964 - ETA: 31s - loss: 0.1013 - accuracy: 0.964 - ETA: 30s - loss: 0.1014 - accuracy: 0.964 - ETA: 30s - loss: 0.1016 - accuracy: 0.964 - ETA: 30s - loss: 0.1016 - accuracy: 0.964 - ETA: 30s - loss: 0.1018 - accuracy: 0.964 - ETA: 30s - loss: 0.1017 - accuracy: 0.964 - ETA: 30s - loss: 0.1019 - accuracy: 0.964 - ETA: 29s - loss: 0.1018 - accuracy: 0.964 - ETA: 29s - loss: 0.1020 - accuracy: 0.964 - ETA: 29s - loss: 0.1019 - accuracy: 0.964 - ETA: 29s - loss: 0.1017 - accuracy: 0.964 - ETA: 29s - loss: 0.1019 - accuracy: 0.964 - ETA: 29s - loss: 0.1020 - accuracy: 0.964 - ETA: 29s - loss: 0.1021 - accuracy: 0.964 - ETA: 28s - loss: 0.1016 - accuracy: 0.964 - ETA: 28s - loss: 0.1018 - accuracy: 0.964 - ETA: 28s - loss: 0.1016 - accuracy: 0.964 - ETA: 28s - loss: 0.1021 - accuracy: 0.964 - ETA: 28s - loss: 0.1020 - accuracy: 0.964 - ETA: 28s - loss: 0.1018 - accuracy: 0.964 - ETA: 27s - loss: 0.1018 - accuracy: 0.964 - ETA: 27s - loss: 0.1018 - accuracy: 0.964 - ETA: 27s - loss: 0.1022 - accuracy: 0.964 - ETA: 27s - loss: 0.1024 - accuracy: 0.964 - ETA: 27s - loss: 0.1025 - accuracy: 0.963 - ETA: 27s - loss: 0.1024 - accuracy: 0.964 - ETA: 26s - loss: 0.1027 - accuracy: 0.963 - ETA: 26s - loss: 0.1028 - accuracy: 0.963 - ETA: 26s - loss: 0.1032 - accuracy: 0.963 - ETA: 26s - loss: 0.1033 - accuracy: 0.963 - ETA: 26s - loss: 0.1029 - accuracy: 0.963 - ETA: 26s - loss: 0.1027 - accuracy: 0.963 - ETA: 26s - loss: 0.1030 - accuracy: 0.963 - ETA: 25s - loss: 0.1030 - accuracy: 0.963 - ETA: 25s - loss: 0.1031 - accuracy: 0.963 - ETA: 25s - loss: 0.1026 - accuracy: 0.963 - ETA: 25s - loss: 0.1026 - accuracy: 0.963 - ETA: 25s - loss: 0.1024 - accuracy: 0.964 - ETA: 25s - loss: 0.1022 - accuracy: 0.964 - ETA: 24s - loss: 0.1019 - accuracy: 0.964 - ETA: 24s - loss: 0.1018 - accuracy: 0.964 - ETA: 24s - loss: 0.1017 - accuracy: 0.964 - ETA: 24s - loss: 0.1019 - accuracy: 0.964 - ETA: 24s - loss: 0.1018 - accuracy: 0.964 - ETA: 24s - loss: 0.1014 - accuracy: 0.964 - ETA: 23s - loss: 0.1014 - accuracy: 0.964 - ETA: 23s - loss: 0.1011 - accuracy: 0.964 - ETA: 23s - loss: 0.1012 - accuracy: 0.964 - ETA: 23s - loss: 0.1013 - accuracy: 0.964 - ETA: 23s - loss: 0.1017 - accuracy: 0.964 - ETA: 23s - loss: 0.1020 - accuracy: 0.964 - ETA: 23s - loss: 0.1019 - accuracy: 0.964 - ETA: 22s - loss: 0.1019 - accuracy: 0.964 - ETA: 22s - loss: 0.1018 - accuracy: 0.964 - ETA: 22s - loss: 0.1021 - accuracy: 0.964 - ETA: 22s - loss: 0.1022 - accuracy: 0.963 - ETA: 22s - loss: 0.1023 - accuracy: 0.963 - ETA: 22s - loss: 0.1022 - accuracy: 0.963 - ETA: 21s - loss: 0.1026 - accuracy: 0.963 - ETA: 21s - loss: 0.1027 - accuracy: 0.963 - ETA: 21s - loss: 0.1027 - accuracy: 0.963 - ETA: 21s - loss: 0.1030 - accuracy: 0.963 - ETA: 21s - loss: 0.1030 - accuracy: 0.963 - ETA: 21s - loss: 0.1028 - accuracy: 0.963 - ETA: 21s - loss: 0.1027 - accuracy: 0.963 - ETA: 20s - loss: 0.1025 - accuracy: 0.963 - ETA: 20s - loss: 0.1026 - accuracy: 0.963 - ETA: 20s - loss: 0.1026 - accuracy: 0.963 - ETA: 20s - loss: 0.1026 - accuracy: 0.963 - ETA: 20s - loss: 0.1025 - accuracy: 0.963 - ETA: 20s - loss: 0.1027 - accuracy: 0.963 - ETA: 19s - loss: 0.1026 - accuracy: 0.963 - ETA: 19s - loss: 0.1030 - accuracy: 0.963 - ETA: 19s - loss: 0.1030 - accuracy: 0.963 - ETA: 19s - loss: 0.1030 - accuracy: 0.963 - ETA: 19s - loss: 0.1028 - accuracy: 0.963 - ETA: 19s - loss: 0.1029 - accuracy: 0.963 - ETA: 18s - loss: 0.1029 - accuracy: 0.963 - ETA: 18s - loss: 0.1029 - accuracy: 0.963 - ETA: 18s - loss: 0.1028 - accuracy: 0.963 - ETA: 18s - loss: 0.1027 - accuracy: 0.963 - ETA: 18s - loss: 0.1025 - accuracy: 0.963 - ETA: 18s - loss: 0.1025 - accuracy: 0.963 - ETA: 18s - loss: 0.1027 - accuracy: 0.963 - ETA: 17s - loss: 0.1029 - accuracy: 0.963 - ETA: 17s - loss: 0.1031 - accuracy: 0.963 - ETA: 17s - loss: 0.1030 - accuracy: 0.963 - ETA: 17s - loss: 0.1028 - accuracy: 0.963 - ETA: 17s - loss: 0.1029 - accuracy: 0.963 - ETA: 17s - loss: 0.1029 - accuracy: 0.963 - ETA: 16s - loss: 0.1030 - accuracy: 0.963 - ETA: 16s - loss: 0.1030 - accuracy: 0.963 - ETA: 16s - loss: 0.1030 - accuracy: 0.963 - ETA: 16s - loss: 0.1031 - accuracy: 0.963 - ETA: 16s - loss: 0.1030 - accuracy: 0.963 - ETA: 16s - loss: 0.1028 - accuracy: 0.963 - ETA: 16s - loss: 0.1027 - accuracy: 0.963 - ETA: 15s - loss: 0.1028 - accuracy: 0.963 - ETA: 15s - loss: 0.1026 - accuracy: 0.963 - ETA: 15s - loss: 0.1025 - accuracy: 0.963 - ETA: 15s - loss: 0.1026 - accuracy: 0.963 - ETA: 15s - loss: 0.1028 - accuracy: 0.963 - ETA: 15s - loss: 0.1027 - accuracy: 0.963 - ETA: 14s - loss: 0.1028 - accuracy: 0.963 - ETA: 14s - loss: 0.1029 - accuracy: 0.963 - ETA: 14s - loss: 0.1030 - accuracy: 0.963 - ETA: 14s - loss: 0.1031 - accuracy: 0.963 - ETA: 14s - loss: 0.1032 - accuracy: 0.963 - ETA: 14s - loss: 0.1033 - accuracy: 0.963 - ETA: 14s - loss: 0.1032 - accuracy: 0.963 - ETA: 13s - loss: 0.1032 - accuracy: 0.963 - ETA: 13s - loss: 0.1032 - accuracy: 0.9635275/276 [============================>.] - ETA: 13s - loss: 0.1032 - accuracy: 0.963 - ETA: 13s - loss: 0.1030 - accuracy: 0.963 - ETA: 13s - loss: 0.1030 - accuracy: 0.963 - ETA: 13s - loss: 0.1030 - accuracy: 0.963 - ETA: 12s - loss: 0.1029 - accuracy: 0.963 - ETA: 12s - loss: 0.1030 - accuracy: 0.963 - ETA: 12s - loss: 0.1030 - accuracy: 0.963 - ETA: 12s - loss: 0.1029 - accuracy: 0.963 - ETA: 12s - loss: 0.1029 - accuracy: 0.963 - ETA: 12s - loss: 0.1030 - accuracy: 0.963 - ETA: 12s - loss: 0.1029 - accuracy: 0.963 - ETA: 11s - loss: 0.1028 - accuracy: 0.963 - ETA: 11s - loss: 0.1027 - accuracy: 0.963 - ETA: 11s - loss: 0.1025 - accuracy: 0.963 - ETA: 11s - loss: 0.1024 - accuracy: 0.963 - ETA: 11s - loss: 0.1024 - accuracy: 0.963 - ETA: 11s - loss: 0.1023 - accuracy: 0.963 - ETA: 10s - loss: 0.1023 - accuracy: 0.963 - ETA: 10s - loss: 0.1023 - accuracy: 0.963 - ETA: 10s - loss: 0.1022 - accuracy: 0.963 - ETA: 10s - loss: 0.1023 - accuracy: 0.963 - ETA: 10s - loss: 0.1023 - accuracy: 0.963 - ETA: 10s - loss: 0.1024 - accuracy: 0.963 - ETA: 10s - loss: 0.1024 - accuracy: 0.963 - ETA: 9s - loss: 0.1024 - accuracy: 0.963 - ETA: 9s - loss: 0.1023 - accuracy: 0.96 - ETA: 9s - loss: 0.1023 - accuracy: 0.96 - ETA: 9s - loss: 0.1024 - accuracy: 0.96 - ETA: 9s - loss: 0.1023 - accuracy: 0.96 - ETA: 9s - loss: 0.1022 - accuracy: 0.96 - ETA: 8s - loss: 0.1023 - accuracy: 0.96 - ETA: 8s - loss: 0.1026 - accuracy: 0.96 - ETA: 8s - loss: 0.1026 - accuracy: 0.96 - ETA: 8s - loss: 0.1025 - accuracy: 0.96 - ETA: 8s - loss: 0.1026 - accuracy: 0.96 - ETA: 8s - loss: 0.1028 - accuracy: 0.96 - ETA: 8s - loss: 0.1029 - accuracy: 0.96 - ETA: 7s - loss: 0.1027 - accuracy: 0.96 - ETA: 7s - loss: 0.1025 - accuracy: 0.96 - ETA: 7s - loss: 0.1025 - accuracy: 0.96 - ETA: 7s - loss: 0.1024 - accuracy: 0.96 - ETA: 7s - loss: 0.1024 - accuracy: 0.96 - ETA: 7s - loss: 0.1024 - accuracy: 0.96 - ETA: 6s - loss: 0.1023 - accuracy: 0.96 - ETA: 6s - loss: 0.1022 - accuracy: 0.96 - ETA: 6s - loss: 0.1024 - accuracy: 0.96 - ETA: 6s - loss: 0.1026 - accuracy: 0.96 - ETA: 6s - loss: 0.1026 - accuracy: 0.96 - ETA: 6s - loss: 0.1027 - accuracy: 0.96 - ETA: 6s - loss: 0.1027 - accuracy: 0.96 - ETA: 5s - loss: 0.1027 - accuracy: 0.96 - ETA: 5s - loss: 0.1027 - accuracy: 0.96 - ETA: 5s - loss: 0.1027 - accuracy: 0.96 - ETA: 5s - loss: 0.1026 - accuracy: 0.96 - ETA: 5s - loss: 0.1026 - accuracy: 0.96 - ETA: 5s - loss: 0.1025 - accuracy: 0.96 - ETA: 5s - loss: 0.1024 - accuracy: 0.96 - ETA: 4s - loss: 0.1023 - accuracy: 0.96 - ETA: 4s - loss: 0.1023 - accuracy: 0.96 - ETA: 4s - loss: 0.1023 - accuracy: 0.96 - ETA: 4s - loss: 0.1025 - accuracy: 0.96 - ETA: 4s - loss: 0.1023 - accuracy: 0.96 - ETA: 4s - loss: 0.1023 - accuracy: 0.96 - ETA: 3s - loss: 0.1022 - accuracy: 0.96 - ETA: 3s - loss: 0.1023 - accuracy: 0.96 - ETA: 3s - loss: 0.1025 - accuracy: 0.96 - ETA: 3s - loss: 0.1026 - accuracy: 0.96 - ETA: 3s - loss: 0.1026 - accuracy: 0.96 - ETA: 3s - loss: 0.1026 - accuracy: 0.96 - ETA: 3s - loss: 0.1026 - accuracy: 0.96 - ETA: 2s - loss: 0.1027 - accuracy: 0.96 - ETA: 2s - loss: 0.1028 - accuracy: 0.96 - ETA: 2s - loss: 0.1028 - accuracy: 0.96 - ETA: 2s - loss: 0.1027 - accuracy: 0.96 - ETA: 2s - loss: 0.1026 - accuracy: 0.96 - ETA: 2s - loss: 0.1026 - accuracy: 0.96 - ETA: 1s - loss: 0.1026 - accuracy: 0.96 - ETA: 1s - loss: 0.1027 - accuracy: 0.96 - ETA: 1s - loss: 0.1026 - accuracy: 0.96 - ETA: 1s - loss: 0.1025 - accuracy: 0.96 - ETA: 1s - loss: 0.1025 - accuracy: 0.96 - ETA: 1s - loss: 0.1026 - accuracy: 0.96 - ETA: 1s - loss: 0.1027 - accuracy: 0.96 - ETA: 0s - loss: 0.1028 - accuracy: 0.96 - ETA: 0s - loss: 0.1029 - accuracy: 0.96 - ETA: 0s - loss: 0.1028 - accuracy: 0.96 - ETA: 0s - loss: 0.1029 - accuracy: 0.96 - ETA: 0s - loss: 0.1027 - accuracy: 0.96 - ETA: 0s - loss: 0.1028 - accuracy: 0.9637\n",
      "Epoch 00007: val_loss did not improve from 0.12622\n",
      "276/276 [==============================] - 46s 166ms/step - loss: 0.1027 - accuracy: 0.9638 - val_loss: 0.1326 - val_accuracy: 0.9580\n",
      "Epoch 8/1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/276 [===================>..........] - ETA: 1:30 - loss: 0.1082 - accuracy: 0.95 - ETA: 1:05 - loss: 0.1078 - accuracy: 0.96 - ETA: 57s - loss: 0.0997 - accuracy: 0.9637 - ETA: 53s - loss: 0.0993 - accuracy: 0.963 - ETA: 50s - loss: 0.1057 - accuracy: 0.961 - ETA: 48s - loss: 0.1064 - accuracy: 0.961 - ETA: 47s - loss: 0.1013 - accuracy: 0.962 - ETA: 46s - loss: 0.0994 - accuracy: 0.963 - ETA: 45s - loss: 0.0999 - accuracy: 0.963 - ETA: 45s - loss: 0.0993 - accuracy: 0.963 - ETA: 44s - loss: 0.1005 - accuracy: 0.963 - ETA: 43s - loss: 0.0989 - accuracy: 0.963 - ETA: 43s - loss: 0.1032 - accuracy: 0.962 - ETA: 43s - loss: 0.1017 - accuracy: 0.962 - ETA: 42s - loss: 0.1026 - accuracy: 0.962 - ETA: 42s - loss: 0.1015 - accuracy: 0.963 - ETA: 41s - loss: 0.1002 - accuracy: 0.963 - ETA: 41s - loss: 0.1006 - accuracy: 0.963 - ETA: 41s - loss: 0.1004 - accuracy: 0.963 - ETA: 41s - loss: 0.1013 - accuracy: 0.963 - ETA: 40s - loss: 0.1007 - accuracy: 0.963 - ETA: 40s - loss: 0.1001 - accuracy: 0.963 - ETA: 40s - loss: 0.1005 - accuracy: 0.963 - ETA: 40s - loss: 0.0991 - accuracy: 0.964 - ETA: 39s - loss: 0.0987 - accuracy: 0.964 - ETA: 39s - loss: 0.0992 - accuracy: 0.964 - ETA: 39s - loss: 0.0991 - accuracy: 0.964 - ETA: 39s - loss: 0.1000 - accuracy: 0.964 - ETA: 38s - loss: 0.0989 - accuracy: 0.964 - ETA: 38s - loss: 0.0982 - accuracy: 0.964 - ETA: 38s - loss: 0.0971 - accuracy: 0.965 - ETA: 38s - loss: 0.0977 - accuracy: 0.965 - ETA: 38s - loss: 0.0971 - accuracy: 0.965 - ETA: 37s - loss: 0.0980 - accuracy: 0.964 - ETA: 37s - loss: 0.0977 - accuracy: 0.965 - ETA: 37s - loss: 0.0984 - accuracy: 0.964 - ETA: 37s - loss: 0.0983 - accuracy: 0.964 - ETA: 37s - loss: 0.0985 - accuracy: 0.964 - ETA: 36s - loss: 0.0983 - accuracy: 0.964 - ETA: 36s - loss: 0.0976 - accuracy: 0.965 - ETA: 36s - loss: 0.0972 - accuracy: 0.965 - ETA: 36s - loss: 0.0976 - accuracy: 0.965 - ETA: 36s - loss: 0.0985 - accuracy: 0.964 - ETA: 36s - loss: 0.0983 - accuracy: 0.964 - ETA: 35s - loss: 0.0991 - accuracy: 0.964 - ETA: 35s - loss: 0.0993 - accuracy: 0.964 - ETA: 35s - loss: 0.0995 - accuracy: 0.964 - ETA: 35s - loss: 0.1000 - accuracy: 0.963 - ETA: 35s - loss: 0.1000 - accuracy: 0.963 - ETA: 35s - loss: 0.0997 - accuracy: 0.964 - ETA: 34s - loss: 0.1000 - accuracy: 0.963 - ETA: 34s - loss: 0.0992 - accuracy: 0.964 - ETA: 34s - loss: 0.0986 - accuracy: 0.964 - ETA: 34s - loss: 0.0984 - accuracy: 0.964 - ETA: 34s - loss: 0.0980 - accuracy: 0.964 - ETA: 33s - loss: 0.0980 - accuracy: 0.964 - ETA: 33s - loss: 0.0980 - accuracy: 0.964 - ETA: 33s - loss: 0.0976 - accuracy: 0.965 - ETA: 33s - loss: 0.0972 - accuracy: 0.965 - ETA: 33s - loss: 0.0979 - accuracy: 0.964 - ETA: 33s - loss: 0.0981 - accuracy: 0.964 - ETA: 32s - loss: 0.0982 - accuracy: 0.964 - ETA: 32s - loss: 0.0981 - accuracy: 0.964 - ETA: 32s - loss: 0.0979 - accuracy: 0.964 - ETA: 32s - loss: 0.0982 - accuracy: 0.964 - ETA: 32s - loss: 0.0989 - accuracy: 0.964 - ETA: 32s - loss: 0.0988 - accuracy: 0.964 - ETA: 32s - loss: 0.0990 - accuracy: 0.964 - ETA: 31s - loss: 0.0994 - accuracy: 0.964 - ETA: 31s - loss: 0.0990 - accuracy: 0.964 - ETA: 31s - loss: 0.0991 - accuracy: 0.964 - ETA: 31s - loss: 0.0991 - accuracy: 0.964 - ETA: 31s - loss: 0.0991 - accuracy: 0.964 - ETA: 31s - loss: 0.0992 - accuracy: 0.964 - ETA: 30s - loss: 0.0992 - accuracy: 0.964 - ETA: 30s - loss: 0.0992 - accuracy: 0.964 - ETA: 30s - loss: 0.0991 - accuracy: 0.964 - ETA: 30s - loss: 0.0990 - accuracy: 0.964 - ETA: 30s - loss: 0.0989 - accuracy: 0.964 - ETA: 30s - loss: 0.0989 - accuracy: 0.964 - ETA: 29s - loss: 0.0986 - accuracy: 0.964 - ETA: 29s - loss: 0.0984 - accuracy: 0.964 - ETA: 29s - loss: 0.0982 - accuracy: 0.965 - ETA: 29s - loss: 0.0984 - accuracy: 0.964 - ETA: 29s - loss: 0.0982 - accuracy: 0.965 - ETA: 29s - loss: 0.0981 - accuracy: 0.965 - ETA: 28s - loss: 0.0979 - accuracy: 0.965 - ETA: 28s - loss: 0.0978 - accuracy: 0.965 - ETA: 28s - loss: 0.0980 - accuracy: 0.965 - ETA: 28s - loss: 0.0978 - accuracy: 0.965 - ETA: 28s - loss: 0.0978 - accuracy: 0.965 - ETA: 28s - loss: 0.0976 - accuracy: 0.965 - ETA: 28s - loss: 0.0977 - accuracy: 0.965 - ETA: 27s - loss: 0.0978 - accuracy: 0.965 - ETA: 27s - loss: 0.0978 - accuracy: 0.965 - ETA: 27s - loss: 0.0980 - accuracy: 0.965 - ETA: 27s - loss: 0.0982 - accuracy: 0.964 - ETA: 27s - loss: 0.0982 - accuracy: 0.964 - ETA: 27s - loss: 0.0983 - accuracy: 0.964 - ETA: 26s - loss: 0.0984 - accuracy: 0.964 - ETA: 26s - loss: 0.0980 - accuracy: 0.965 - ETA: 26s - loss: 0.0978 - accuracy: 0.965 - ETA: 26s - loss: 0.0979 - accuracy: 0.965 - ETA: 26s - loss: 0.0981 - accuracy: 0.964 - ETA: 26s - loss: 0.0980 - accuracy: 0.965 - ETA: 25s - loss: 0.0978 - accuracy: 0.965 - ETA: 25s - loss: 0.0978 - accuracy: 0.965 - ETA: 25s - loss: 0.0978 - accuracy: 0.965 - ETA: 25s - loss: 0.0979 - accuracy: 0.965 - ETA: 25s - loss: 0.0980 - accuracy: 0.965 - ETA: 25s - loss: 0.0981 - accuracy: 0.965 - ETA: 25s - loss: 0.0979 - accuracy: 0.965 - ETA: 24s - loss: 0.0980 - accuracy: 0.965 - ETA: 24s - loss: 0.0979 - accuracy: 0.965 - ETA: 24s - loss: 0.0977 - accuracy: 0.965 - ETA: 24s - loss: 0.0978 - accuracy: 0.965 - ETA: 24s - loss: 0.0976 - accuracy: 0.965 - ETA: 24s - loss: 0.0976 - accuracy: 0.965 - ETA: 23s - loss: 0.0975 - accuracy: 0.965 - ETA: 23s - loss: 0.0975 - accuracy: 0.965 - ETA: 23s - loss: 0.0979 - accuracy: 0.965 - ETA: 23s - loss: 0.0983 - accuracy: 0.964 - ETA: 23s - loss: 0.0984 - accuracy: 0.964 - ETA: 23s - loss: 0.0985 - accuracy: 0.964 - ETA: 23s - loss: 0.0985 - accuracy: 0.964 - ETA: 22s - loss: 0.0985 - accuracy: 0.964 - ETA: 22s - loss: 0.0986 - accuracy: 0.964 - ETA: 22s - loss: 0.0985 - accuracy: 0.964 - ETA: 22s - loss: 0.0984 - accuracy: 0.964 - ETA: 22s - loss: 0.0987 - accuracy: 0.964 - ETA: 22s - loss: 0.0988 - accuracy: 0.964 - ETA: 21s - loss: 0.0989 - accuracy: 0.964 - ETA: 21s - loss: 0.0987 - accuracy: 0.964 - ETA: 21s - loss: 0.0988 - accuracy: 0.964 - ETA: 21s - loss: 0.0990 - accuracy: 0.964 - ETA: 21s - loss: 0.0990 - accuracy: 0.964 - ETA: 21s - loss: 0.0990 - accuracy: 0.964 - ETA: 21s - loss: 0.0992 - accuracy: 0.964 - ETA: 20s - loss: 0.0994 - accuracy: 0.964 - ETA: 20s - loss: 0.0995 - accuracy: 0.964 - ETA: 20s - loss: 0.0995 - accuracy: 0.964 - ETA: 20s - loss: 0.0997 - accuracy: 0.964 - ETA: 20s - loss: 0.0995 - accuracy: 0.964 - ETA: 20s - loss: 0.0993 - accuracy: 0.964 - ETA: 19s - loss: 0.0993 - accuracy: 0.964 - ETA: 19s - loss: 0.0993 - accuracy: 0.964 - ETA: 19s - loss: 0.0992 - accuracy: 0.964 - ETA: 19s - loss: 0.0995 - accuracy: 0.964 - ETA: 19s - loss: 0.0996 - accuracy: 0.964 - ETA: 19s - loss: 0.0996 - accuracy: 0.964 - ETA: 19s - loss: 0.0996 - accuracy: 0.964 - ETA: 18s - loss: 0.0996 - accuracy: 0.964 - ETA: 18s - loss: 0.0996 - accuracy: 0.964 - ETA: 18s - loss: 0.0997 - accuracy: 0.964 - ETA: 18s - loss: 0.0994 - accuracy: 0.964 - ETA: 18s - loss: 0.0995 - accuracy: 0.964 - ETA: 18s - loss: 0.0992 - accuracy: 0.964 - ETA: 17s - loss: 0.0989 - accuracy: 0.964 - ETA: 17s - loss: 0.0987 - accuracy: 0.964 - ETA: 17s - loss: 0.0987 - accuracy: 0.964 - ETA: 17s - loss: 0.0987 - accuracy: 0.964 - ETA: 17s - loss: 0.0986 - accuracy: 0.964 - ETA: 17s - loss: 0.0988 - accuracy: 0.964 - ETA: 17s - loss: 0.0988 - accuracy: 0.964 - ETA: 16s - loss: 0.0986 - accuracy: 0.964 - ETA: 16s - loss: 0.0985 - accuracy: 0.964 - ETA: 16s - loss: 0.0986 - accuracy: 0.964 - ETA: 16s - loss: 0.0988 - accuracy: 0.964 - ETA: 16s - loss: 0.0988 - accuracy: 0.964 - ETA: 16s - loss: 0.0989 - accuracy: 0.964 - ETA: 16s - loss: 0.0989 - accuracy: 0.964 - ETA: 15s - loss: 0.0989 - accuracy: 0.964 - ETA: 15s - loss: 0.0989 - accuracy: 0.964 - ETA: 15s - loss: 0.0989 - accuracy: 0.964 - ETA: 15s - loss: 0.0989 - accuracy: 0.964 - ETA: 15s - loss: 0.0988 - accuracy: 0.964 - ETA: 15s - loss: 0.0987 - accuracy: 0.964 - ETA: 14s - loss: 0.0987 - accuracy: 0.964 - ETA: 14s - loss: 0.0986 - accuracy: 0.964 - ETA: 14s - loss: 0.0985 - accuracy: 0.964 - ETA: 14s - loss: 0.0985 - accuracy: 0.964 - ETA: 14s - loss: 0.0984 - accuracy: 0.964 - ETA: 14s - loss: 0.0983 - accuracy: 0.964 - ETA: 13s - loss: 0.0980 - accuracy: 0.964 - ETA: 13s - loss: 0.0980 - accuracy: 0.964 - ETA: 13s - loss: 0.0984 - accuracy: 0.9646"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/276 [============================>.] - ETA: 13s - loss: 0.0984 - accuracy: 0.964 - ETA: 13s - loss: 0.0983 - accuracy: 0.964 - ETA: 13s - loss: 0.0983 - accuracy: 0.964 - ETA: 13s - loss: 0.0982 - accuracy: 0.964 - ETA: 12s - loss: 0.0982 - accuracy: 0.964 - ETA: 12s - loss: 0.0981 - accuracy: 0.964 - ETA: 12s - loss: 0.0981 - accuracy: 0.964 - ETA: 12s - loss: 0.0981 - accuracy: 0.964 - ETA: 12s - loss: 0.0980 - accuracy: 0.964 - ETA: 12s - loss: 0.0981 - accuracy: 0.964 - ETA: 11s - loss: 0.0981 - accuracy: 0.964 - ETA: 11s - loss: 0.0981 - accuracy: 0.964 - ETA: 11s - loss: 0.0981 - accuracy: 0.964 - ETA: 11s - loss: 0.0980 - accuracy: 0.964 - ETA: 11s - loss: 0.0979 - accuracy: 0.964 - ETA: 11s - loss: 0.0981 - accuracy: 0.964 - ETA: 11s - loss: 0.0981 - accuracy: 0.964 - ETA: 10s - loss: 0.0981 - accuracy: 0.964 - ETA: 10s - loss: 0.0980 - accuracy: 0.964 - ETA: 10s - loss: 0.0980 - accuracy: 0.964 - ETA: 10s - loss: 0.0979 - accuracy: 0.964 - ETA: 10s - loss: 0.0979 - accuracy: 0.964 - ETA: 10s - loss: 0.0979 - accuracy: 0.964 - ETA: 10s - loss: 0.0979 - accuracy: 0.964 - ETA: 9s - loss: 0.0979 - accuracy: 0.964 - ETA: 9s - loss: 0.0981 - accuracy: 0.96 - ETA: 9s - loss: 0.0983 - accuracy: 0.96 - ETA: 9s - loss: 0.0984 - accuracy: 0.96 - ETA: 9s - loss: 0.0984 - accuracy: 0.96 - ETA: 9s - loss: 0.0983 - accuracy: 0.96 - ETA: 8s - loss: 0.0982 - accuracy: 0.96 - ETA: 8s - loss: 0.0982 - accuracy: 0.96 - ETA: 8s - loss: 0.0983 - accuracy: 0.96 - ETA: 8s - loss: 0.0981 - accuracy: 0.96 - ETA: 8s - loss: 0.0982 - accuracy: 0.96 - ETA: 8s - loss: 0.0981 - accuracy: 0.96 - ETA: 8s - loss: 0.0982 - accuracy: 0.96 - ETA: 7s - loss: 0.0981 - accuracy: 0.96 - ETA: 7s - loss: 0.0980 - accuracy: 0.96 - ETA: 7s - loss: 0.0980 - accuracy: 0.96 - ETA: 7s - loss: 0.0979 - accuracy: 0.96 - ETA: 7s - loss: 0.0980 - accuracy: 0.96 - ETA: 7s - loss: 0.0983 - accuracy: 0.96 - ETA: 6s - loss: 0.0984 - accuracy: 0.96 - ETA: 6s - loss: 0.0984 - accuracy: 0.96 - ETA: 6s - loss: 0.0983 - accuracy: 0.96 - ETA: 6s - loss: 0.0984 - accuracy: 0.96 - ETA: 6s - loss: 0.0984 - accuracy: 0.96 - ETA: 6s - loss: 0.0983 - accuracy: 0.96 - ETA: 6s - loss: 0.0983 - accuracy: 0.96 - ETA: 5s - loss: 0.0982 - accuracy: 0.96 - ETA: 5s - loss: 0.0982 - accuracy: 0.96 - ETA: 5s - loss: 0.0982 - accuracy: 0.96 - ETA: 5s - loss: 0.0981 - accuracy: 0.96 - ETA: 5s - loss: 0.0980 - accuracy: 0.96 - ETA: 5s - loss: 0.0981 - accuracy: 0.96 - ETA: 5s - loss: 0.0980 - accuracy: 0.96 - ETA: 4s - loss: 0.0979 - accuracy: 0.96 - ETA: 4s - loss: 0.0977 - accuracy: 0.96 - ETA: 4s - loss: 0.0978 - accuracy: 0.96 - ETA: 4s - loss: 0.0977 - accuracy: 0.96 - ETA: 4s - loss: 0.0976 - accuracy: 0.96 - ETA: 4s - loss: 0.0976 - accuracy: 0.96 - ETA: 3s - loss: 0.0977 - accuracy: 0.96 - ETA: 3s - loss: 0.0977 - accuracy: 0.96 - ETA: 3s - loss: 0.0978 - accuracy: 0.96 - ETA: 3s - loss: 0.0977 - accuracy: 0.96 - ETA: 3s - loss: 0.0978 - accuracy: 0.96 - ETA: 3s - loss: 0.0977 - accuracy: 0.96 - ETA: 3s - loss: 0.0979 - accuracy: 0.96 - ETA: 2s - loss: 0.0980 - accuracy: 0.96 - ETA: 2s - loss: 0.0980 - accuracy: 0.96 - ETA: 2s - loss: 0.0980 - accuracy: 0.96 - ETA: 2s - loss: 0.0979 - accuracy: 0.96 - ETA: 2s - loss: 0.0980 - accuracy: 0.96 - ETA: 2s - loss: 0.0980 - accuracy: 0.96 - ETA: 1s - loss: 0.0980 - accuracy: 0.96 - ETA: 1s - loss: 0.0980 - accuracy: 0.96 - ETA: 1s - loss: 0.0982 - accuracy: 0.96 - ETA: 1s - loss: 0.0982 - accuracy: 0.96 - ETA: 1s - loss: 0.0983 - accuracy: 0.96 - ETA: 1s - loss: 0.0982 - accuracy: 0.96 - ETA: 1s - loss: 0.0982 - accuracy: 0.96 - ETA: 0s - loss: 0.0982 - accuracy: 0.96 - ETA: 0s - loss: 0.0981 - accuracy: 0.96 - ETA: 0s - loss: 0.0981 - accuracy: 0.96 - ETA: 0s - loss: 0.0981 - accuracy: 0.96 - ETA: 0s - loss: 0.0980 - accuracy: 0.96 - ETA: 0s - loss: 0.0982 - accuracy: 0.9648\n",
      "Epoch 00008: val_loss did not improve from 0.12622\n",
      "276/276 [==============================] - 46s 166ms/step - loss: 0.0982 - accuracy: 0.9649 - val_loss: 0.1298 - val_accuracy: 0.9581\n",
      "Epoch 9/1024\n",
      "186/276 [===================>..........] - ETA: 1:30 - loss: 0.1024 - accuracy: 0.96 - ETA: 1:05 - loss: 0.1081 - accuracy: 0.96 - ETA: 57s - loss: 0.1044 - accuracy: 0.9626 - ETA: 53s - loss: 0.0978 - accuracy: 0.965 - ETA: 50s - loss: 0.0929 - accuracy: 0.967 - ETA: 48s - loss: 0.0927 - accuracy: 0.966 - ETA: 47s - loss: 0.0980 - accuracy: 0.963 - ETA: 46s - loss: 0.0962 - accuracy: 0.964 - ETA: 45s - loss: 0.0971 - accuracy: 0.964 - ETA: 44s - loss: 0.0975 - accuracy: 0.963 - ETA: 44s - loss: 0.0973 - accuracy: 0.964 - ETA: 43s - loss: 0.0960 - accuracy: 0.964 - ETA: 43s - loss: 0.0958 - accuracy: 0.964 - ETA: 42s - loss: 0.0956 - accuracy: 0.964 - ETA: 42s - loss: 0.0969 - accuracy: 0.964 - ETA: 42s - loss: 0.0945 - accuracy: 0.965 - ETA: 41s - loss: 0.0936 - accuracy: 0.965 - ETA: 41s - loss: 0.0926 - accuracy: 0.965 - ETA: 41s - loss: 0.0946 - accuracy: 0.965 - ETA: 40s - loss: 0.0937 - accuracy: 0.965 - ETA: 40s - loss: 0.0956 - accuracy: 0.964 - ETA: 40s - loss: 0.0964 - accuracy: 0.964 - ETA: 40s - loss: 0.0979 - accuracy: 0.963 - ETA: 39s - loss: 0.0987 - accuracy: 0.963 - ETA: 39s - loss: 0.0971 - accuracy: 0.963 - ETA: 39s - loss: 0.0988 - accuracy: 0.963 - ETA: 39s - loss: 0.0983 - accuracy: 0.963 - ETA: 39s - loss: 0.0976 - accuracy: 0.963 - ETA: 38s - loss: 0.0979 - accuracy: 0.963 - ETA: 38s - loss: 0.0975 - accuracy: 0.963 - ETA: 38s - loss: 0.0977 - accuracy: 0.963 - ETA: 38s - loss: 0.0974 - accuracy: 0.963 - ETA: 38s - loss: 0.0959 - accuracy: 0.964 - ETA: 37s - loss: 0.0961 - accuracy: 0.964 - ETA: 37s - loss: 0.0970 - accuracy: 0.964 - ETA: 37s - loss: 0.0963 - accuracy: 0.964 - ETA: 37s - loss: 0.0962 - accuracy: 0.964 - ETA: 37s - loss: 0.0952 - accuracy: 0.965 - ETA: 36s - loss: 0.0954 - accuracy: 0.965 - ETA: 36s - loss: 0.0953 - accuracy: 0.965 - ETA: 36s - loss: 0.0953 - accuracy: 0.965 - ETA: 35s - loss: 0.0947 - accuracy: 0.965 - ETA: 35s - loss: 0.0946 - accuracy: 0.965 - ETA: 35s - loss: 0.0955 - accuracy: 0.964 - ETA: 35s - loss: 0.0950 - accuracy: 0.965 - ETA: 35s - loss: 0.0952 - accuracy: 0.964 - ETA: 35s - loss: 0.0946 - accuracy: 0.965 - ETA: 34s - loss: 0.0946 - accuracy: 0.965 - ETA: 34s - loss: 0.0951 - accuracy: 0.965 - ETA: 34s - loss: 0.0948 - accuracy: 0.965 - ETA: 34s - loss: 0.0952 - accuracy: 0.965 - ETA: 34s - loss: 0.0946 - accuracy: 0.965 - ETA: 34s - loss: 0.0949 - accuracy: 0.965 - ETA: 33s - loss: 0.0960 - accuracy: 0.964 - ETA: 33s - loss: 0.0958 - accuracy: 0.964 - ETA: 33s - loss: 0.0961 - accuracy: 0.964 - ETA: 33s - loss: 0.0956 - accuracy: 0.965 - ETA: 33s - loss: 0.0958 - accuracy: 0.964 - ETA: 33s - loss: 0.0964 - accuracy: 0.964 - ETA: 33s - loss: 0.0964 - accuracy: 0.964 - ETA: 32s - loss: 0.0961 - accuracy: 0.964 - ETA: 32s - loss: 0.0962 - accuracy: 0.964 - ETA: 32s - loss: 0.0964 - accuracy: 0.964 - ETA: 32s - loss: 0.0962 - accuracy: 0.964 - ETA: 32s - loss: 0.0960 - accuracy: 0.964 - ETA: 32s - loss: 0.0961 - accuracy: 0.964 - ETA: 31s - loss: 0.0962 - accuracy: 0.964 - ETA: 31s - loss: 0.0956 - accuracy: 0.965 - ETA: 31s - loss: 0.0962 - accuracy: 0.964 - ETA: 31s - loss: 0.0961 - accuracy: 0.964 - ETA: 31s - loss: 0.0960 - accuracy: 0.964 - ETA: 31s - loss: 0.0959 - accuracy: 0.964 - ETA: 30s - loss: 0.0957 - accuracy: 0.965 - ETA: 30s - loss: 0.0958 - accuracy: 0.964 - ETA: 30s - loss: 0.0959 - accuracy: 0.964 - ETA: 30s - loss: 0.0957 - accuracy: 0.965 - ETA: 30s - loss: 0.0960 - accuracy: 0.964 - ETA: 30s - loss: 0.0965 - accuracy: 0.964 - ETA: 30s - loss: 0.0965 - accuracy: 0.964 - ETA: 29s - loss: 0.0965 - accuracy: 0.964 - ETA: 29s - loss: 0.0965 - accuracy: 0.964 - ETA: 29s - loss: 0.0966 - accuracy: 0.964 - ETA: 29s - loss: 0.0965 - accuracy: 0.964 - ETA: 29s - loss: 0.0962 - accuracy: 0.964 - ETA: 29s - loss: 0.0968 - accuracy: 0.964 - ETA: 28s - loss: 0.0967 - accuracy: 0.964 - ETA: 28s - loss: 0.0968 - accuracy: 0.964 - ETA: 28s - loss: 0.0969 - accuracy: 0.964 - ETA: 28s - loss: 0.0970 - accuracy: 0.964 - ETA: 28s - loss: 0.0967 - accuracy: 0.964 - ETA: 28s - loss: 0.0964 - accuracy: 0.964 - ETA: 28s - loss: 0.0968 - accuracy: 0.964 - ETA: 27s - loss: 0.0967 - accuracy: 0.964 - ETA: 27s - loss: 0.0968 - accuracy: 0.964 - ETA: 27s - loss: 0.0965 - accuracy: 0.964 - ETA: 27s - loss: 0.0966 - accuracy: 0.964 - ETA: 27s - loss: 0.0967 - accuracy: 0.964 - ETA: 27s - loss: 0.0964 - accuracy: 0.964 - ETA: 26s - loss: 0.0964 - accuracy: 0.964 - ETA: 26s - loss: 0.0961 - accuracy: 0.964 - ETA: 26s - loss: 0.0958 - accuracy: 0.964 - ETA: 26s - loss: 0.0958 - accuracy: 0.964 - ETA: 26s - loss: 0.0958 - accuracy: 0.965 - ETA: 26s - loss: 0.0961 - accuracy: 0.964 - ETA: 26s - loss: 0.0961 - accuracy: 0.964 - ETA: 25s - loss: 0.0960 - accuracy: 0.964 - ETA: 25s - loss: 0.0958 - accuracy: 0.965 - ETA: 25s - loss: 0.0959 - accuracy: 0.964 - ETA: 25s - loss: 0.0958 - accuracy: 0.965 - ETA: 25s - loss: 0.0958 - accuracy: 0.965 - ETA: 25s - loss: 0.0961 - accuracy: 0.964 - ETA: 24s - loss: 0.0961 - accuracy: 0.964 - ETA: 24s - loss: 0.0959 - accuracy: 0.965 - ETA: 24s - loss: 0.0960 - accuracy: 0.964 - ETA: 24s - loss: 0.0958 - accuracy: 0.965 - ETA: 24s - loss: 0.0960 - accuracy: 0.964 - ETA: 24s - loss: 0.0960 - accuracy: 0.964 - ETA: 24s - loss: 0.0959 - accuracy: 0.965 - ETA: 23s - loss: 0.0958 - accuracy: 0.965 - ETA: 23s - loss: 0.0959 - accuracy: 0.965 - ETA: 23s - loss: 0.0958 - accuracy: 0.965 - ETA: 23s - loss: 0.0958 - accuracy: 0.965 - ETA: 23s - loss: 0.0958 - accuracy: 0.965 - ETA: 23s - loss: 0.0959 - accuracy: 0.965 - ETA: 22s - loss: 0.0959 - accuracy: 0.965 - ETA: 22s - loss: 0.0960 - accuracy: 0.965 - ETA: 22s - loss: 0.0961 - accuracy: 0.965 - ETA: 22s - loss: 0.0962 - accuracy: 0.964 - ETA: 22s - loss: 0.0964 - accuracy: 0.964 - ETA: 22s - loss: 0.0964 - accuracy: 0.964 - ETA: 22s - loss: 0.0963 - accuracy: 0.964 - ETA: 21s - loss: 0.0963 - accuracy: 0.964 - ETA: 21s - loss: 0.0963 - accuracy: 0.964 - ETA: 21s - loss: 0.0963 - accuracy: 0.964 - ETA: 21s - loss: 0.0964 - accuracy: 0.964 - ETA: 21s - loss: 0.0963 - accuracy: 0.964 - ETA: 21s - loss: 0.0962 - accuracy: 0.964 - ETA: 20s - loss: 0.0962 - accuracy: 0.964 - ETA: 20s - loss: 0.0961 - accuracy: 0.964 - ETA: 20s - loss: 0.0959 - accuracy: 0.965 - ETA: 20s - loss: 0.0959 - accuracy: 0.965 - ETA: 20s - loss: 0.0958 - accuracy: 0.965 - ETA: 20s - loss: 0.0959 - accuracy: 0.965 - ETA: 20s - loss: 0.0959 - accuracy: 0.965 - ETA: 19s - loss: 0.0962 - accuracy: 0.964 - ETA: 19s - loss: 0.0963 - accuracy: 0.964 - ETA: 19s - loss: 0.0964 - accuracy: 0.964 - ETA: 19s - loss: 0.0965 - accuracy: 0.964 - ETA: 19s - loss: 0.0965 - accuracy: 0.964 - ETA: 19s - loss: 0.0965 - accuracy: 0.964 - ETA: 18s - loss: 0.0965 - accuracy: 0.964 - ETA: 18s - loss: 0.0966 - accuracy: 0.964 - ETA: 18s - loss: 0.0966 - accuracy: 0.964 - ETA: 18s - loss: 0.0968 - accuracy: 0.964 - ETA: 18s - loss: 0.0966 - accuracy: 0.964 - ETA: 18s - loss: 0.0967 - accuracy: 0.964 - ETA: 18s - loss: 0.0969 - accuracy: 0.964 - ETA: 17s - loss: 0.0967 - accuracy: 0.964 - ETA: 17s - loss: 0.0967 - accuracy: 0.964 - ETA: 17s - loss: 0.0970 - accuracy: 0.964 - ETA: 17s - loss: 0.0969 - accuracy: 0.964 - ETA: 17s - loss: 0.0967 - accuracy: 0.964 - ETA: 17s - loss: 0.0966 - accuracy: 0.964 - ETA: 17s - loss: 0.0966 - accuracy: 0.964 - ETA: 16s - loss: 0.0966 - accuracy: 0.964 - ETA: 16s - loss: 0.0964 - accuracy: 0.964 - ETA: 16s - loss: 0.0963 - accuracy: 0.964 - ETA: 16s - loss: 0.0960 - accuracy: 0.964 - ETA: 16s - loss: 0.0959 - accuracy: 0.964 - ETA: 16s - loss: 0.0958 - accuracy: 0.965 - ETA: 15s - loss: 0.0958 - accuracy: 0.965 - ETA: 15s - loss: 0.0958 - accuracy: 0.965 - ETA: 15s - loss: 0.0959 - accuracy: 0.964 - ETA: 15s - loss: 0.0960 - accuracy: 0.964 - ETA: 15s - loss: 0.0959 - accuracy: 0.964 - ETA: 15s - loss: 0.0961 - accuracy: 0.964 - ETA: 15s - loss: 0.0961 - accuracy: 0.964 - ETA: 14s - loss: 0.0962 - accuracy: 0.964 - ETA: 14s - loss: 0.0960 - accuracy: 0.964 - ETA: 14s - loss: 0.0961 - accuracy: 0.964 - ETA: 14s - loss: 0.0961 - accuracy: 0.964 - ETA: 14s - loss: 0.0961 - accuracy: 0.964 - ETA: 14s - loss: 0.0960 - accuracy: 0.964 - ETA: 13s - loss: 0.0960 - accuracy: 0.965 - ETA: 13s - loss: 0.0961 - accuracy: 0.964 - ETA: 13s - loss: 0.0961 - accuracy: 0.9649275/276 [============================>.] - ETA: 13s - loss: 0.0961 - accuracy: 0.964 - ETA: 13s - loss: 0.0961 - accuracy: 0.964 - ETA: 13s - loss: 0.0959 - accuracy: 0.965 - ETA: 13s - loss: 0.0958 - accuracy: 0.965 - ETA: 12s - loss: 0.0958 - accuracy: 0.965 - ETA: 12s - loss: 0.0958 - accuracy: 0.965 - ETA: 12s - loss: 0.0956 - accuracy: 0.965 - ETA: 12s - loss: 0.0956 - accuracy: 0.965 - ETA: 12s - loss: 0.0956 - accuracy: 0.965 - ETA: 12s - loss: 0.0955 - accuracy: 0.965 - ETA: 11s - loss: 0.0954 - accuracy: 0.965 - ETA: 11s - loss: 0.0953 - accuracy: 0.965 - ETA: 11s - loss: 0.0952 - accuracy: 0.965 - ETA: 11s - loss: 0.0951 - accuracy: 0.965 - ETA: 11s - loss: 0.0951 - accuracy: 0.965 - ETA: 11s - loss: 0.0952 - accuracy: 0.965 - ETA: 11s - loss: 0.0952 - accuracy: 0.965 - ETA: 10s - loss: 0.0951 - accuracy: 0.965 - ETA: 10s - loss: 0.0951 - accuracy: 0.965 - ETA: 10s - loss: 0.0950 - accuracy: 0.965 - ETA: 10s - loss: 0.0949 - accuracy: 0.965 - ETA: 10s - loss: 0.0947 - accuracy: 0.965 - ETA: 10s - loss: 0.0948 - accuracy: 0.965 - ETA: 10s - loss: 0.0947 - accuracy: 0.965 - ETA: 9s - loss: 0.0946 - accuracy: 0.965 - ETA: 9s - loss: 0.0946 - accuracy: 0.96 - ETA: 9s - loss: 0.0946 - accuracy: 0.96 - ETA: 9s - loss: 0.0946 - accuracy: 0.96 - ETA: 9s - loss: 0.0947 - accuracy: 0.96 - ETA: 9s - loss: 0.0948 - accuracy: 0.96 - ETA: 8s - loss: 0.0948 - accuracy: 0.96 - ETA: 8s - loss: 0.0948 - accuracy: 0.96 - ETA: 8s - loss: 0.0948 - accuracy: 0.96 - ETA: 8s - loss: 0.0949 - accuracy: 0.96 - ETA: 8s - loss: 0.0949 - accuracy: 0.96 - ETA: 8s - loss: 0.0949 - accuracy: 0.96 - ETA: 8s - loss: 0.0949 - accuracy: 0.96 - ETA: 7s - loss: 0.0949 - accuracy: 0.96 - ETA: 7s - loss: 0.0948 - accuracy: 0.96 - ETA: 7s - loss: 0.0949 - accuracy: 0.96 - ETA: 7s - loss: 0.0950 - accuracy: 0.96 - ETA: 7s - loss: 0.0949 - accuracy: 0.96 - ETA: 7s - loss: 0.0949 - accuracy: 0.96 - ETA: 6s - loss: 0.0949 - accuracy: 0.96 - ETA: 6s - loss: 0.0948 - accuracy: 0.96 - ETA: 6s - loss: 0.0948 - accuracy: 0.96 - ETA: 6s - loss: 0.0949 - accuracy: 0.96 - ETA: 6s - loss: 0.0950 - accuracy: 0.96 - ETA: 6s - loss: 0.0949 - accuracy: 0.96 - ETA: 6s - loss: 0.0948 - accuracy: 0.96 - ETA: 5s - loss: 0.0948 - accuracy: 0.96 - ETA: 5s - loss: 0.0947 - accuracy: 0.96 - ETA: 5s - loss: 0.0948 - accuracy: 0.96 - ETA: 5s - loss: 0.0950 - accuracy: 0.96 - ETA: 5s - loss: 0.0950 - accuracy: 0.96 - ETA: 5s - loss: 0.0949 - accuracy: 0.96 - ETA: 5s - loss: 0.0948 - accuracy: 0.96 - ETA: 4s - loss: 0.0949 - accuracy: 0.96 - ETA: 4s - loss: 0.0948 - accuracy: 0.96 - ETA: 4s - loss: 0.0946 - accuracy: 0.96 - ETA: 4s - loss: 0.0948 - accuracy: 0.96 - ETA: 4s - loss: 0.0948 - accuracy: 0.96 - ETA: 4s - loss: 0.0946 - accuracy: 0.96 - ETA: 3s - loss: 0.0945 - accuracy: 0.96 - ETA: 3s - loss: 0.0944 - accuracy: 0.96 - ETA: 3s - loss: 0.0943 - accuracy: 0.96 - ETA: 3s - loss: 0.0943 - accuracy: 0.96 - ETA: 3s - loss: 0.0943 - accuracy: 0.96 - ETA: 3s - loss: 0.0944 - accuracy: 0.96 - ETA: 3s - loss: 0.0943 - accuracy: 0.96 - ETA: 2s - loss: 0.0943 - accuracy: 0.96 - ETA: 2s - loss: 0.0943 - accuracy: 0.96 - ETA: 2s - loss: 0.0942 - accuracy: 0.96 - ETA: 2s - loss: 0.0942 - accuracy: 0.96 - ETA: 2s - loss: 0.0942 - accuracy: 0.96 - ETA: 2s - loss: 0.0941 - accuracy: 0.96 - ETA: 1s - loss: 0.0942 - accuracy: 0.96 - ETA: 1s - loss: 0.0943 - accuracy: 0.96 - ETA: 1s - loss: 0.0943 - accuracy: 0.96 - ETA: 1s - loss: 0.0942 - accuracy: 0.96 - ETA: 1s - loss: 0.0942 - accuracy: 0.96 - ETA: 1s - loss: 0.0942 - accuracy: 0.96 - ETA: 1s - loss: 0.0941 - accuracy: 0.96 - ETA: 0s - loss: 0.0940 - accuracy: 0.96 - ETA: 0s - loss: 0.0940 - accuracy: 0.96 - ETA: 0s - loss: 0.0939 - accuracy: 0.96 - ETA: 0s - loss: 0.0940 - accuracy: 0.96 - ETA: 0s - loss: 0.0940 - accuracy: 0.96 - ETA: 0s - loss: 0.0940 - accuracy: 0.9659\n",
      "Epoch 00009: val_loss did not improve from 0.12622\n",
      "276/276 [==============================] - 46s 166ms/step - loss: 0.0939 - accuracy: 0.9659 - val_loss: 0.1307 - val_accuracy: 0.9581\n",
      "Epoch 10/1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/276 [===================>..........] - ETA: 1:29 - loss: 0.1087 - accuracy: 0.95 - ETA: 1:05 - loss: 0.0739 - accuracy: 0.97 - ETA: 56s - loss: 0.0787 - accuracy: 0.9717 - ETA: 52s - loss: 0.0784 - accuracy: 0.971 - ETA: 50s - loss: 0.0810 - accuracy: 0.970 - ETA: 48s - loss: 0.0816 - accuracy: 0.971 - ETA: 47s - loss: 0.0843 - accuracy: 0.970 - ETA: 46s - loss: 0.0822 - accuracy: 0.971 - ETA: 45s - loss: 0.0821 - accuracy: 0.971 - ETA: 44s - loss: 0.0848 - accuracy: 0.970 - ETA: 44s - loss: 0.0891 - accuracy: 0.968 - ETA: 43s - loss: 0.0863 - accuracy: 0.969 - ETA: 43s - loss: 0.0852 - accuracy: 0.970 - ETA: 42s - loss: 0.0858 - accuracy: 0.969 - ETA: 42s - loss: 0.0854 - accuracy: 0.969 - ETA: 42s - loss: 0.0851 - accuracy: 0.969 - ETA: 41s - loss: 0.0865 - accuracy: 0.969 - ETA: 41s - loss: 0.0872 - accuracy: 0.968 - ETA: 41s - loss: 0.0856 - accuracy: 0.969 - ETA: 40s - loss: 0.0844 - accuracy: 0.969 - ETA: 40s - loss: 0.0840 - accuracy: 0.970 - ETA: 40s - loss: 0.0823 - accuracy: 0.970 - ETA: 40s - loss: 0.0824 - accuracy: 0.970 - ETA: 38s - loss: 0.0818 - accuracy: 0.970 - ETA: 38s - loss: 0.0827 - accuracy: 0.970 - ETA: 38s - loss: 0.0818 - accuracy: 0.970 - ETA: 38s - loss: 0.0821 - accuracy: 0.970 - ETA: 38s - loss: 0.0824 - accuracy: 0.970 - ETA: 38s - loss: 0.0827 - accuracy: 0.970 - ETA: 37s - loss: 0.0819 - accuracy: 0.970 - ETA: 37s - loss: 0.0822 - accuracy: 0.970 - ETA: 37s - loss: 0.0830 - accuracy: 0.970 - ETA: 37s - loss: 0.0828 - accuracy: 0.970 - ETA: 37s - loss: 0.0828 - accuracy: 0.970 - ETA: 36s - loss: 0.0828 - accuracy: 0.970 - ETA: 36s - loss: 0.0833 - accuracy: 0.970 - ETA: 36s - loss: 0.0828 - accuracy: 0.970 - ETA: 36s - loss: 0.0834 - accuracy: 0.970 - ETA: 36s - loss: 0.0833 - accuracy: 0.970 - ETA: 36s - loss: 0.0835 - accuracy: 0.969 - ETA: 35s - loss: 0.0845 - accuracy: 0.969 - ETA: 35s - loss: 0.0850 - accuracy: 0.969 - ETA: 35s - loss: 0.0849 - accuracy: 0.969 - ETA: 35s - loss: 0.0851 - accuracy: 0.969 - ETA: 35s - loss: 0.0857 - accuracy: 0.969 - ETA: 35s - loss: 0.0858 - accuracy: 0.969 - ETA: 35s - loss: 0.0854 - accuracy: 0.969 - ETA: 34s - loss: 0.0856 - accuracy: 0.969 - ETA: 34s - loss: 0.0853 - accuracy: 0.969 - ETA: 34s - loss: 0.0853 - accuracy: 0.969 - ETA: 34s - loss: 0.0851 - accuracy: 0.969 - ETA: 34s - loss: 0.0847 - accuracy: 0.969 - ETA: 34s - loss: 0.0848 - accuracy: 0.969 - ETA: 33s - loss: 0.0847 - accuracy: 0.969 - ETA: 33s - loss: 0.0851 - accuracy: 0.969 - ETA: 33s - loss: 0.0857 - accuracy: 0.969 - ETA: 33s - loss: 0.0856 - accuracy: 0.969 - ETA: 33s - loss: 0.0861 - accuracy: 0.968 - ETA: 33s - loss: 0.0862 - accuracy: 0.968 - ETA: 32s - loss: 0.0867 - accuracy: 0.968 - ETA: 32s - loss: 0.0867 - accuracy: 0.968 - ETA: 32s - loss: 0.0863 - accuracy: 0.968 - ETA: 32s - loss: 0.0861 - accuracy: 0.968 - ETA: 32s - loss: 0.0870 - accuracy: 0.968 - ETA: 32s - loss: 0.0865 - accuracy: 0.968 - ETA: 32s - loss: 0.0865 - accuracy: 0.968 - ETA: 31s - loss: 0.0865 - accuracy: 0.968 - ETA: 31s - loss: 0.0861 - accuracy: 0.968 - ETA: 31s - loss: 0.0858 - accuracy: 0.968 - ETA: 31s - loss: 0.0857 - accuracy: 0.968 - ETA: 31s - loss: 0.0858 - accuracy: 0.968 - ETA: 31s - loss: 0.0856 - accuracy: 0.969 - ETA: 30s - loss: 0.0859 - accuracy: 0.968 - ETA: 30s - loss: 0.0859 - accuracy: 0.968 - ETA: 30s - loss: 0.0860 - accuracy: 0.968 - ETA: 30s - loss: 0.0858 - accuracy: 0.968 - ETA: 30s - loss: 0.0858 - accuracy: 0.968 - ETA: 30s - loss: 0.0857 - accuracy: 0.968 - ETA: 29s - loss: 0.0858 - accuracy: 0.968 - ETA: 29s - loss: 0.0857 - accuracy: 0.968 - ETA: 29s - loss: 0.0859 - accuracy: 0.968 - ETA: 29s - loss: 0.0861 - accuracy: 0.968 - ETA: 29s - loss: 0.0862 - accuracy: 0.968 - ETA: 29s - loss: 0.0866 - accuracy: 0.968 - ETA: 29s - loss: 0.0863 - accuracy: 0.968 - ETA: 28s - loss: 0.0864 - accuracy: 0.968 - ETA: 28s - loss: 0.0864 - accuracy: 0.968 - ETA: 28s - loss: 0.0863 - accuracy: 0.968 - ETA: 28s - loss: 0.0864 - accuracy: 0.968 - ETA: 28s - loss: 0.0865 - accuracy: 0.968 - ETA: 28s - loss: 0.0863 - accuracy: 0.968 - ETA: 27s - loss: 0.0865 - accuracy: 0.968 - ETA: 27s - loss: 0.0867 - accuracy: 0.968 - ETA: 27s - loss: 0.0869 - accuracy: 0.968 - ETA: 27s - loss: 0.0870 - accuracy: 0.968 - ETA: 27s - loss: 0.0871 - accuracy: 0.968 - ETA: 27s - loss: 0.0875 - accuracy: 0.968 - ETA: 27s - loss: 0.0876 - accuracy: 0.968 - ETA: 26s - loss: 0.0877 - accuracy: 0.968 - ETA: 26s - loss: 0.0875 - accuracy: 0.968 - ETA: 26s - loss: 0.0876 - accuracy: 0.968 - ETA: 26s - loss: 0.0875 - accuracy: 0.968 - ETA: 26s - loss: 0.0875 - accuracy: 0.968 - ETA: 26s - loss: 0.0876 - accuracy: 0.968 - ETA: 25s - loss: 0.0877 - accuracy: 0.968 - ETA: 25s - loss: 0.0878 - accuracy: 0.968 - ETA: 25s - loss: 0.0878 - accuracy: 0.968 - ETA: 25s - loss: 0.0877 - accuracy: 0.968 - ETA: 25s - loss: 0.0878 - accuracy: 0.968 - ETA: 25s - loss: 0.0879 - accuracy: 0.968 - ETA: 25s - loss: 0.0876 - accuracy: 0.968 - ETA: 24s - loss: 0.0877 - accuracy: 0.968 - ETA: 24s - loss: 0.0878 - accuracy: 0.968 - ETA: 24s - loss: 0.0877 - accuracy: 0.968 - ETA: 24s - loss: 0.0876 - accuracy: 0.968 - ETA: 24s - loss: 0.0877 - accuracy: 0.968 - ETA: 24s - loss: 0.0879 - accuracy: 0.968 - ETA: 24s - loss: 0.0880 - accuracy: 0.968 - ETA: 23s - loss: 0.0878 - accuracy: 0.968 - ETA: 23s - loss: 0.0875 - accuracy: 0.968 - ETA: 23s - loss: 0.0877 - accuracy: 0.968 - ETA: 23s - loss: 0.0877 - accuracy: 0.968 - ETA: 23s - loss: 0.0877 - accuracy: 0.968 - ETA: 23s - loss: 0.0876 - accuracy: 0.968 - ETA: 22s - loss: 0.0879 - accuracy: 0.968 - ETA: 22s - loss: 0.0877 - accuracy: 0.968 - ETA: 22s - loss: 0.0878 - accuracy: 0.968 - ETA: 22s - loss: 0.0882 - accuracy: 0.968 - ETA: 22s - loss: 0.0879 - accuracy: 0.968 - ETA: 22s - loss: 0.0883 - accuracy: 0.968 - ETA: 22s - loss: 0.0884 - accuracy: 0.967 - ETA: 21s - loss: 0.0886 - accuracy: 0.967 - ETA: 21s - loss: 0.0887 - accuracy: 0.967 - ETA: 21s - loss: 0.0888 - accuracy: 0.967 - ETA: 21s - loss: 0.0885 - accuracy: 0.967 - ETA: 21s - loss: 0.0887 - accuracy: 0.967 - ETA: 21s - loss: 0.0887 - accuracy: 0.967 - ETA: 20s - loss: 0.0885 - accuracy: 0.967 - ETA: 20s - loss: 0.0886 - accuracy: 0.967 - ETA: 20s - loss: 0.0889 - accuracy: 0.967 - ETA: 20s - loss: 0.0890 - accuracy: 0.967 - ETA: 20s - loss: 0.0890 - accuracy: 0.967 - ETA: 20s - loss: 0.0891 - accuracy: 0.967 - ETA: 20s - loss: 0.0892 - accuracy: 0.967 - ETA: 19s - loss: 0.0891 - accuracy: 0.967 - ETA: 19s - loss: 0.0891 - accuracy: 0.967 - ETA: 19s - loss: 0.0892 - accuracy: 0.967 - ETA: 19s - loss: 0.0895 - accuracy: 0.967 - ETA: 19s - loss: 0.0893 - accuracy: 0.967 - ETA: 19s - loss: 0.0894 - accuracy: 0.967 - ETA: 18s - loss: 0.0895 - accuracy: 0.967 - ETA: 18s - loss: 0.0895 - accuracy: 0.967 - ETA: 18s - loss: 0.0896 - accuracy: 0.967 - ETA: 18s - loss: 0.0896 - accuracy: 0.967 - ETA: 18s - loss: 0.0895 - accuracy: 0.967 - ETA: 18s - loss: 0.0896 - accuracy: 0.967 - ETA: 18s - loss: 0.0895 - accuracy: 0.967 - ETA: 17s - loss: 0.0896 - accuracy: 0.967 - ETA: 17s - loss: 0.0897 - accuracy: 0.967 - ETA: 17s - loss: 0.0896 - accuracy: 0.967 - ETA: 17s - loss: 0.0897 - accuracy: 0.967 - ETA: 17s - loss: 0.0901 - accuracy: 0.967 - ETA: 17s - loss: 0.0899 - accuracy: 0.967 - ETA: 16s - loss: 0.0903 - accuracy: 0.967 - ETA: 16s - loss: 0.0901 - accuracy: 0.967 - ETA: 16s - loss: 0.0901 - accuracy: 0.967 - ETA: 16s - loss: 0.0901 - accuracy: 0.967 - ETA: 16s - loss: 0.0902 - accuracy: 0.967 - ETA: 16s - loss: 0.0904 - accuracy: 0.967 - ETA: 16s - loss: 0.0904 - accuracy: 0.967 - ETA: 15s - loss: 0.0904 - accuracy: 0.967 - ETA: 15s - loss: 0.0904 - accuracy: 0.967 - ETA: 15s - loss: 0.0903 - accuracy: 0.967 - ETA: 15s - loss: 0.0903 - accuracy: 0.967 - ETA: 15s - loss: 0.0903 - accuracy: 0.967 - ETA: 15s - loss: 0.0901 - accuracy: 0.967 - ETA: 15s - loss: 0.0901 - accuracy: 0.967 - ETA: 14s - loss: 0.0901 - accuracy: 0.967 - ETA: 14s - loss: 0.0901 - accuracy: 0.967 - ETA: 14s - loss: 0.0903 - accuracy: 0.967 - ETA: 14s - loss: 0.0904 - accuracy: 0.967 - ETA: 14s - loss: 0.0906 - accuracy: 0.967 - ETA: 14s - loss: 0.0907 - accuracy: 0.967 - ETA: 13s - loss: 0.0907 - accuracy: 0.967 - ETA: 13s - loss: 0.0907 - accuracy: 0.967 - ETA: 13s - loss: 0.0905 - accuracy: 0.9670"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275/276 [============================>.] - ETA: 13s - loss: 0.0905 - accuracy: 0.967 - ETA: 13s - loss: 0.0903 - accuracy: 0.967 - ETA: 13s - loss: 0.0904 - accuracy: 0.967 - ETA: 13s - loss: 0.0903 - accuracy: 0.967 - ETA: 12s - loss: 0.0902 - accuracy: 0.967 - ETA: 12s - loss: 0.0903 - accuracy: 0.967 - ETA: 12s - loss: 0.0902 - accuracy: 0.967 - ETA: 12s - loss: 0.0901 - accuracy: 0.967 - ETA: 12s - loss: 0.0900 - accuracy: 0.967 - ETA: 12s - loss: 0.0900 - accuracy: 0.967 - ETA: 11s - loss: 0.0900 - accuracy: 0.967 - ETA: 11s - loss: 0.0898 - accuracy: 0.967 - ETA: 11s - loss: 0.0899 - accuracy: 0.967 - ETA: 11s - loss: 0.0898 - accuracy: 0.967 - ETA: 11s - loss: 0.0898 - accuracy: 0.967 - ETA: 11s - loss: 0.0897 - accuracy: 0.967 - ETA: 11s - loss: 0.0898 - accuracy: 0.967 - ETA: 10s - loss: 0.0897 - accuracy: 0.967 - ETA: 10s - loss: 0.0897 - accuracy: 0.967 - ETA: 10s - loss: 0.0897 - accuracy: 0.967 - ETA: 10s - loss: 0.0897 - accuracy: 0.967 - ETA: 10s - loss: 0.0898 - accuracy: 0.967 - ETA: 10s - loss: 0.0896 - accuracy: 0.967 - ETA: 10s - loss: 0.0897 - accuracy: 0.967 - ETA: 9s - loss: 0.0897 - accuracy: 0.967 - ETA: 9s - loss: 0.0897 - accuracy: 0.96 - ETA: 9s - loss: 0.0899 - accuracy: 0.96 - ETA: 9s - loss: 0.0899 - accuracy: 0.96 - ETA: 9s - loss: 0.0899 - accuracy: 0.96 - ETA: 9s - loss: 0.0900 - accuracy: 0.96 - ETA: 8s - loss: 0.0899 - accuracy: 0.96 - ETA: 8s - loss: 0.0898 - accuracy: 0.96 - ETA: 8s - loss: 0.0898 - accuracy: 0.96 - ETA: 8s - loss: 0.0899 - accuracy: 0.96 - ETA: 8s - loss: 0.0899 - accuracy: 0.96 - ETA: 8s - loss: 0.0898 - accuracy: 0.96 - ETA: 8s - loss: 0.0899 - accuracy: 0.96 - ETA: 7s - loss: 0.0899 - accuracy: 0.96 - ETA: 7s - loss: 0.0900 - accuracy: 0.96 - ETA: 7s - loss: 0.0899 - accuracy: 0.96 - ETA: 7s - loss: 0.0900 - accuracy: 0.96 - ETA: 7s - loss: 0.0899 - accuracy: 0.96 - ETA: 7s - loss: 0.0899 - accuracy: 0.96 - ETA: 6s - loss: 0.0899 - accuracy: 0.96 - ETA: 6s - loss: 0.0899 - accuracy: 0.96 - ETA: 6s - loss: 0.0898 - accuracy: 0.96 - ETA: 6s - loss: 0.0898 - accuracy: 0.96 - ETA: 6s - loss: 0.0899 - accuracy: 0.96 - ETA: 6s - loss: 0.0899 - accuracy: 0.96 - ETA: 6s - loss: 0.0900 - accuracy: 0.96 - ETA: 5s - loss: 0.0899 - accuracy: 0.96 - ETA: 5s - loss: 0.0899 - accuracy: 0.96 - ETA: 5s - loss: 0.0899 - accuracy: 0.96 - ETA: 5s - loss: 0.0901 - accuracy: 0.96 - ETA: 5s - loss: 0.0900 - accuracy: 0.96 - ETA: 5s - loss: 0.0902 - accuracy: 0.96 - ETA: 5s - loss: 0.0902 - accuracy: 0.96 - ETA: 4s - loss: 0.0901 - accuracy: 0.96 - ETA: 4s - loss: 0.0900 - accuracy: 0.96 - ETA: 4s - loss: 0.0901 - accuracy: 0.96 - ETA: 4s - loss: 0.0902 - accuracy: 0.96 - ETA: 4s - loss: 0.0901 - accuracy: 0.96 - ETA: 4s - loss: 0.0900 - accuracy: 0.96 - ETA: 3s - loss: 0.0900 - accuracy: 0.96 - ETA: 3s - loss: 0.0899 - accuracy: 0.96 - ETA: 3s - loss: 0.0900 - accuracy: 0.96 - ETA: 3s - loss: 0.0900 - accuracy: 0.96 - ETA: 3s - loss: 0.0900 - accuracy: 0.96 - ETA: 3s - loss: 0.0902 - accuracy: 0.96 - ETA: 3s - loss: 0.0902 - accuracy: 0.96 - ETA: 2s - loss: 0.0903 - accuracy: 0.96 - ETA: 2s - loss: 0.0902 - accuracy: 0.96 - ETA: 2s - loss: 0.0901 - accuracy: 0.96 - ETA: 2s - loss: 0.0902 - accuracy: 0.96 - ETA: 2s - loss: 0.0902 - accuracy: 0.96 - ETA: 2s - loss: 0.0902 - accuracy: 0.96 - ETA: 1s - loss: 0.0901 - accuracy: 0.96 - ETA: 1s - loss: 0.0900 - accuracy: 0.96 - ETA: 1s - loss: 0.0900 - accuracy: 0.96 - ETA: 1s - loss: 0.0901 - accuracy: 0.96 - ETA: 1s - loss: 0.0900 - accuracy: 0.96 - ETA: 1s - loss: 0.0902 - accuracy: 0.96 - ETA: 1s - loss: 0.0901 - accuracy: 0.96 - ETA: 0s - loss: 0.0902 - accuracy: 0.96 - ETA: 0s - loss: 0.0903 - accuracy: 0.96 - ETA: 0s - loss: 0.0903 - accuracy: 0.96 - ETA: 0s - loss: 0.0904 - accuracy: 0.96 - ETA: 0s - loss: 0.0903 - accuracy: 0.96 - ETA: 0s - loss: 0.0903 - accuracy: 0.9671\n",
      "Epoch 00010: val_loss did not improve from 0.12622\n",
      "276/276 [==============================] - 46s 166ms/step - loss: 0.0903 - accuracy: 0.9671 - val_loss: 0.1309 - val_accuracy: 0.9571\n",
      "Epoch 00010: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "INFO:root:Evaluating model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 2s - loss: 0.158 - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.159 - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.154 - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.158 - ETA: 0s - loss: 0.155 - ETA: 0s - loss: 0.157 - 1s 18ms/step - loss: 0.1566\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 6s - loss: 0.1324 - accuracy: 0.95 - ETA: 4s - loss: 0.1314 - accuracy: 0.95 - ETA: 3s - loss: 0.1351 - accuracy: 0.95 - ETA: 3s - loss: 0.1361 - accuracy: 0.95 - ETA: 2s - loss: 0.1278 - accuracy: 0.95 - ETA: 2s - loss: 0.1272 - accuracy: 0.95 - ETA: 2s - loss: 0.1333 - accuracy: 0.95 - ETA: 2s - loss: 0.1301 - accuracy: 0.95 - ETA: 2s - loss: 0.1311 - accuracy: 0.95 - ETA: 1s - loss: 0.1331 - accuracy: 0.95 - ETA: 1s - loss: 0.1385 - accuracy: 0.95 - ETA: 1s - loss: 0.1345 - accuracy: 0.95 - ETA: 1s - loss: 0.1351 - accuracy: 0.95 - ETA: 1s - loss: 0.1339 - accuracy: 0.95 - ETA: 1s - loss: 0.1338 - accuracy: 0.95 - ETA: 1s - loss: 0.1336 - accuracy: 0.95 - ETA: 1s - loss: 0.1351 - accuracy: 0.95 - ETA: 1s - loss: 0.1344 - accuracy: 0.95 - ETA: 1s - loss: 0.1332 - accuracy: 0.95 - ETA: 1s - loss: 0.1326 - accuracy: 0.95 - ETA: 1s - loss: 0.1340 - accuracy: 0.95 - ETA: 1s - loss: 0.1345 - accuracy: 0.95 - ETA: 0s - loss: 0.1339 - accuracy: 0.95 - ETA: 0s - loss: 0.1330 - accuracy: 0.95 - ETA: 0s - loss: 0.1359 - accuracy: 0.95 - ETA: 0s - loss: 0.1349 - accuracy: 0.95 - ETA: 0s - loss: 0.1364 - accuracy: 0.95 - ETA: 0s - loss: 0.1356 - accuracy: 0.95 - ETA: 0s - loss: 0.1334 - accuracy: 0.95 - ETA: 0s - loss: 0.1345 - accuracy: 0.95 - ETA: 0s - loss: 0.1345 - accuracy: 0.95 - ETA: 0s - loss: 0.1346 - accuracy: 0.95 - ETA: 0s - loss: 0.1338 - accuracy: 0.95 - ETA: 0s - loss: 0.1337 - accuracy: 0.95 - ETA: 0s - loss: 0.1349 - accuracy: 0.95 - ETA: 0s - loss: 0.1360 - accuracy: 0.95 - ETA: 0s - loss: 0.1370 - accuracy: 0.95 - ETA: 0s - loss: 0.1363 - accuracy: 0.95 - ETA: 0s - loss: 0.1363 - accuracy: 0.95 - 2s 55ms/step - loss: 0.1360 - accuracy: 0.9547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Predicting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 2s 52ms/step\n",
      "40/40 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing performance:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.90\n",
      "FLAT ROC AUC: 0.89\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       genes                                     hp_annotations  \\\n",
      "2698  284111  {HP:0000163, HP:0004302, HP:0000929, HP:000066...   \n",
      "3532    7045  {HP:0000504, HP:0007809, HP:0007802, HP:000769...   \n",
      "58      8289  {HP:0000163, HP:0001167, HP:0009810, HP:000092...   \n",
      "3689    7355  {HP:0000163, HP:0000929, HP:0007367, HP:001097...   \n",
      "2       8195  {HP:0000163, HP:0001167, HP:0045017, HP:000139...   \n",
      "208      353  {HP:0012622, HP:0000790, HP:0010978, HP:000007...   \n",
      "3883   57167  {HP:0011452, HP:0045060, HP:0001507, HP:000058...   \n",
      "2647   30009  {HP:0002795, HP:0002086, HP:0012042, HP:000000...   \n",
      "938     1644  {HP:0001276, HP:0002493, HP:0011024, HP:000061...   \n",
      "251   254394  {HP:0000924, HP:0000858, HP:0004322, HP:000150...   \n",
      "391     8893  {HP:0000929, HP:0000256, HP:0001276, HP:000249...   \n",
      "1299   10342  {HP:0004302, HP:0007367, HP:0001276, HP:004029...   \n",
      "1372    2253  {HP:0000163, HP:0003187, HP:0000929, HP:000017...   \n",
      "1735   27445  {HP:0000163, HP:0000929, HP:0007367, HP:000127...   \n",
      "3178   22943  {HP:0000929, HP:0012366, HP:0012535, HP:000075...   \n",
      "541     1001  {HP:0000163, HP:0009810, HP:0000968, HP:001135...   \n",
      "3683    7343  {HP:0001273, HP:0011458, HP:0012759, HP:001180...   \n",
      "959     1678  {HP:0007325, HP:0007367, HP:0001276, HP:000737...   \n",
      "1222   51196  {HP:0012337, HP:0012622, HP:0000077, HP:001146...   \n",
      "2065    3785  {HP:0007367, HP:0001276, HP:0000759, HP:000249...   \n",
      "2590   29914  {HP:0011492, HP:0000005, HP:0007760, HP:000795...   \n",
      "3704    7390  {HP:0009810, HP:0011355, HP:0010978, HP:000096...   \n",
      "1851    3094  {HP:0004302, HP:0001167, HP:0009810, HP:001253...   \n",
      "3058    6390  {HP:0010784, HP:0011452, HP:0004302, HP:000160...   \n",
      "575     1071  {HP:0003077, HP:0012184, HP:0000005, HP:000000...   \n",
      "2329    4649  {HP:0000163, HP:0004302, HP:0000657, HP:000092...   \n",
      "3424    6892  {HP:0002837, HP:0000929, HP:0002788, HP:001182...   \n",
      "1280    2121  {HP:0000163, HP:0001167, HP:0009810, HP:000066...   \n",
      "1660    2707  {HP:0001167, HP:0000929, HP:0011355, HP:001097...   \n",
      "3161   22926  {HP:0000504, HP:0012547, HP:0000496, HP:000047...   \n",
      "...      ...                                                ...   \n",
      "3617    7170  {HP:0000163, HP:0004302, HP:0003687, HP:000981...   \n",
      "574    91179  {HP:0000163, HP:0001167, HP:0009810, HP:000092...   \n",
      "3194    6578  {HP:0010978, HP:0011024, HP:0000759, HP:000189...   \n",
      "3435    6904  {HP:0000163, HP:0000929, HP:0007367, HP:000127...   \n",
      "3416    6878  {HP:0000163, HP:0000929, HP:0000174, HP:000042...   \n",
      "2102    3909  {HP:0000163, HP:0001167, HP:0009810, HP:000066...   \n",
      "2443    4998  {HP:0000163, HP:0001167, HP:0003187, HP:000981...   \n",
      "239     8604  {HP:0012759, HP:0011804, HP:0031797, HP:001263...   \n",
      "356      631  {HP:0100019, HP:0007971, HP:0000005, HP:000000...   \n",
      "1552    2556  {HP:0002493, HP:0001824, HP:0002900, HP:000150...   \n",
      "2419    4914  {HP:0011772, HP:0000163, HP:0000929, HP:001135...   \n",
      "3492   23387  {HP:0000163, HP:0002694, HP:0009810, HP:000092...   \n",
      "2550  259236  {HP:0000598, HP:0000364, HP:0031704, HP:000000...   \n",
      "40     57410  {HP:0004302, HP:0001395, HP:0001276, HP:004029...   \n",
      "2538    5261  {HP:0410042, HP:0002012, HP:0012759, HP:001180...   \n",
      "2304    4598  {HP:0000163, HP:0000929, HP:0007367, HP:001135...   \n",
      "1097    1889  {HP:0011772, HP:0001167, HP:0009600, HP:000981...   \n",
      "1032   50939  {HP:0011452, HP:0011355, HP:0000050, HP:000055...   \n",
      "2042    3737  {HP:0004302, HP:0000163, HP:0000929, HP:000066...   \n",
      "1949   85358  {HP:0000163, HP:0001167, HP:0000929, HP:001278...   \n",
      "1520    2516  {HP:0010788, HP:0001167, HP:0000929, HP:000255...   \n",
      "733    25914  {HP:0001273, HP:0012759, HP:0011804, HP:000092...   \n",
      "2009    3643  {HP:0000163, HP:0000929, HP:0000274, HP:000139...   \n",
      "1180   51142  {HP:0011458, HP:0012638, HP:0000707, HP:000128...   \n",
      "3195   63924  {HP:0410042, HP:0002012, HP:0000147, HP:000195...   \n",
      "3441    6911  {HP:0000163, HP:0000929, HP:0000256, HP:000017...   \n",
      "1344   10397  {HP:0011804, HP:0031797, HP:0030236, HP:001263...   \n",
      "527      974  {HP:0000163, HP:0000929, HP:0002788, HP:000017...   \n",
      "3197   63925  {HP:0000929, HP:0007367, HP:0001276, HP:003079...   \n",
      "1289    2137  {HP:0009810, HP:0010574, HP:0000929, HP:001097...   \n",
      "\n",
      "                                         go_annotations  \\\n",
      "2698  {GO:0005623, GO:0005829, GO:0043227, GO:005117...   \n",
      "3532  {GO:0005623, GO:0043227, GO:0005518, GO:000579...   \n",
      "58    {GO:1901576, GO:0034728, GO:0061629, GO:004809...   \n",
      "3689  {GO:0072334, GO:0005623, GO:0043227, GO:001578...   \n",
      "2     {GO:1905349, GO:0061629, GO:0005856, GO:003070...   \n",
      "208   {GO:1901576, GO:0044444, GO:0051179, GO:000565...   \n",
      "3883  {GO:0005623, GO:0043227, GO:0005654, GO:000563...   \n",
      "2647  {GO:1901576, GO:0051179, GO:0050789, GO:190350...   \n",
      "938   {GO:0005623, GO:1901576, GO:0005829, GO:004243...   \n",
      "251   {GO:0032406, GO:1901576, GO:0017111, GO:005117...   \n",
      "391   {GO:1901576, GO:0044444, GO:0050789, GO:000573...   \n",
      "1299  {GO:0071840, GO:0005623, GO:0005829, GO:004322...   \n",
      "1372  {GO:0045843, GO:0051241, GO:0007517, GO:005078...   \n",
      "1735  {GO:0005623, GO:0005856, GO:0044444, GO:004322...   \n",
      "3178  {GO:0048332, GO:0051241, GO:0007517, GO:190350...   \n",
      "541   {GO:1901576, GO:0051241, GO:0051179, GO:005078...   \n",
      "3683  {GO:1901576, GO:0043933, GO:0005654, GO:000104...   \n",
      "959   {GO:0005623, GO:0031970, GO:0043227, GO:005117...   \n",
      "1222  {GO:0051128, GO:1901576, GO:0044444, GO:005078...   \n",
      "2065  {GO:0098660, GO:0005623, GO:0030001, GO:000726...   \n",
      "2590  {GO:0005789, GO:1901576, GO:0005623, GO:004322...   \n",
      "3704  {GO:0005623, GO:1901576, GO:0005829, GO:001975...   \n",
      "1851  {GO:1901576, GO:0009259, GO:0005856, GO:004444...   \n",
      "3058  {GO:0005623, GO:0043227, GO:0008152, GO:000565...   \n",
      "575   {GO:0010743, GO:0032373, GO:0043933, GO:005117...   \n",
      "2329  {GO:0071840, GO:0035556, GO:0005623, GO:000582...   \n",
      "3424  {GO:0043933, GO:0044444, GO:0050789, GO:005117...   \n",
      "1280  {GO:0005623, GO:0043227, GO:0032991, GO:000751...   \n",
      "1660  {GO:0044464, GO:0005623, GO:0043231, GO:003005...   \n",
      "3161  {GO:0030968, GO:1901576, GO:1900103, GO:004444...   \n",
      "...                                                 ...   \n",
      "3617  {GO:0005623, GO:0001725, GO:0005829, GO:000585...   \n",
      "574                                                  {}   \n",
      "3194  {GO:0044464, GO:0005623, GO:0008150, GO:000531...   \n",
      "3435  {GO:0051128, GO:0005856, GO:0043933, GO:001711...   \n",
      "3416  {GO:1901576, GO:0061629, GO:0009301, GO:004444...   \n",
      "2102  {GO:0071840, GO:0048332, GO:0005623, GO:000170...   \n",
      "2443  {GO:1901576, GO:0044444, GO:0050789, GO:000565...   \n",
      "239   {GO:1901576, GO:0044444, GO:0051179, GO:000573...   \n",
      "356   {GO:0071840, GO:0005198, GO:0006996, GO:000998...   \n",
      "1552  {GO:0044464, GO:0005623, GO:0060089, GO:004216...   \n",
      "2419  {GO:0051128, GO:1901576, GO:0060089, GO:004444...   \n",
      "3492  {GO:0035556, GO:0005623, GO:0031929, GO:002305...   \n",
      "2550                                                 {}   \n",
      "40    {GO:0005623, GO:0005829, GO:0006890, GO:004322...   \n",
      "2538  {GO:0044444, GO:0050789, GO:0050321, GO:000573...   \n",
      "2304  {GO:0005623, GO:1901576, GO:0005829, GO:004322...   \n",
      "1097  {GO:0044444, GO:0050789, GO:0033093, GO:000573...   \n",
      "1032  {GO:0043235, GO:0050953, GO:0007601, GO:000815...   \n",
      "2042  {GO:0098660, GO:0005623, GO:0030001, GO:003299...   \n",
      "1949  {GO:0071840, GO:0005623, GO:0005829, GO:012003...   \n",
      "1520  {GO:1901576, GO:0005654, GO:0050789, GO:000551...   \n",
      "733   {GO:0036064, GO:0005623, GO:0005856, GO:003299...   \n",
      "2009  {GO:0045429, GO:0004619, GO:0006082, GO:004573...   \n",
      "1180  {GO:1901576, GO:0044444, GO:0050789, GO:000573...   \n",
      "3195  {GO:0071840, GO:0120031, GO:0005623, GO:000582...   \n",
      "3441  {GO:0009653, GO:0008150, GO:0007498, GO:000367...   \n",
      "1344  {GO:0005856, GO:0044444, GO:0050789, GO:000573...   \n",
      "527   {GO:0071840, GO:0005623, GO:0023052, GO:004445...   \n",
      "3197  {GO:0005654, GO:0050789, GO:0030154, GO:000815...   \n",
      "1289  {GO:0005789, GO:1901576, GO:0005623, GO:003462...   \n",
      "\n",
      "                                        iea_annotations  \\\n",
      "2698  {GO:0015291, GO:0044444, GO:0051179, GO:000565...   \n",
      "3532  {GO:0050840, GO:0044444, GO:0050789, GO:000573...   \n",
      "58    {GO:0048332, GO:1901576, GO:0034728, GO:006162...   \n",
      "3689  {GO:0072334, GO:0005623, GO:0043227, GO:000579...   \n",
      "2     {GO:0048332, GO:1905349, GO:0030705, GO:000170...   \n",
      "208   {GO:0046129, GO:1901576, GO:0006166, GO:000925...   \n",
      "3883  {GO:1901576, GO:0005654, GO:0050789, GO:190350...   \n",
      "2647  {GO:1902106, GO:0002294, GO:0050868, GO:005124...   \n",
      "938   {GO:1901576, GO:0019752, GO:0044444, GO:005117...   \n",
      "251   {GO:0032406, GO:1901576, GO:0051179, GO:001711...   \n",
      "391   {GO:1901576, GO:0008135, GO:0044444, GO:005078...   \n",
      "1299  {GO:1901576, GO:0043933, GO:0044444, GO:005078...   \n",
      "1372  {GO:0048332, GO:0048839, GO:0051241, GO:000751...   \n",
      "1735  {GO:0005856, GO:0030073, GO:0044444, GO:005078...   \n",
      "3178  {GO:0048332, GO:0051241, GO:0007517, GO:190350...   \n",
      "541   {GO:1901576, GO:0051241, GO:0007043, GO:005117...   \n",
      "3683  {GO:1901576, GO:0043933, GO:0005654, GO:005078...   \n",
      "959   {GO:0005623, GO:0031970, GO:0043227, GO:005117...   \n",
      "1222  {GO:0051128, GO:1901576, GO:0044444, GO:000717...   \n",
      "2065  {GO:0051179, GO:0050789, GO:0044304, GO:004520...   \n",
      "2590  {GO:1901576, GO:0044444, GO:1901663, GO:000573...   \n",
      "3704  {GO:0005623, GO:1901576, GO:0010243, GO:001975...   \n",
      "1851  {GO:1901576, GO:0009259, GO:0005856, GO:004444...   \n",
      "3058  {GO:1990204, GO:0019752, GO:0044444, GO:000608...   \n",
      "575   {GO:0010743, GO:0032373, GO:0043933, GO:005117...   \n",
      "2329  {GO:0051128, GO:0005856, GO:0044444, GO:005078...   \n",
      "3424  {GO:0043933, GO:0044444, GO:0050789, GO:005117...   \n",
      "1280  {GO:0036064, GO:0005623, GO:0005856, GO:004322...   \n",
      "1660  {GO:0005623, GO:0043227, GO:0032991, GO:004445...   \n",
      "3161  {GO:0030968, GO:1901576, GO:1900103, GO:004444...   \n",
      "...                                                 ...   \n",
      "3617  {GO:0071840, GO:0005623, GO:0005829, GO:000172...   \n",
      "574   {GO:0071840, GO:0007155, GO:0051179, GO:007016...   \n",
      "3194  {GO:0015908, GO:0015718, GO:0005623, GO:000534...   \n",
      "3435  {GO:0051128, GO:0005856, GO:0007043, GO:004444...   \n",
      "3416  {GO:1901576, GO:0061629, GO:0009301, GO:004444...   \n",
      "2102  {GO:0048332, GO:0044444, GO:0050789, GO:005117...   \n",
      "2443  {GO:1901576, GO:0044444, GO:0050789, GO:000565...   \n",
      "239   {GO:0031646, GO:1901576, GO:0009259, GO:000906...   \n",
      "356   {GO:0071840, GO:0005623, GO:0005856, GO:004444...   \n",
      "1552  {GO:0005253, GO:0060089, GO:0004890, GO:005117...   \n",
      "2419  {GO:0048265, GO:0017076, GO:0044271, GO:014009...   \n",
      "3492  {GO:0038203, GO:0050789, GO:0050321, GO:000573...   \n",
      "2550  {GO:0048839, GO:0048731, GO:0016020, GO:000760...   \n",
      "40    {GO:0005856, GO:0021510, GO:0044444, GO:005117...   \n",
      "2538  {GO:1901576, GO:0044444, GO:0050789, GO:005032...   \n",
      "2304  {GO:1901576, GO:0009259, GO:0044444, GO:005078...   \n",
      "1097  {GO:0044444, GO:0050789, GO:0033093, GO:000573...   \n",
      "1032  {GO:0005623, GO:0032991, GO:0031012, GO:001602...   \n",
      "2042  {GO:0050433, GO:0099056, GO:0043933, GO:004444...   \n",
      "1949  {GO:0031646, GO:0072657, GO:0007528, GO:009710...   \n",
      "1520  {GO:1901576, GO:0060089, GO:0051241, GO:005117...   \n",
      "733   {GO:0071840, GO:0007049, GO:0005623, GO:003606...   \n",
      "2009  {GO:0045429, GO:0004619, GO:0006082, GO:004573...   \n",
      "1180  {GO:1901576, GO:0044444, GO:0050789, GO:000573...   \n",
      "3195  {GO:0071840, GO:0120031, GO:0005623, GO:000582...   \n",
      "3441  {GO:0048332, GO:0051961, GO:1901576, GO:005112...   \n",
      "1344  {GO:0005856, GO:0044444, GO:0050789, GO:000573...   \n",
      "527   {GO:0060089, GO:0043933, GO:0050789, GO:003015...   \n",
      "3197  {GO:0051128, GO:1901576, GO:0005654, GO:005078...   \n",
      "1289  {GO:0070085, GO:0051128, GO:1901576, GO:003096...   \n",
      "\n",
      "                                     deepgo_annotations  \\\n",
      "2698  [GO:0003333|0.046, GO:0003674|0.653, GO:000521...   \n",
      "3532  [GO:0001525|0.174, GO:0001568|0.174, GO:000194...   \n",
      "58    [GO:0000003|0.224, GO:0000075|0.011, GO:000007...   \n",
      "3689  [GO:0000139|0.385, GO:0003674|0.645, GO:000521...   \n",
      "2     [GO:0000003|0.595, GO:0000226|0.861, GO:000110...   \n",
      "208   [GO:0000166|0.285, GO:0000287|0.041, GO:000177...   \n",
      "3883  [GO:0000003|0.072, GO:0000118|0.046, GO:000012...   \n",
      "2647  [GO:0000003|0.073, GO:0000018|0.065, GO:000012...   \n",
      "938   [GO:0000096|0.006, GO:0000098|0.006, GO:000030...   \n",
      "251   [GO:0000003|0.151, GO:0000082|0.047, GO:000022...   \n",
      "391   [GO:0000003|0.173, GO:0001541|0.12, GO:0002181...   \n",
      "1299  [GO:0000139|0.547, GO:0003674|0.865, GO:000548...   \n",
      "1372  [GO:0000003|0.534, GO:0000165|0.565, GO:000022...   \n",
      "1735  [GO:0000003|0.026, GO:0000149|0.009, GO:000027...   \n",
      "3178  [GO:0000122|0.345, GO:0000165|0.298, GO:000090...   \n",
      "541   [GO:0000003|0.069, GO:0000165|0.039, GO:000022...   \n",
      "3683  [GO:0000003|0.043, GO:0000183|0.156, GO:000097...   \n",
      "959   [GO:0003008|0.091, GO:0003674|0.57, GO:0005488...   \n",
      "1222  [GO:0000003|0.084, GO:0000165|0.578, GO:000018...   \n",
      "2065  [GO:0000003|0.023, GO:0000302|0.004, GO:000032...   \n",
      "2590  [GO:0000139|0.427, GO:0001885|0.162, GO:000206...   \n",
      "3704  [GO:0003674|0.93, GO:0003824|0.881, GO:0004852...   \n",
      "1851  [GO:0000118|0.115, GO:0003674|0.683, GO:000382...   \n",
      "3058  [GO:0000104|0.087, GO:0003674|0.485, GO:000382...   \n",
      "575   [GO:0003674|0.936, GO:0005215|0.638, GO:000531...   \n",
      "2329  [GO:0000003|0.135, GO:0000146|0.214, GO:000016...   \n",
      "3424  [GO:0000139|0.343, GO:0002376|0.747, GO:000239...   \n",
      "1280  [GO:0001501|0.313, GO:0003416|0.192, GO:000557...   \n",
      "1660  [GO:0000003|0.233, GO:0000139|0.019, GO:000030...   \n",
      "3161  [GO:0000003|0.045, GO:0000139|0.19, GO:0000228...   \n",
      "...                                                 ...   \n",
      "3617  [GO:0000302|0.057, GO:0000902|0.057, GO:000170...   \n",
      "574   [GO:0003674|0.74, GO:0004888|0.138, GO:0005044...   \n",
      "3194  [GO:0000323|0.031, GO:0001775|0.014, GO:000188...   \n",
      "3435  [GO:0000003|0.168, GO:0000226|0.587, GO:000027...   \n",
      "3416  [GO:0000003|0.054, GO:0000123|0.238, GO:000012...   \n",
      "2102  [GO:0000003|0.069, GO:0000226|0.05, GO:0000768...   \n",
      "2443  [GO:0000003|0.048, GO:0000070|0.028, GO:000007...   \n",
      "239   [GO:0000002|0.02, GO:0000959|0.007, GO:0001558...   \n",
      "356   [GO:0001654|0.183, GO:0002064|0.183, GO:000208...   \n",
      "1552  [GO:0001505|0.028, GO:0001662|0.023, GO:000220...   \n",
      "2419  [GO:0000003|0.088, GO:0000139|0.208, GO:000016...   \n",
      "3492  [GO:0000003|0.018, GO:0000075|0.011, GO:000007...   \n",
      "2550  [GO:0003008|0.544, GO:0007275|0.725, GO:000742...   \n",
      "40    [GO:0005575|0.995, GO:0005622|0.992, GO:000562...   \n",
      "2538  [GO:0000003|0.059, GO:0000082|0.067, GO:000008...   \n",
      "2304  [GO:0000166|0.15, GO:0000287|0.127, GO:0003674...   \n",
      "1097  [GO:0000003|0.065, GO:0000139|0.09, GO:0000165...   \n",
      "1032  [GO:0003008|0.317, GO:0003674|0.777, GO:000519...   \n",
      "2042  [GO:0000003|0.035, GO:0000082|0.019, GO:000027...   \n",
      "1949  [GO:0000003|0.011, GO:0000165|0.144, GO:000016...   \n",
      "1520  [GO:0000003|0.449, GO:0000122|0.119, GO:000022...   \n",
      "733   [GO:0005575|0.757, GO:0005622|0.732, GO:000562...   \n",
      "2009  [GO:0000003|0.444, GO:0000045|0.011, GO:000013...   \n",
      "1180  [GO:0001666|0.383, GO:0003674|0.826, GO:000367...   \n",
      "3195  [GO:0000228|0.02, GO:0000737|0.109, GO:0000785...   \n",
      "3441  [GO:0000122|0.176, GO:0000228|0.211, GO:000057...   \n",
      "1344  [GO:0000165|0.116, GO:0001655|0.048, GO:000166...   \n",
      "527   [GO:0002253|0.225, GO:0002376|0.731, GO:000242...   \n",
      "3197  [GO:0000902|0.348, GO:0000976|0.265, GO:000097...   \n",
      "1289  [GO:0000003|0.044, GO:0000139|0.115, GO:000027...   \n",
      "\n",
      "                                              sequences  \\\n",
      "2698  MASALSYVSKFKSFVILFVTPLLLLPLVILMPAKFVRCAYVIILMA...   \n",
      "3532  MALFVRLLALALALALGPAATLAGPAKSPYQLVLQHSRLRGRQHGP...   \n",
      "58    MAAQVAPAAASSLGNPPPPPPSELKKAEQQQREEAGGEAAAAAAAE...   \n",
      "3689  MAAVGAGGSTAAPGPGAVSAGALEPGTASAAHRRLKYISLAVLVVQ...   \n",
      "2     MSRLEAKKPSLCKSEPLTTERVRTTLSVLKRIVTSCYGPSGRLKQL...   \n",
      "208   MADSELQLVEQRIRSFPDFPTPGVVFRDISPVLKDPASFRAAIGLL...   \n",
      "3883  MSRRKQAKPQHINSEEDQGEQQPQQQTPEFADAAPAAPAAGELGAP...   \n",
      "2647  MGIVEPGCGDMLTGTEPMPGSDEGRAPGADPQHRYFYPEPGAQDAD...   \n",
      "938   MNASEFRRRGKEMVDYMANYMEGIEGRQVYPDVEPGYLRPLIPAAA...   \n",
      "251   MNSDQVTLVGQVFESYVSEYHKNDILLILKERDEDAHYPVVVNAMT...   \n",
      "391   MAAPVVAPPGVVVSRANKRSGAGPGGSGGGGARGAEEEPPPPLQAV...   \n",
      "1299  MNGQLDLSGKLIIKAQLGEDIRRIPIHNEDITYDELVLMMQRVFRG...   \n",
      "1372  MGSPRSALSCLLLHLLVLCLQAQEGPGRGPALGRELASLFRAGREP...   \n",
      "1735  MGNEASLEGEGLPEGLAAAAAAGGGASGAGSPSHTAIPAGMEADLS...   \n",
      "3178  MMALGAAGATRVFVAMVAAALGGHPLLGVSATLNSVLNSNAIKNLP...   \n",
      "541   MGLPRGPLASLLLLQVCWLQCAASEPCRAVFREAEVTLEAGGAEQE...   \n",
      "3683  MNGEADCPTDLEMAAPKGQDRWSQEDMLTLLECMKNNLPSNDSSKF...   \n",
      "959   MDSSSSSSAAGLGAVDPQLQHFIEVETQKQRFQQLVHQMTELCWEK...   \n",
      "1222  MTSEEMTASVLIPVTQRKVVSAQSAADESSEKVSDINISKAHTVRR...   \n",
      "2065  MVQKSRNGGVYPGPSGEKKLKVGFVGLDPGAPDSTRDGALLIAGSE...   \n",
      "2590  MAASQVLGEKINILSGETVKAGDRDPLGNDCPEQDRLPQRSWRQKC...   \n",
      "3704  MKVLLLKDAKEDDCGQDPYIRELGLYGLEATLIPVLSFEFLSLPSF...   \n",
      "1851  MADEIAKAQVARPGGDTIFGKIIRKEIPAKIIFEDDRCLAFHDISP...   \n",
      "3058  MAAVVALSLRRRLPATTLGGACLQASRGAQTAAATAPRIKKFAIYR...   \n",
      "575   MLAATVLTLALLGNAHACSKGTSHEAGIVCRITKPALLVLNHETAK...   \n",
      "2329  MNINDGGRRRFEDNEHTLRIYPGAISEGTIYCPIPARKNSTAAEVI...   \n",
      "3424  MKSLSLLLAVALGLATAVSAGPAVIECWFVEDASGKGLAKRPGALL...   \n",
      "1280  MARGGAACKSDARLLLGRDALRPAPALLAPAVLLGAALGLGLGLWL...   \n",
      "1660  MDWKTLQALLSGVNKYSTAFGRIWLSVVFVFRVLVYVVAAERVWGD...   \n",
      "3161  MGEPAGVAGTMESPFSPGLFHRLDEDWDSALFAELGYFTDTDELQL...   \n",
      "...                                                 ...   \n",
      "3617  MMEAIKKKMQMLKLDKENALDRAEQAEAEQKQAEERSKQLEDELAA...   \n",
      "574   MEGAGPRGAGPARRRGAGGPPSPLLPSLLLLLLLWMLPDTVAPQEL...   \n",
      "3194  MGLLPKLGASQGSDTSTSRAGRCARSVFGNIKVFVLCQGLLQLCQL...   \n",
      "3435  MALSDEPAAGGPEEEAEDETLAFGAALEAFGESAETRALLGRLREV...   \n",
      "3416  MAEEKKLKLSNTVLPSESMKVVAESMGIAQIQEETCQLLTDEVSYR...   \n",
      "2102  MAAAARPRGRALGPVLPPTPLLLLVLRVLPACGATARDPGAAAGLS...   \n",
      "2443  MAHYPTRLKTRKTYSWVGRPLLDRKLHYQTYREMCVKTEGCSTEIH...   \n",
      "239   MAVKVQTTKRGDPHELRNIFLQYASTEVDGERYMTPEDFVQRYLGL...   \n",
      "356   MYRRSYVFQTRKEQYEHADEASRAAEPERPADEGWAGATSLAALQG...   \n",
      "1552  MIITQTSHCYMTSLGILFLINILPGTTGQGESRRQEPGDFVKQDIG...   \n",
      "2419  MLRGGRRGQLGWHSWAAGPGSLLAWLILASAGAAPCPDACCPHGSS...   \n",
      "3492  MAAAAASGAGGAAGAGTGGAGPAGRLLPPPAPGSPAAPAAVSPAAG...   \n",
      "2550  MAGWPGAGPLCVLGGAALGVCLAGVAGQLVEPSTAPPKPKPPPLTK...   \n",
      "40    MWFFARDPVRDFPFELIPEPPEGGLPGPWALHRGRKKATGSPVSIF...   \n",
      "2538  MTLDVGPEDELPDWAAAKEFYQKYDPKDVIGRGVSSVVRRCVHRAT...   \n",
      "2304  MLSEVLLVSAPGKVILHGEHAVVHGKVALAVSLNLRTFLRLQPHSN...   \n",
      "1097  MRGVWPPPVSALLSALGMSTYKRATLDEEDLVDSLSEGDAYPNGLQ...   \n",
      "1032  MIMFPLFGKISLGILIFVLIEGDFPSLTAQTYLSIEEIQEPKSAVS...   \n",
      "2042  MTVATGDPADEAAALPGHPQDTYDPEADHECCERVVINISGLRFET...   \n",
      "1949  MDGPGASAVVVRVGIPDLQQTKCLRLDPAAPVWAAKQRVLCALNHS...   \n",
      "1520  MDYSYDEDLDELCPVCGDKVSGYHYGLLTCESCKGFFKRTVQNNKH...   \n",
      "733   MVLAGLIRKLGHQLAEIRERALKSILCKIEHNLICYADLIQERQLF...   \n",
      "2009  MATGGRRGAAAAPLLVAVAALLLGAAGHLYPGEVCPGMDIRNNLTR...   \n",
      "1180  MPRGSRSRTSRMAPPASRAPQMRAAPRPAPVAQPPAAAPPSAVGSS...   \n",
      "3195  MEYAMKSLSLLYPKSLSRHVSVRTSVVTQQLLSEPSPKAPRARPCR...   \n",
      "3441  MYHPRELYPSLGAGYRLGPAQPGADSSFPPALAEGYRYPELDTPKL...   \n",
      "1344  MSREMQDVDLAEVKPLVEKGETITGLLQEFDVQEQDIETLHGSVHV...   \n",
      "527   MARLALSPVPSHWMVALLLLLSAEPVPAARSEDRYRNPKGSACSRI...   \n",
      "3197  MEENEVESSSDAAPGPGRPEEPSESGLGVGTSEAVSADSSDAAAAP...   \n",
      "1289  MTGYTMLRNGGAGNGGQTCMLRWSNRIRLTWLSFTLFVILVFFPLI...   \n",
      "\n",
      "                                            expressions  \\\n",
      "2698  [0.018867925, 0.018867925, 0.003144654, 0.0025...   \n",
      "3532  [0.0024600246, 0.0012300123, 0.007380074, 0.00...   \n",
      "58    [0.2, 0.23076923, 0.24615385, 1.0, 0.41538462,...   \n",
      "3689  [0.22580644, 0.29032257, 0.29032257, 0.6774193...   \n",
      "2     [0.5, 0.71428573, 0.39285713, 0.4642857, 0.428...   \n",
      "208   [0.11235955, 0.11797753, 0.15730338, 1.0, 0.60...   \n",
      "3883  [0.009090909, 0.009090909, 0.018181818, 0.0, 0...   \n",
      "2647  [0.0043478264, 0.0043478264, 0.02173913, 0.782...   \n",
      "938   [0.0021739132, 0.0021739132, 0.0021739132, 0.0...   \n",
      "251   [0.12857142, 0.14285715, 0.2857143, 0.5714286,...   \n",
      "391   [0.25, 0.33333334, 0.31666666, 0.75, 0.4333333...   \n",
      "1299  [0.2, 0.23809524, 0.2857143, 0.7904762, 0.4285...   \n",
      "1372  [0.3, 0.3, 0.05, 0.4, 0.0, 0.1, 0.05, 0.0, 0.0...   \n",
      "1735  [0.16666667, 0.25, 0.083333336, 0.0, 0.0277777...   \n",
      "3178  [0.0009708738, 0.0009708738, 0.00038834952, 9....   \n",
      "541   [0.020833334, 0.041666668, 0.018749999, 0.0062...   \n",
      "3683  [0.30666667, 0.37333333, 0.36, 0.81333333, 0.4...   \n",
      "959   [0.1764706, 0.23529412, 0.1764706, 1.0, 0.2352...   \n",
      "1222  [0.11111111, 0.11111111, 0.11111111, 0.0370370...   \n",
      "2065  [0.32142857, 0.37244898, 0.025510205, 0.001020...   \n",
      "2590  [0.1764706, 0.23529412, 0.5294118, 0.88235295,...   \n",
      "3704  [0.88461536, 0.9230769, 0.42307693, 0.88461536...   \n",
      "1851  [0.42068964, 0.50574714, 0.29655173, 1.0, 0.42...   \n",
      "3058  [0.34188035, 0.41880342, 0.34188035, 0.982906,...   \n",
      "575   [0.008, 0.008, 0.016, 0.2, 0.06, 0.006, 0.0139...   \n",
      "2329  [0.2, 0.25, 0.15, 0.45, 0.15, 0.15, 0.5, 0.15,...   \n",
      "3424  [0.15238096, 0.17142858, 0.10952381, 0.7571428...   \n",
      "1280  [0.02, 0.02, 0.016, 0.016, 0.12, 0.02, 0.66, 0...   \n",
      "1660  [0.002072539, 0.0025906735, 0.00051813474, 0.0...   \n",
      "3161  [0.23333333, 0.3, 0.43333334, 1.0, 0.5, 0.2, 0...   \n",
      "...                                                 ...   \n",
      "3617  [0.066755675, 0.07209613, 0.06275033, 0.287049...   \n",
      "574   [0.033333335, 0.044444446, 0.022222223, 0.0055...   \n",
      "3194  [0.0041841003, 0.008368201, 0.008368201, 0.000...   \n",
      "3435  [0.21276596, 0.27659574, 0.19148937, 0.4255319...   \n",
      "3416  [0.4090909, 0.5, 0.18181819, 0.54545456, 0.5, ...   \n",
      "2102  [0.0625, 0.0625, 0.0625, 0.009375, 0.03125, 0....   \n",
      "2443  [0.0057142857, 0.008571429, 0.008571429, 1.0, ...   \n",
      "239   [0.40384614, 0.6730769, 0.23076923, 0.5576923,...   \n",
      "356   [0.2857143, 0.42857143, 0.71428573, 0.0, 0.142...   \n",
      "1552  [0.8, 1.0, 0.05, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0,...   \n",
      "2419  [0.04, 0.04, 0.060000002, 0.02, 0.16, 0.04, 0....   \n",
      "3492  [0.3809524, 0.47619048, 0.52380955, 0.30952382...   \n",
      "2550  [0.45454547, 0.54545456, 0.45454547, 0.0, 0.09...   \n",
      "40    [0.37209302, 0.5, 0.29069766, 0.6744186, 0.779...   \n",
      "2538  [0.085365854, 0.1097561, 0.085365854, 0.158536...   \n",
      "2304  [0.25, 0.25, 0.6666667, 0.6666667, 0.45833334,...   \n",
      "1097  [0.09259259, 0.09259259, 0.08024691, 0.0679012...   \n",
      "1032  [0.1, 0.2, 0.2, 0.2, 0.2, 0.1, 0.4, 0.2, 0.1, ...   \n",
      "2042  [0.36666667, 0.6333333, 0.06666667, 0.01666666...   \n",
      "1949  [0.23381294, 0.24820144, 0.09352518, 0.0007194...   \n",
      "1520  [0.00035335688, 0.00035335688, 0.0, 0.00070671...   \n",
      "733   [0.045454547, 0.09090909, 0.22727273, 1.0, 0.0...   \n",
      "2009  [0.07070707, 0.09090909, 0.13131313, 0.2222222...   \n",
      "1180  [0.34213686, 0.3877551, 0.3037215, 1.0, 0.8583...   \n",
      "3195  [0.00023584906, 0.00023584906, 0.00023584906, ...   \n",
      "3441  [0.083333336, 0.083333336, 0.16666667, 0.25, 0...   \n",
      "1344  [0.05740741, 0.087037034, 0.43333334, 0.009259...   \n",
      "527   [0.0013953489, 0.0013953489, 0.0023255814, 0.8...   \n",
      "3197  [0.25714287, 0.2857143, 0.2, 0.6, 0.37142858, ...   \n",
      "1289  [0.5263158, 0.68421054, 0.47368422, 0.2631579,...   \n",
      "\n",
      "                                                  preds  \n",
      "2698  [0.19209206, 0.04018191, 0.14017226, 0.0485381...  \n",
      "3532  [0.20887914, 0.06126128, 0.15507013, 0.0643614...  \n",
      "58    [0.2520368, 0.16511309, 0.28524995, 0.134498, ...  \n",
      "3689  [0.19858009, 0.074258655, 0.20464727, 0.068128...  \n",
      "2     [0.42025974, 0.024316674, 0.5791876, 0.0153474...  \n",
      "208   [0.17192492, 0.04659087, 0.15839137, 0.0668447...  \n",
      "3883  [0.22663808, 0.14577536, 0.2608071, 0.1179569,...  \n",
      "2647  [0.08588546, 0.034036554, 0.071842, 0.08084798...  \n",
      "938   [0.19036384, 0.015872734, 0.06867119, 0.020440...  \n",
      "251   [0.12081747, 0.081845224, 0.16911907, 0.085053...  \n",
      "391   [0.23967081, 0.02178575, 0.08322014, 0.0249905...  \n",
      "1299  [0.16970871, 0.056840666, 0.124063306, 0.06014...  \n",
      "1372  [0.34454924, 0.24550632, 0.35891664, 0.2134659...  \n",
      "1735  [0.22994395, 0.016027177, 0.058111303, 0.02363...  \n",
      "3178  [0.2410076, 0.14036301, 0.22934102, 0.12527747...  \n",
      "541   [0.20856252, 0.09641355, 0.1765763, 0.1070396,...  \n",
      "3683  [0.24067785, 0.10562554, 0.24003866, 0.0904748...  \n",
      "959   [0.24199696, 0.017561516, 0.088415034, 0.02003...  \n",
      "1222  [0.26384318, 0.06439117, 0.15767352, 0.0771372...  \n",
      "2065  [0.30548513, 0.027444199, 0.112954535, 0.03452...  \n",
      "2590  [0.16069749, 0.020187875, 0.08617772, 0.026249...  \n",
      "3704  [0.116199896, 0.0158915, 0.059874486, 0.027611...  \n",
      "1851  [0.19218504, 0.03330219, 0.09167009, 0.0413900...  \n",
      "3058  [0.49792504, 0.0661552, 0.23306474, 0.06121333...  \n",
      "575   [0.088557586, 0.017442413, 0.04582334, 0.04106...  \n",
      "2329  [0.2739262, 0.045478996, 0.19296348, 0.0407750...  \n",
      "3424  [0.07786189, 0.027703736, 0.08129617, 0.074027...  \n",
      "1280  [0.30396888, 0.08021277, 0.35618463, 0.0575223...  \n",
      "1660  [0.1585195, 0.11735096, 0.19438416, 0.11180143...  \n",
      "3161  [0.22726965, 0.04433681, 0.11540503, 0.0441688...  \n",
      "...                                                 ...  \n",
      "3617  [0.2054079, 0.023585787, 0.0771382, 0.04470581...  \n",
      "574   [0.17311923, 0.08953311, 0.15762292, 0.1021582...  \n",
      "3194  [0.08169976, 0.037470568, 0.084632486, 0.06211...  \n",
      "3435  [0.21319047, 0.083171055, 0.18744372, 0.075889...  \n",
      "3416  [0.2541981, 0.05708581, 0.1533319, 0.052231606...  \n",
      "2102  [0.21777809, 0.07857263, 0.16250968, 0.1073320...  \n",
      "2443  [0.23149653, 0.10906119, 0.25130075, 0.0899152...  \n",
      "239   [0.22904804, 0.016721552, 0.06675987, 0.024618...  \n",
      "356   [0.105377235, 0.023165973, 0.07692785, 0.03319...  \n",
      "1552  [0.23252979, 0.022670884, 0.08932696, 0.023648...  \n",
      "2419  [0.25763527, 0.07363778, 0.14117841, 0.0772241...  \n",
      "3492  [0.17309313, 0.046819214, 0.103743345, 0.04873...  \n",
      "2550  [0.16269374, 0.015219499, 0.06964819, 0.017370...  \n",
      "40    [0.22547597, 0.051022086, 0.16927053, 0.045511...  \n",
      "2538  [0.17620984, 0.026541578, 0.07896834, 0.036244...  \n",
      "2304  [0.09557471, 0.030537065, 0.08935163, 0.055492...  \n",
      "1097  [0.122736506, 0.030108487, 0.07638657, 0.05965...  \n",
      "1032  [0.3201306, 0.013135758, 0.12380442, 0.0126329...  \n",
      "2042  [0.23479654, 0.016090775, 0.06350707, 0.024476...  \n",
      "1949  [0.24219409, 0.012646109, 0.050726887, 0.01616...  \n",
      "1520  [0.20833915, 0.073187955, 0.13461366, 0.071216...  \n",
      "733   [0.26776916, 0.033648938, 0.38883042, 0.021296...  \n",
      "2009  [0.29900694, 0.07772634, 0.17483144, 0.0856800...  \n",
      "1180  [0.2167285, 0.023793964, 0.07231131, 0.0288411...  \n",
      "3195  [0.19794375, 0.057995368, 0.15736565, 0.064595...  \n",
      "3441  [0.20671579, 0.12168327, 0.20865884, 0.0964734...  \n",
      "1344  [0.2141597, 0.039580315, 0.12442839, 0.0489801...  \n",
      "527   [0.106651746, 0.040803824, 0.11291562, 0.11485...  \n",
      "3197  [0.22892313, 0.12702572, 0.26344013, 0.1041697...  \n",
      "1289  [0.20967278, 0.07390104, 0.19491072, 0.0674332...  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[393 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saving predictions\n"
     ]
    }
   ],
   "source": [
    "# main(hp_file, data_file, terms_file, gos_file, model_file,out_file, fold, batch_size, epochs, load, logger_file, threshold,device):\n",
    "\n",
    "main('data/hp.obo',\n",
    "     \n",
    "     'data/My_Implementations/human.pkl',\n",
    "     \n",
    "     'data/all_terms.pkl',\n",
    "     \n",
    "     'data/My_Implementations/gos.pkl',\n",
    "     \n",
    "     'data/My_Implementations/Trial_6/model_mohamed.h5',\n",
    "     \n",
    "     'data/My_Implementations/Trial_6/predictions.pkl',\n",
    "     \n",
    "     1,\n",
    "#batch_size\n",
    "     10,\n",
    "#Number of epochs     \n",
    "     1024,\n",
    "     \n",
    "     False,\n",
    "                                   \n",
    "     'data/My_Implementations/Trial_6/training.csv',\n",
    "     \n",
    "     0.5,\n",
    "    \n",
    "     'GPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial_8 --> LSTM only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {'input_shape': (24274,), 'nb_layers': 1, 'loss': 'binary_crossentropy', 'rate': 0.3, 'learning_rate': 0.001, 'units': 1500, 'model_file': 'data/My_Implementations/Trial_8/model_mohamed.h5'}\n",
      "Phenotypes 2600\n",
      "\n",
      "2753 785 393\n",
      "3933 2753 785 393\n",
      "2600\n",
      "Loading pretrained model\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: data/My_Implementations/Trial_8/model_mohamed.h5\\{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-4b422a4a7df9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m      \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m      'GPU:0')\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-a52082e05075>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(hp_file, data_file, terms_file, gos_file, model_file, out_file, fold, batch_size, epochs, load, logger_file, threshold, device)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loading pretrained model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'HPOLayer'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mHPOLayer\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m             \u001b[0mflat_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_flat.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3-1\\lib\\site-packages\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m   raise IOError(\n",
      "\u001b[1;32m~\\Anaconda3-1\\lib\\site-packages\\keras\\saving\\saved_model\\load.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(path, compile, options)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;31m# Look for metadata file or parse the SavedModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m   \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaved_metadata_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSavedMetadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m   \u001b[0mmeta_graph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta_graphs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m   \u001b[0mobject_graph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobject_graph_def\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m   \u001b[0mpath_to_metadata_pb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstants\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSAVED_METADATA_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3-1\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;34m\"SavedModel file does not exist at: %s%s{%s|%s}\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         (export_dir, os.path.sep, constants.SAVED_MODEL_FILENAME_PBTXT,\n\u001b[1;32m--> 121\u001b[1;33m          constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: SavedModel file does not exist at: data/My_Implementations/Trial_8/model_mohamed.h5\\{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "# main(hp_file, data_file, terms_file, gos_file, model_file,out_file, fold, batch_size, epochs, load, logger_file, threshold,device):\n",
    "\n",
    "main('data/hp.obo',\n",
    "     \n",
    "     'data/My_Implementations/human.pkl',\n",
    "     \n",
    "     'data/all_terms.pkl',\n",
    "     \n",
    "     'data/My_Implementations/gos.pkl',\n",
    "     \n",
    "     'data/My_Implementations/Trial_8/model_mohamed.h5',\n",
    "     \n",
    "     'data/My_Implementations/Trial_8/predictions.pkl',\n",
    "     \n",
    "     1,\n",
    "#batch_size\n",
    "     5,\n",
    "#Number of epochs     \n",
    "     1024,\n",
    "     \n",
    "     'load',\n",
    "                                   \n",
    "     'data/My_Implementations/Trial_8/training.csv',\n",
    "     \n",
    "     0.5,\n",
    "    \n",
    "     'GPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {'input_shape': (24274,), 'nb_layers': 1, 'loss': 'binary_crossentropy', 'rate': 0.3, 'learning_rate': 0.001, 'units': 1500, 'model_file': 'data/My_Implementations/Trial_8/model_mohamed.h5'}\n",
      "Phenotypes 2600\n",
      "\n",
      "2753 785 393\n",
      "3933 2753 785 393\n",
      "2600\n",
      "Creating a new model\n",
      "Training data size: 2753\n",
      "Validation data size: 785\n",
      "INFO:tensorflow:Reloading Oracle from data-cafa\\pheno\\oracle.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from data-cafa\\pheno\\oracle.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 24274)]           0         \n",
      "_________________________________________________________________\n",
      "dense_0 (Dense)              (None, 250)               6068750   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_out (Dense)            (None, 2600)              652600    \n",
      "=================================================================\n",
      "Total params: 6,721,350\n",
      "Trainable params: 6,721,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Reloading Tuner from data-cafa\\pheno\\tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from data-cafa\\pheno\\tuner0.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Results summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Results in data-cafa\\pheno</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Showing 10 best trials</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Objective: Objective(name='val_loss', direction='min') Score: 0.12414573827746568</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 24274)]           0         \n",
      "_________________________________________________________________\n",
      "dense_0 (Dense)              (None, 1000)              24275000  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_out (Dense)            (None, 2600)              2602600   \n",
      "=================================================================\n",
      "Total params: 26,877,600\n",
      "Trainable params: 26,877,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 24274)]           0         \n",
      "_________________________________________________________________\n",
      "dense_0 (Dense)              (None, 1000)              24275000  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_out (Dense)            (None, 2600)              2602600   \n",
      "=================================================================\n",
      "Total params: 26,877,600\n",
      "Trainable params: 26,877,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "785/785 [==============================] - ETA: 5s - loss: 0.149 - ETA: 0s - loss: 0.124 - ETA: 0s - loss: 0.124 - 0s 460us/sample - loss: 0.1241\n",
      "Valid loss 0.124146\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 24274)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model (Model)                   (None, 2600)         26877600    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "hpo_layer (HPOLayer)            (None, 2600, 2600)   6760000     model[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 8)            83488       hpo_layer[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 16)           167488      hpo_layer[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 24)           252000      hpo_layer[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8)            0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 16)           0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 24)           0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 48)           0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_out (Dense)               (None, 2600)         127400      concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 34,267,976\n",
      "Trainable params: 27,507,976\n",
      "Non-trainable params: 6,760,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Compilation finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 24274)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model (Model)                   (None, 2600)         26877600    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "hpo_layer (HPOLayer)            (None, 2600, 2600)   6760000     model[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 8)            83488       hpo_layer[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 16)           167488      hpo_layer[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 24)           252000      hpo_layer[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8)            0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 16)           0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 24)           0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 48)           0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_out (Dense)               (None, 2600)         127400      concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 34,267,976\n",
      "Trainable params: 27,507,976\n",
      "Non-trainable params: 6,760,000\n",
      "__________________________________________________________________________________________________\n",
      "Starting training the flat model\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 551 steps, validate for 157 steps\n",
      "Epoch 1/1024\n",
      " 54/551 [=>............................] - ETA: 29:58 - loss: 0.6931 - accuracy: 0.497 - ETA: 15:58 - loss: 0.6929 - accuracy: 0.723 - ETA: 11:14 - loss: 0.6927 - accuracy: 0.797 - ETA: 8:55 - loss: 0.6925 - accuracy: 0.838 - ETA: 7:31 - loss: 0.6922 - accuracy: 0.86 - ETA: 6:36 - loss: 0.6920 - accuracy: 0.87 - ETA: 5:57 - loss: 0.6918 - accuracy: 0.88 - ETA: 5:27 - loss: 0.6915 - accuracy: 0.89 - ETA: 5:04 - loss: 0.6913 - accuracy: 0.90 - ETA: 4:45 - loss: 0.6910 - accuracy: 0.90 - ETA: 4:30 - loss: 0.6907 - accuracy: 0.91 - ETA: 4:17 - loss: 0.6904 - accuracy: 0.91 - ETA: 4:06 - loss: 0.6902 - accuracy: 0.91 - ETA: 3:58 - loss: 0.6899 - accuracy: 0.91 - ETA: 3:49 - loss: 0.6896 - accuracy: 0.92 - ETA: 3:42 - loss: 0.6892 - accuracy: 0.92 - ETA: 3:36 - loss: 0.6888 - accuracy: 0.92 - ETA: 3:30 - loss: 0.6884 - accuracy: 0.92 - ETA: 3:25 - loss: 0.6880 - accuracy: 0.93 - ETA: 3:20 - loss: 0.6875 - accuracy: 0.93 - ETA: 3:16 - loss: 0.6870 - accuracy: 0.93 - ETA: 3:13 - loss: 0.6864 - accuracy: 0.93 - ETA: 3:09 - loss: 0.6858 - accuracy: 0.93 - ETA: 3:06 - loss: 0.6851 - accuracy: 0.93 - ETA: 3:03 - loss: 0.6844 - accuracy: 0.93 - ETA: 3:00 - loss: 0.6835 - accuracy: 0.93 - ETA: 2:57 - loss: 0.6825 - accuracy: 0.93 - ETA: 2:55 - loss: 0.6813 - accuracy: 0.93 - ETA: 2:53 - loss: 0.6797 - accuracy: 0.93 - ETA: 2:50 - loss: 0.6778 - accuracy: 0.93 - ETA: 2:48 - loss: 0.6756 - accuracy: 0.94 - ETA: 2:46 - loss: 0.6730 - accuracy: 0.94 - ETA: 2:45 - loss: 0.6701 - accuracy: 0.94 - ETA: 2:43 - loss: 0.6670 - accuracy: 0.94 - ETA: 2:41 - loss: 0.6632 - accuracy: 0.94 - ETA: 2:40 - loss: 0.6593 - accuracy: 0.94 - ETA: 2:38 - loss: 0.6548 - accuracy: 0.94 - ETA: 2:37 - loss: 0.6500 - accuracy: 0.94 - ETA: 2:35 - loss: 0.6452 - accuracy: 0.94 - ETA: 2:34 - loss: 0.6402 - accuracy: 0.94 - ETA: 2:33 - loss: 0.6349 - accuracy: 0.94 - ETA: 2:32 - loss: 0.6293 - accuracy: 0.94 - ETA: 2:31 - loss: 0.6243 - accuracy: 0.94 - ETA: 2:29 - loss: 0.6186 - accuracy: 0.94 - ETA: 2:28 - loss: 0.6130 - accuracy: 0.94 - ETA: 2:27 - loss: 0.6071 - accuracy: 0.94 - ETA: 2:26 - loss: 0.6012 - accuracy: 0.94 - ETA: 2:25 - loss: 0.5951 - accuracy: 0.94 - ETA: 2:24 - loss: 0.5893 - accuracy: 0.94 - ETA: 2:24 - loss: 0.5834 - accuracy: 0.94 - ETA: 2:23 - loss: 0.5780 - accuracy: 0.94 - ETA: 2:22 - loss: 0.5720 - accuracy: 0.94 - ETA: 2:21 - loss: 0.5664 - accuracy: 0.94 - ETA: 2:20 - loss: 0.5664 - accuracy: 0.9498WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": " [_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 2600, 8, 1, 2600, 5, 8] \n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\n\t [[StatefulPartitionedCall]] [Op:__inference_distributed_function_9236]\n\nFunction call stack:\ndistributed_function -> distributed_function -> distributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-81faaa23a953>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m      \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m      'GPU:0')\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-117eab2bd0ae>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(hp_file, data_file, terms_file, gos_file, model_file, out_file, fold, batch_size, epochs, load, logger_file, threshold, device)\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m                 \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m                 callbacks=[logger, checkpointer, earlystopper])\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loading best model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\Anaconda_3_v_1\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m:  [_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 2600, 8, 1, 2600, 5, 8] \n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\n\t [[StatefulPartitionedCall]] [Op:__inference_distributed_function_9236]\n\nFunction call stack:\ndistributed_function -> distributed_function -> distributed_function\n"
     ]
    }
   ],
   "source": [
    "# main(hp_file, data_file, terms_file, gos_file, model_file,out_file, fold, batch_size, epochs, load, logger_file, threshold,device):\n",
    "\n",
    "main('data/hp.obo',\n",
    "     \n",
    "     'data/My_Implementations/human.pkl',\n",
    "     \n",
    "     'data/all_terms.pkl',\n",
    "     \n",
    "     'data/My_Implementations/gos.pkl',\n",
    "     \n",
    "     'data/My_Implementations/Trial_8/model_mohamed.h5',\n",
    "     \n",
    "     'data/My_Implementations/Trial_8/predictions.pkl',\n",
    "     \n",
    "     1,\n",
    "#batch_size\n",
    "     5,\n",
    "#Number of epochs     \n",
    "     1024,\n",
    "     \n",
    "     False,\n",
    "                                   \n",
    "     'data/My_Implementations/Trial_8/training.csv',\n",
    "     \n",
    "     0.5,\n",
    "    \n",
    "     'GPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
