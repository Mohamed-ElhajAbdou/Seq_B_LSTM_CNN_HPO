{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohamed\\Anaconda_3_v_1\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Mohamed\\Anaconda_3_v_1\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\Users\\Mohamed\\Anaconda_3_v_1\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-760be180699f>:38: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-760be180699f>:38: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available:  True\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "#This script is used to train the model\n",
    "import click as ck\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import math\n",
    "from collections import deque\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Embedding, Conv1D, Flatten, Concatenate,\n",
    "    MaxPooling1D, Dropout, Maximum, Layer,LSTM, Dense,TimeDistributed,experimental\n",
    ")\n",
    "\n",
    "# from tensorflow.keras.layers.experimental import (RandomFourierFeatures)\n",
    "# from tensorflow.keras.layers import (\n",
    "#     Input, Dense, Embedding, Conv1D, Flatten, Concatenate,\n",
    "#     MaxPooling1D, Dropout,LSTM, Dense,TimeDistributed,\n",
    "# )\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, matthews_corrcoef\n",
    "from aminoacids import MAXLEN, to_onehot\n",
    "from utils import Ontology, FUNC_DICT, is_exp_code\n",
    "\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner import HyperModel\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "print(\"GPU Available: \", tf.test.is_gpu_available())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HPOLayer(Layer):\n",
    "\n",
    "    def __init__(self, nb_classes, **kwargs):\n",
    "        self.nb_classes = nb_classes\n",
    "        self.hpo_matrix = np.zeros((nb_classes, nb_classes), dtype=np.float32)\n",
    "        super(HPOLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def set_hpo_matrix(self, hpo_matrix):\n",
    "        self.hpo_matrix = hpo_matrix\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(HPOLayer, self).get_config()\n",
    "        config['nb_classes'] = self.nb_classes\n",
    "        return config\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        assert input_shape[1] == self.nb_classes\n",
    "        self.kernel = K.variable(\n",
    "            self.hpo_matrix, name='{}_kernel'.format(self.name))\n",
    "        self.non_trainable_weights.append(self.kernel)\n",
    "        super(HPOLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.keras.backend.repeat(x, self.nb_classes)\n",
    "        return tf.math.multiply(x, self.kernel)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [input_shape[0], self.nb_classes, self.nb_classes] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0-rc2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf; print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras; print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model(params, hpo_matrix):\n",
    "    inp = Input(shape=params['input_shape'], dtype=np.float32)\n",
    "    # Load flat model\n",
    "    flat_model = load_model(params['model_file'] + '_flat.h5')\n",
    "    \n",
    "    net = flat_model(inp)\n",
    "    \n",
    "    hpo_layer = HPOLayer(params['nb_classes'])\n",
    "\n",
    "\n",
    "    hpo_layer.trainable = False\n",
    "    \n",
    "    hpo_layer.set_hpo_matrix(hpo_matrix)\n",
    "    \n",
    "    net = hpo_layer(net)\n",
    "    \n",
    "    net = (Conv1D(filters=1,kernel_size=1,padding='valid',kernel_initializer= 'glorot_normal'))(net)\n",
    "    \n",
    "    net=MaxPooling1D(pool_size=1)(net)\n",
    "    \n",
    "    net=LSTM(1, activation=\"tanh\")(net)\n",
    "    \n",
    "#     net = MaxPooling1D(pool_size=params['nb_classes'])(net) \n",
    "    net= Flatten()(net)\n",
    "    \n",
    "#     output = Flatten()(net)\n",
    "#     output=RandomFourierFeatures( params['nb_classes'],\n",
    "                                              \n",
    "#                                               kernel_initializer='laplacian', \n",
    "                                              \n",
    "#                                               scale=None,\n",
    "                                              \n",
    "#                                               trainable=False, name='Output_SVM')(net)\n",
    "\n",
    "    output=Dense(params['nb_classes'], activation='sigmoid', name='dense_out',kernel_regularizer=regularizers.l2(0.0001))(net)\n",
    "    \n",
    "#     output=Dense(20, activation='sigmoid', name='dense_out')(net)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "#     model.compile( optimizer=Adam(lr=params['learning_rate']),   \n",
    "    model.compile(\n",
    "        optimizer=Adam(lr=params['learning_rate']),\n",
    "        metrics=['accuracy'],\n",
    "        loss=params['loss'])    \n",
    "    logging.info('Compilation finished')\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc(labels, preds):\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr, tpr, _ = roc_curve(labels.flatten(), preds.flatten())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    return roc_auc\n",
    "\n",
    "def load_data(data_file, terms, fold=1):\n",
    "    df = pd.read_pickle(data_file)\n",
    "    # Split train/valid\n",
    "    n = len(df)\n",
    "    index = np.arange(n)\n",
    "    np.random.seed(seed=10)\n",
    "    np.random.shuffle(index)\n",
    "    index = list(index)\n",
    "    train_index = []\n",
    "    test_index = []\n",
    "\n",
    "    # All Swissprot proteins\n",
    "    train_n = int(n * 0.9)\n",
    "    train_df = df.iloc[index[:train_n]]\n",
    "    valid_df = df.iloc[index[train_n:]]\n",
    "    test_df=df.iloc[index[train_n:]]\n",
    "\n",
    "    print(len(df), len(train_df), len(valid_df), len(test_df))\n",
    "    return train_df, valid_df, test_df\n",
    "    \n",
    "\n",
    "class MyHyperModel(HyperModel):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def build(self, hp):\n",
    "        inp = Input(shape=self.params['input_shape'], dtype=np.float32)\n",
    "        net = inp\n",
    "        for i in range(self.params['nb_layers']):\n",
    "            net = Dense(\n",
    "                units=hp.Int(\n",
    "                    'units', min_value=250, max_value=2000, step=250),\n",
    "                name=f'dense_{i}', activation='relu')(net)\n",
    "            net = Dropout(hp.Choice('rate', values=[0.3, 0.5]))(net)\n",
    "        output = Dense(\n",
    "            self.params['nb_classes'], activation='sigmoid',\n",
    "            name='dense_out')(net)\n",
    "\n",
    "        model = Model(inputs=inp, outputs=output)\n",
    "        model.summary()\n",
    "        model.compile(\n",
    "            optimizer=Adam(\n",
    "                hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "            loss=self.params['loss'])\n",
    "        return model\n",
    "\n",
    "\n",
    "def get_hpo_matrix(hpo, terms_dict):\n",
    "    nb_classes = len(terms_dict)\n",
    "    res = np.zeros((nb_classes, nb_classes), dtype=np.float32)\n",
    "    for hp_id, i in terms_dict.items():\n",
    "        subs = hpo.get_term_set(hp_id)\n",
    "        res[i, i] = 1\n",
    "        for h_id in subs:\n",
    "            if h_id in terms_dict:\n",
    "                res[i, terms_dict[h_id]] = 1\n",
    "    return res\n",
    "\n",
    "\n",
    "def create_flat_model(params):\n",
    "    inp = Input(shape=params['input_shape'], dtype=np.float32)\n",
    "\n",
    "    net = inp\n",
    "    for i in range(params['nb_layers']):\n",
    "        net = Dense(\n",
    "            units=params['units'], name=f'dense_{i}', activation='relu')(net)\n",
    "        net = Dropout(rate=params['rate'])(net)\n",
    "    net = Dense(\n",
    "        params['nb_classes'], activation='sigmoid',\n",
    "        name='dense_out')(net)\n",
    "    output = Flatten()(net)\n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    model.summary()\n",
    "    model.compile(\n",
    "        optimizer=Adam(lr=params['learning_rate']),\n",
    "        loss=params['loss'])\n",
    "    logging.info('Compilation finished')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DFGenerator(Sequence):                                                                                                               \n",
    "                                                                                                                                         \n",
    "    def __init__(self, df, gos_dict, terms_dict, batch_size):\n",
    "        self.start = 0\n",
    "        self.size = len(df)\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.terms_dict = terms_dict\n",
    "        self.gos_dict = gos_dict\n",
    "    def reset(self):\n",
    "        self.start = 0                                                                                                                                           \n",
    "    def __len__(self):                                                                                                                   \n",
    "        return np.ceil(len(self.df) / float(self.batch_size)).astype(np.int32)                                                           \n",
    "                                                                                                                                      \n",
    "    def __getitem__(self, idx):                                                                                                          \n",
    "        batch_index = np.arange(                                                                                                         \n",
    "            idx * self.batch_size, min(self.size, (idx + 1) * self.batch_size))                                                          \n",
    "        df = self.df.iloc[batch_index]                                                                                                   \n",
    "        data_seq = np.zeros((len(df), MAXLEN, 21), dtype=np.float32)\n",
    "        data_gos = np.zeros((len(df), len(self.gos_dict)), dtype=np.float32)\n",
    "        labels = np.zeros((len(df), len(self.terms_dict)), dtype=np.int32)\n",
    "        for i, row in enumerate(df.itertuples()):\n",
    "            seq = row.sequences\n",
    "            data_seq[i, :] = to_onehot(seq)\n",
    "            \n",
    "            for item in row.deepgo_annotations:\n",
    "                t_id, score = item.split('|')\n",
    "                if t_id in self.gos_dict:\n",
    "                    data_gos[i, self.gos_dict[t_id]] = float(score)\n",
    "\n",
    "            for t_id in row.iea_annotations:\n",
    "                if t_id in self.gos_dict:\n",
    "                    data_gos[i, self.gos_dict[t_id]] = 1\n",
    "\n",
    "            for t_id in row.go_annotations:\n",
    "                if t_id in self.gos_dict:\n",
    "                    data_gos[i, self.gos_dict[t_id]] = 1\n",
    "                \n",
    "            for t_id in row.hp_annotations:\n",
    "                if t_id in self.terms_dict:\n",
    "                    labels[i, self.terms_dict[t_id]] = 1\n",
    "        return (data_gos, labels)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(hp_file, data_file, terms_file, gos_file, model_file,out_file, fold, batch_size, epochs, load, \n",
    "         logger_file, threshold,device):\n",
    "    gos_df = pd.read_pickle(gos_file)\n",
    "    gos = gos_df['gos'].values.flatten()\n",
    "    gos_dict = {v: i for i, v in enumerate(gos)}\n",
    "\n",
    "    # cross validation settings\n",
    "    # model_file = f'fold{fold}_' + model_file\n",
    "    # out_file = f'fold{fold}_' + out_file\n",
    "    params = {\n",
    "        'nb_classes': 3783,\n",
    "        'input_shape': (len(gos),),\n",
    "#         'input_shape': (20,),\n",
    "\n",
    "        'nb_layers': 1,\n",
    "        'loss': 'binary_crossentropy',\n",
    "#         'loss': 'Hinge',        \n",
    "\n",
    "        'rate': 0.3,\n",
    "        'learning_rate': 0.001,\n",
    "        'units': 1500, # 750\n",
    "        'model_file': model_file\n",
    "    }\n",
    "    \n",
    "    print('Params:', params)\n",
    "    global hpo\n",
    "    hpo = Ontology(hp_file, with_rels=True)\n",
    "    terms_df = pd.read_pickle(terms_file)\n",
    "    global terms\n",
    "    terms = terms_df['terms'].values.flatten()\n",
    "    print('Phenotypes', len(terms))\n",
    "    global term_set\n",
    "    term_set = set(terms)\n",
    "    train_df, valid_df, test_df = load_data(data_file, terms, fold)\n",
    "    terms_dict = {v: i for i, v in enumerate(terms)}\n",
    "    hpo_matrix = get_hpo_matrix(hpo, terms_dict)\n",
    "    nb_classes = len(terms)\n",
    "    print ('nb_classes=len(terms)_______________________ ',len(terms))\n",
    "    params['nb_classes'] = nb_classes\n",
    "    print('len(terms_dict)_______________________ ',len(terms_dict))\n",
    "    test_steps = int(math.ceil(len(test_df) / batch_size))\n",
    "    test_generator = DFGenerator(test_df, gos_dict, terms_dict,batch_size)\n",
    "    valid_steps = int(math.ceil(len(valid_df) / batch_size))\n",
    "    train_steps = int(math.ceil(len(train_df) / batch_size))\n",
    "    print ( 'test_steps',test_steps,'valid_steps',valid_steps,'train_steps',train_steps)\n",
    "    test_steps=5\n",
    "    valid_steps=5\n",
    "    train_steps=5\n",
    "    xy_generator = DFGenerator(train_df, gos_dict, terms_dict,len(train_df))\n",
    "    x, y = xy_generator[0]\n",
    "    val_generator = DFGenerator(valid_df, gos_dict, terms_dict,len(valid_df))\n",
    "    val_x, val_y = val_generator[0]\n",
    "\n",
    "    train_generator = DFGenerator(train_df, gos_dict, terms_dict,batch_size)\n",
    "    valid_generator = DFGenerator(valid_df, gos_dict, terms_dict,batch_size)\n",
    "    \n",
    "    with tf.device(device):\n",
    "        if load:\n",
    "            print('Loading pretrained model')\n",
    "            model = load_model(model_file, custom_objects={'HPOLayer': HPOLayer})\n",
    "            flat_model = load_model(model_file + '_flat.h5')\n",
    "        else:\n",
    "            \n",
    "            print('Creating a new model')\n",
    "            flat_model = MyHyperModel(params)\n",
    "            # flat_model = create_flat_model(params)\n",
    "\n",
    "            print(\"Training data size: %d\" % len(train_df))\n",
    "            print(\"Validation data size: %d\" % len(valid_df))\n",
    "            checkpointer = ModelCheckpoint( filepath=model_file + '_flat.h5', verbose=1, save_best_only=True)\n",
    "            \n",
    "            earlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "            logger = CSVLogger(logger_file)\n",
    "\n",
    "\n",
    "            tuner = RandomSearch( flat_model, objective='val_loss',max_trials=1,directory='data_new',project_name='dl_ml')\n",
    "            \n",
    "            tuner.search( x, y, epochs=2, validation_data=(val_x, val_y), )\n",
    "            \n",
    "            tuner.results_summary()\n",
    "            \n",
    "            logging.info('Loading best model')\n",
    "            \n",
    "            flat_model = tuner.get_best_models(num_models=1)[0]\n",
    "            flat_model.summary()\n",
    "            \n",
    "            loss = flat_model.evaluate(val_x, val_y)\n",
    "            print('Valid loss %f' % loss)\n",
    "            flat_model.save(model_file + '_flat.h5')\n",
    "\n",
    "            model = create_model(params, hpo_matrix)\n",
    "\n",
    "\n",
    "            checkpointer = ModelCheckpoint( filepath=model_file, verbose=1, save_best_only=True)\n",
    "            model.summary()\n",
    "            print('Starting training the flat model')\n",
    "            model.fit(\n",
    "                train_generator,\n",
    "                steps_per_epoch=train_steps,\n",
    "                epochs=epochs,\n",
    "                validation_data=valid_generator,\n",
    "                validation_steps=valid_steps,\n",
    "                max_queue_size=batch_size,\n",
    "                workers=1,\n",
    "                callbacks=[logger, checkpointer, earlystopper])\n",
    "\n",
    "            logging.info('Loading best model')\n",
    "            model = load_model(model_file, custom_objects={'HPOLayer': HPOLayer})\n",
    "            flat_model = load_model(model_file + '_flat.h5')\n",
    "            \n",
    "        logging.info('Evaluating model')\n",
    "        loss = flat_model.evaluate(test_generator, steps=test_steps)\n",
    "#         print('Flat Test loss %f' % loss)\n",
    "        loss = model.evaluate(test_generator, steps=test_steps)\n",
    "#         print('Test loss %f' % loss)\n",
    "\n",
    "        logging.info('Predicting')\n",
    "        #-----------------------------------------------------\n",
    "        test_generator.reset()\n",
    "        #3944\n",
    "        test_steps = int(math.ceil(len(test_df) / batch_size))\n",
    "        preds = model.predict(test_generator, steps=test_steps)\n",
    "        flat_preds = flat_model.predict(test_generator, steps=test_steps)\n",
    "\n",
    "        all_terms_df = pd.read_pickle(terms_file)\n",
    "        all_terms = all_terms_df['terms'].values\n",
    "        all_terms_dict = {v:k for k,v in enumerate(all_terms)}\n",
    "        all_labels = np.zeros((len(test_df), len(all_terms)), dtype=np.int32)\n",
    "        for i, row in enumerate(test_df.itertuples()):\n",
    "            for hp_id in row.hp_annotations:\n",
    "                if hp_id in all_terms_dict:\n",
    "                    all_labels[i, all_terms_dict[hp_id]] = 1\n",
    "        \n",
    "        all_preds = np.zeros((len(test_df), len(all_terms)), dtype=np.float32)\n",
    "        all_flat_preds = np.zeros((len(test_df), len(all_terms)), dtype=np.float32)\n",
    "\n",
    "        test_df['preds'] = list(preds)\n",
    "#         print(test_df)\n",
    "        logging.info('Saving predictions')\n",
    "        test_df.to_pickle(out_file)\n",
    "\n",
    "        test_df['preds'] = list(flat_preds)\n",
    "        test_df.to_pickle(out_file + '_flat.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {'nb_classes': 3783, 'input_shape': (24274,), 'nb_layers': 1, 'loss': 'binary_crossentropy', 'rate': 0.3, 'learning_rate': 0.001, 'units': 1500, 'model_file': 'data/My_Implementations/Trial_2/model_mohamed.h5'}\n",
      "Phenotypes 8693\n",
      "3933 3539 394 394\n",
      "nb_classes=len(terms)_______________________  8693\n",
      "len(terms_dict)_______________________  8693\n",
      "test_steps 394 valid_steps 394 train_steps 3539\n",
      "Creating a new model\n",
      "Training data size: 3539\n",
      "Validation data size: 394\n",
      "INFO:tensorflow:Reloading Oracle from data_new\\dl_ml\\oracle.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from data_new\\dl_ml\\oracle.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 24274)]           0         \n",
      "_________________________________________________________________\n",
      "dense_0 (Dense)              (None, 250)               6068750   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_out (Dense)            (None, 8693)              2181943   \n",
      "=================================================================\n",
      "Total params: 8,250,693\n",
      "Trainable params: 8,250,693\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Reloading Tuner from data_new\\dl_ml\\tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from data_new\\dl_ml\\tuner0.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Results summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Results in data_new\\dl_ml</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Showing 10 best trials</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Objective: Objective(name='val_loss', direction='min') Score: 0.06921965647772484</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 24274)]           0         \n",
      "_________________________________________________________________\n",
      "dense_0 (Dense)              (None, 1000)              24275000  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_out (Dense)            (None, 8693)              8701693   \n",
      "=================================================================\n",
      "Total params: 32,976,693\n",
      "Trainable params: 32,976,693\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 24274)]           0         \n",
      "_________________________________________________________________\n",
      "dense_0 (Dense)              (None, 1000)              24275000  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_out (Dense)            (None, 8693)              8701693   \n",
      "=================================================================\n",
      "Total params: 32,976,693\n",
      "Trainable params: 32,976,693\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "394/394 [==============================] - ETA: 1s - loss: 0.063 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 1s 2ms/sample - loss: 0.0692\n",
      "Valid loss 0.069220\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 24274)]           0         \n",
      "_________________________________________________________________\n",
      "model (Model)                (None, 8693)              32976693  \n",
      "_________________________________________________________________\n",
      "hpo_layer (HPOLayer)         (None, 8693, 8693)        75568249  \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 8693, 1)           8694      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 8693, 1)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 1)                 12        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_out (Dense)            (None, 8693)              17386     \n",
      "=================================================================\n",
      "Total params: 108,571,034\n",
      "Trainable params: 33,002,785\n",
      "Non-trainable params: 75,568,249\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Compilation finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 24274)]           0         \n",
      "_________________________________________________________________\n",
      "model (Model)                (None, 8693)              32976693  \n",
      "_________________________________________________________________\n",
      "hpo_layer (HPOLayer)         (None, 8693, 8693)        75568249  \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 8693, 1)           8694      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 8693, 1)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 1)                 12        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_out (Dense)            (None, 8693)              17386     \n",
      "=================================================================\n",
      "Total params: 108,571,034\n",
      "Trainable params: 33,002,785\n",
      "Non-trainable params: 75,568,249\n",
      "_________________________________________________________________\n",
      "Starting training the flat model\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 5 steps, validate for 5 steps\n",
      "4/5 [=======================>......] - ETA: 52s - loss: 0.6934 - accuracy: 0.496 - ETA: 34s - loss: 0.6931 - accuracy: 0.733 - ETA: 21s - loss: 0.6929 - accuracy: 0.810 - ETA: 10s - loss: 0.6927 - accuracy: 0.8475\n",
      "Epoch 00001: val_loss improved from inf to 0.69101, saving model to data/My_Implementations/Trial_2/model_mohamed.h5\n",
      "5/5 [==============================] - 65s 13s/step - loss: 0.6924 - accuracy: 0.8758 - val_loss: 0.6910 - val_accuracy: 0.9819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "INFO:root:Evaluating model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - ETA: 0s - loss: 0.050 - ETA: 0s - loss: 0.048 - 0s 40ms/step - loss: 0.0609\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - ETA: 3s - loss: 0.6910 - accuracy: 0.98 - ETA: 2s - loss: 0.6910 - accuracy: 0.98 - ETA: 1s - loss: 0.6910 - accuracy: 0.98 - ETA: 0s - loss: 0.6910 - accuracy: 0.98 - 4s 774ms/step - loss: 0.6910 - accuracy: 0.9819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Predicting\n",
      "INFO:root:Saving predictions\n"
     ]
    }
   ],
   "source": [
    "# main(hp_file, data_file, terms_file, gos_file, model_file,out_file, fold, batch_size, epochs, load, logger_file, threshold,device):\n",
    "\n",
    "main('data/hp.obo',\n",
    "     \n",
    "     'data/My_Implementations/human.pkl',\n",
    "     \n",
    "     'data/My_Implementations/terms.pkl',\n",
    "     \n",
    "     'data/My_Implementations/gos.pkl',\n",
    "     \n",
    "     'data/My_Implementations/Trial_2/model_mohamed.h5',\n",
    "     \n",
    "     'data/My_Implementations/Trial_2/predictions.pkl',\n",
    "     \n",
    "     1,\n",
    "     \n",
    "     1,\n",
    "     \n",
    "     1,\n",
    "     \n",
    "     False,\n",
    "     \n",
    "     'data/My_Implementations/Trial_2/training.csv',\n",
    "     \n",
    "     0.5,\n",
    "     \n",
    "     'CPU:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Implementing the HPO predictor without Flat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HPOLayer(Layer):\n",
    "\n",
    "    def __init__(self, nb_classes, **kwargs):\n",
    "        self.nb_classes = nb_classes\n",
    "        self.hpo_matrix = np.zeros((nb_classes, nb_classes), dtype=np.float32)\n",
    "        super(HPOLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def set_hpo_matrix(self, hpo_matrix):\n",
    "        self.hpo_matrix = hpo_matrix\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(HPOLayer, self).get_config()\n",
    "        config['nb_classes'] = self.nb_classes\n",
    "        return config\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        assert input_shape[1] == self.nb_classes\n",
    "        self.kernel = K.variable(\n",
    "            self.hpo_matrix, name='{}_kernel'.format(self.name))\n",
    "        self.non_trainable_weights.append(self.kernel)\n",
    "        super(HPOLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.keras.backend.repeat(x, self.nb_classes)\n",
    "        return tf.math.multiply(x, self.kernel)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [input_shape[0], self.nb_classes, self.nb_classes] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model(params, hpo_matrix):\n",
    "    inp = Input(shape=(8693, 8693), dtype=np.float32)\n",
    "    # Load flat model\n",
    "#     flat_model = load_model(params['model_file'] + '_flat.h5')\n",
    "    \n",
    "#     net = flat_model(inp)\n",
    "    \n",
    "#     hpo_layer = HPOLayer(params['nb_classes'])\n",
    "\n",
    "\n",
    "#     hpo_layer.trainable = False\n",
    "    \n",
    "#     hpo_layer.set_hpo_matrix(hpo_matrix)\n",
    "    \n",
    "#     net = hpo_layer(inp)\n",
    "    \n",
    "#     net = (Conv1D(filters=1,kernel_size=1,padding='valid',kernel_initializer= 'glorot_normal'))(net)\n",
    "    net = (Conv1D(filters=1,kernel_size=1,padding='valid',kernel_initializer= 'glorot_normal'))(inp)\n",
    "\n",
    "    net=MaxPooling1D(pool_size=1)(net)\n",
    "    \n",
    "    net=LSTM(1, activation=\"tanh\")(net)\n",
    "    \n",
    "#     net = MaxPooling1D(pool_size=params['nb_classes'])(net) \n",
    "    net= Flatten()(net)\n",
    "    \n",
    "#     output = Flatten()(net)\n",
    "#     output=RandomFourierFeatures( params['nb_classes'],\n",
    "                                              \n",
    "#                                               kernel_initializer='laplacian', \n",
    "                                              \n",
    "#                                               scale=None,\n",
    "                                              \n",
    "#                                               trainable=False, name='Output_SVM')(net)\n",
    "\n",
    "    output=Dense(params['nb_classes'], activation='sigmoid', name='dense_out',kernel_regularizer=regularizers.l2(0.0001))(net)\n",
    "    \n",
    "#     output=Dense(20, activation='sigmoid', name='dense_out')(net)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=output)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "#     model.compile( optimizer=Adam(lr=params['learning_rate']),   \n",
    "    model.compile(\n",
    "        optimizer=Adam(lr=params['learning_rate']),\n",
    "        metrics=['accuracy'],\n",
    "        loss=params['loss'])    \n",
    "    logging.info('Compilation finished')\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(hp_file, data_file, terms_file, gos_file, model_file,out_file, fold, batch_size, epochs, load, \n",
    "         logger_file, threshold,device):\n",
    "    gos_df = pd.read_pickle(gos_file)\n",
    "    gos = gos_df['gos'].values.flatten()\n",
    "    gos_dict = {v: i for i, v in enumerate(gos)}\n",
    "\n",
    "    # cross validation settings\n",
    "    # model_file = f'fold{fold}_' + model_file\n",
    "    # out_file = f'fold{fold}_' + out_file\n",
    "    params = {\n",
    "        'nb_classes': 3783,\n",
    "        'input_shape': (len(gos),),\n",
    "#         'input_shape': (20,),\n",
    "\n",
    "        'nb_layers': 1,\n",
    "        'loss': 'binary_crossentropy',\n",
    "#         'loss': 'Hinge',        \n",
    "\n",
    "        'rate': 0.3,\n",
    "        'learning_rate': 0.001,\n",
    "        'units': 1500, # 750\n",
    "        'model_file': model_file\n",
    "    }\n",
    "    \n",
    "    print('Params:', params)\n",
    "    global hpo\n",
    "    hpo = Ontology(hp_file, with_rels=True)\n",
    "    terms_df = pd.read_pickle(terms_file)\n",
    "    global terms\n",
    "    terms = terms_df['terms'].values.flatten()\n",
    "    print('Phenotypes', len(terms))\n",
    "    global term_set\n",
    "    term_set = set(terms)\n",
    "    train_df, valid_df, test_df = load_data(data_file, terms, fold)\n",
    "    terms_dict = {v: i for i, v in enumerate(terms)}\n",
    "    hpo_matrix = get_hpo_matrix(hpo, terms_dict)\n",
    "    nb_classes = len(terms)\n",
    "    print ('nb_classes=len(terms)_______________________ ',len(terms))\n",
    "    params['nb_classes'] = nb_classes\n",
    "    print('len(terms_dict)_______________________ ',len(terms_dict))\n",
    "    test_steps = int(math.ceil(len(test_df) / batch_size))\n",
    "    test_generator = DFGenerator(test_df, gos_dict, terms_dict,batch_size)\n",
    "    valid_steps = int(math.ceil(len(valid_df) / batch_size))\n",
    "    train_steps = int(math.ceil(len(train_df) / batch_size))\n",
    "    print ( 'test_steps',test_steps,'valid_steps',valid_steps,'train_steps',train_steps)\n",
    "    test_steps=5\n",
    "    valid_steps=5\n",
    "    train_steps=5\n",
    "    xy_generator = DFGenerator(train_df, gos_dict, terms_dict,len(train_df))\n",
    "    x, y = xy_generator[0]\n",
    "    val_generator = DFGenerator(valid_df, gos_dict, terms_dict,len(valid_df))\n",
    "    val_x, val_y = val_generator[0]\n",
    "\n",
    "    train_generator = DFGenerator(train_df, gos_dict, terms_dict,batch_size)\n",
    "    valid_generator = DFGenerator(valid_df, gos_dict, terms_dict,batch_size)\n",
    "    \n",
    "    with tf.device(device):\n",
    "        if load:\n",
    "            print('Loading pretrained model')\n",
    "            model = load_model(model_file)\n",
    "        else:\n",
    "            \n",
    "            print('Creating a new model')\n",
    "\n",
    "\n",
    "            print(\"Training data size: %d\" % len(train_df))\n",
    "            print(\"Validation data size: %d\" % len(valid_df))\n",
    "            logger = CSVLogger(logger_file)\n",
    "            earlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            model = create_model(params, hpo_matrix)\n",
    "\n",
    "\n",
    "            checkpointer = ModelCheckpoint( filepath=model_file, verbose=1, save_best_only=True)\n",
    "            model.summary()\n",
    "            print('Starting training the my Implemented model')\n",
    "            \n",
    "            model.fit(train_generator, steps_per_epoch=train_steps, epochs=epochs,\n",
    "                      \n",
    "                validation_data=valid_generator, validation_steps=valid_steps, max_queue_size=batch_size,\n",
    "                      \n",
    "                workers=1, callbacks=[logger, checkpointer, earlystopper])\n",
    "\n",
    "            logging.info('Loading best model')\n",
    "            \n",
    "#             model = load_model(model_file, custom_objects={'HPOLayer': HPOLayer})\n",
    "            model = load_model(model_file)\n",
    "            \n",
    "        logging.info('Evaluating model')\n",
    "        loss = model.evaluate(test_generator, steps=test_steps)\n",
    "\n",
    "        logging.info('Predicting')\n",
    "        #-----------------------------------------------------\n",
    "        test_generator.reset()\n",
    "        #3944\n",
    "        test_steps = int(math.ceil(len(test_df) / batch_size))\n",
    "        \n",
    "        preds = model.predict(test_generator, steps=test_steps)\n",
    "        \n",
    "        logging.info('creating the dataFrames for saving the predictions')\n",
    "\n",
    "        all_terms_df = pd.read_pickle(terms_file)\n",
    "        all_terms = all_terms_df['terms'].values\n",
    "        all_terms_dict = {v:k for k,v in enumerate(all_terms)}\n",
    "        \n",
    "        all_labels = np.zeros((len(test_df), len(all_terms)), dtype=np.int32)\n",
    "        \n",
    "        for i, row in enumerate(test_df.itertuples()):\n",
    "            for hp_id in row.hp_annotations:\n",
    "                if hp_id in all_terms_dict:\n",
    "                    all_labels[i, all_terms_dict[hp_id]] = 1\n",
    "                    \n",
    "                    \n",
    "        logging.info('Placing the  predictions')\n",
    "        all_preds = np.zeros((len(test_df), len(all_terms)), dtype=np.float32)\n",
    "\n",
    "        test_df['preds'] = list(preds)\n",
    "        logging.info('Saving my implemented predictions')\n",
    "        test_df.to_pickle(out_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {'nb_classes': 3783, 'input_shape': (24274,), 'nb_layers': 1, 'loss': 'binary_crossentropy', 'rate': 0.3, 'learning_rate': 0.001, 'units': 1500, 'model_file': 'data/My_Implementations/Trial_3/model_mohamed.h5'}\n",
      "Phenotypes 8693\n",
      "3933 3539 394 394\n",
      "nb_classes=len(terms)_______________________  8693\n",
      "len(terms_dict)_______________________  8693\n",
      "test_steps 394 valid_steps 394 train_steps 3539\n",
      "Creating a new model\n",
      "Training data size: 3539\n",
      "Validation data size: 394\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 8693, 8693)]      0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 8693, 1)           8694      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 8693, 1)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 1)                 12        \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_out (Dense)            (None, 8693)              17386     \n",
      "=================================================================\n",
      "Total params: 26,092\n",
      "Trainable params: 26,092\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Compilation finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 8693, 8693)]      0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 8693, 1)           8694      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 8693, 1)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 1)                 12        \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_out (Dense)            (None, 8693)              17386     \n",
      "=================================================================\n",
      "Total params: 26,092\n",
      "Trainable params: 26,092\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Starting training the my Implemented model\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in converted code:\n\n    C:\\Users\\Mohamed Elhaj_Abdou\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py:677 map_fn\n        batch_size=None)\n    C:\\Users\\Mohamed Elhaj_Abdou\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py:2410 _standardize_tensors\n        exception_prefix='input')\n    C:\\Users\\Mohamed Elhaj_Abdou\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py:573 standardize_input_data\n        'with shape ' + str(data_shape))\n\n    ValueError: Error when checking input: expected input_10 to have 3 dimensions, but got array with shape (None, None)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-4ac2f605d95f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m      \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m      'CPU:0')\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-84644effb268>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(hp_file, data_file, terms_file, gos_file, model_file, out_file, fold, batch_size, epochs, load, logger_file, threshold, device)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                 workers=1, callbacks=[logger, checkpointer, earlystopper])\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loading best model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, standardize_function, shuffle, workers, use_multiprocessing, max_queue_size, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m         **kwargs)\n\u001b[0m\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, standardize_function, workers, use_multiprocessing, max_queue_size, **kwargs)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstandardize_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardize_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shuffle\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mstandardize_function\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m    682\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 684\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[0;32m   1589\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1590\u001b[0m       return ParallelMapDataset(\n\u001b[1;32m-> 1591\u001b[1;33m           self, map_func, num_parallel_calls, preserve_cardinality=True)\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   3924\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3925\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3926\u001b[1;33m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[0;32m   3927\u001b[0m     self._num_parallel_calls = ops.convert_to_tensor(\n\u001b[0;32m   3928\u001b[0m         num_parallel_calls, dtype=dtypes.int32, name=\"num_parallel_calls\")\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3145\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3146\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3147\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2393\u001b[0m     \u001b[1;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2394\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[1;32m-> 2395\u001b[1;33m         *args, **kwargs)\n\u001b[0m\u001b[0;32m   2396\u001b[0m     \u001b[1;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2397\u001b[0m     \u001b[1;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2388\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2389\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2390\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2703\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2705\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2593\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2595\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    976\u001b[0m                                           converted_func)\n\u001b[0;32m    977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3138\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m   3139\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3140\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3141\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3080\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m       \u001b[1;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m       \u001b[1;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in converted code:\n\n    C:\\Users\\Mohamed Elhaj_Abdou\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py:677 map_fn\n        batch_size=None)\n    C:\\Users\\Mohamed Elhaj_Abdou\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py:2410 _standardize_tensors\n        exception_prefix='input')\n    C:\\Users\\Mohamed Elhaj_Abdou\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py:573 standardize_input_data\n        'with shape ' + str(data_shape))\n\n    ValueError: Error when checking input: expected input_10 to have 3 dimensions, but got array with shape (None, None)\n"
     ]
    }
   ],
   "source": [
    "# main(hp_file, data_file, terms_file, gos_file, model_file,out_file, fold, batch_size, epochs, load, logger_file, threshold,device):\n",
    "\n",
    "main('data/hp.obo',\n",
    "     \n",
    "     'data/My_Implementations/human.pkl',\n",
    "     \n",
    "     'data/My_Implementations/terms.pkl',\n",
    "     \n",
    "     'data/My_Implementations/gos.pkl',\n",
    "     \n",
    "     'data/My_Implementations/Trial_3/model_mohamed.h5',\n",
    "     \n",
    "     'data/My_Implementations/Trial_3/predictions.pkl',\n",
    "     \n",
    "     1,\n",
    "     \n",
    "     1,\n",
    "     \n",
    "     1,\n",
    "     \n",
    "     False,\n",
    "     \n",
    "     'data/My_Implementations/Trial_3/training.csv',\n",
    "     \n",
    "     0.5,\n",
    "     \n",
    "     'CPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ( \"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3060 Ti "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
